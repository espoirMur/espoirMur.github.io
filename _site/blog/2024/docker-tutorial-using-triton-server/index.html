{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to prepare an encoder decoder transformer model to production."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimize the M2M100 model with ONNX\n",
    "\n",
    "\n",
    "In this series of posts, we will learn how to productionalize  a machine translation model. We will start from a HuggingFace transformer model and learn how to deploy it in a production setting and make it accessible to users.\n",
    "\n",
    "In the first notebook, we will learn how to prepare the model for production. We will load the model from the HuggingFace library and then quantize it; after quantization, we will use the triton server to deploy it in a docker container, and finally, we will learn how to make inference requests  to our model.\n",
    "\n",
    "In the second post,  we will learn how to scale the model using Kserve and how to optimize the first version of our model.\n",
    "\n",
    "This post is for Machine Learning Engineers/Enthusiasts with some knowledge of transformers models and Docker and who would like to learn how to deploy an encoder-decoder model in a production setting.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment requirements\n",
    "\n",
    "To run this code, you need to have `python 3.11` installed on your local machine. \n",
    "\n",
    "You can install these libraries directly from your Python interpreter, or you can create a virtual environment to run Python. I would rather recommend using a Python interpreter from a virtual environment.\n",
    "\n",
    "### Install libraries\n",
    "\n",
    "To install the useful libraries, you can use the following code: \n",
    "\n",
    "`pip transformers==4.30.2 optimum==1.9.0 onnxruntime==1.15.1 onnx==1.14.0`\n",
    "\n",
    "# A brief history of the M2M100 Model\n",
    "\n",
    "### Encoder-Decoder model \n",
    "\n",
    "Encoder-decoder models are large language models built with two components: the encoder and the decoder. They are used for natural language processing tasks that involve understanding input sequences and generating output sequences with different lengths and structures.\n",
    "\n",
    "The encoder is a neural network that takes a variable-length sequence as an input and transforms it into a  vector representation.  For our machine translation task, the encoder takes the token in the source language and returns a vector representation of the source language. \n",
    "\n",
    "The decoder, on the other hand, is also a neural network  that takes the vector representation of the source text and generates the translation in the target language.\n",
    "\n",
    "<Put the Image here.>\n",
    "You can learn more about transformer models and encoder-decoder models, particularly [here](https://jalammar.github.io/illustrated-transformer/)\n",
    "\n",
    "\n",
    "### The M2M100 model:\n",
    "\n",
    "The M2M100 stands for Many to Many multilingual translation model that can translate between any pair of 100 languages it was trained on. It helps to alleviate the fact that most machine translation training is done from or to English. You can learn more about the M2M100 model [here](https://huggingface.co/docs/transformers/model_doc/m2m_100).\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "It was trained to translate English to Swahili. Why did I pick Swahili? Because I am a native Swahili speaker. \n",
    "\n",
    "## Testing the raw model\n",
    "\n",
    "We will start by loading our model from the huggingface repository!\n",
    "The below code will load the model from the HuggingFace library and perform a translation inference by using the generate method.\n",
    "\n",
    "<Briefly talk about the encoder-decoder architecture and particularity  of M2M100.>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, M2M100ForConditionalGeneration, pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"masakhane/m2m100_418M_en_swa_rel_news\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model: M2M100ForConditionalGeneration = M2M100ForConditionalGeneration.from_pretrained(MODEL_NAME)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_to_translate = \"Hello, my name is Espoir Murhabazi,  I am a Software Engineer from Congo DRC but living in UK\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_input = tokenizer(text_to_translate, return_tensors=\"pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_input.keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_tokens = model.generate(**model_input, forced_bos_token_id=tokenizer.lang_code_to_id[\"sw\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point our model have generate the translation token, the next step is to use our tokenizer to convert back the token to the text. This is called decoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translated_text = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translated_text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The translated test shows us that the model is working. The next step is to prepare the production model. \n",
    "To productionalize our model, we will deploy it to ONNX format.\n",
    "\n",
    "#### What is the ONNX format?\n",
    "\n",
    "ONNX stands for Open Neural Network Exchange. It is an open format built to represent machine learning models in a framework-agnostic way.\n",
    "\n",
    "As you may know, neural networks are computation graphs with input, weights, and operations. [Cite the source here.]\n",
    "\n",
    "ONNX format is a way of saving neural networks as computation graphs. That  computational graph represents the flow of data through the neural network.\n",
    "\n",
    "\n",
    "The key benefits of saving neural networks in the ONNX format are interoperability and hardware access. Any deep learning platform can read a neural network saved in the ONNX format.  For example, a model trained in Pytorch can be exported to ONNX format and imported in Tensorflow and vice versa.\n",
    "\n",
    "You don't need to use Python to read a model saved as ONNX; you can use any programming language of your choice, such as Javascript, C, or C++. \n",
    "\n",
    "ONNX makes the model easier to access hardware optimizations, and you can apply other optimizations, such as quantization, to your ONNX model.\n",
    "\n",
    "Let us see how we can convert our model to ONNX format to use the full benefits of it."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying to export the model manually and see if we can load the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To export the model to onnx format we will be using the optimum cli from Huggingface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! optimum-cli export onnx --model masakhane/m2m100_418M_en_swa_rel_news --task seq2seq-lm-with-past --for-ort onnx/m2m100_418M_en_swa_rel_news\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check if the model is correct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the previous command was run successfully, we can see our model saved at `onnx/m2m100_418M_en_swa_rel_news`. \n",
    "\n",
    "By checking the size we notice data our encoder model have 1.1 Gb, and our decoder model have 1.7Gb which make our model size to 2.8GB. Additionally, in the same folder we have the tokenizer data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_onnx_dir = Path.cwd().joinpath('onnx').joinpath('m2m100_418M_en_swa_rel_news')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_onnx_dir.exists()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying Quantization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quantization is the process of reducing the model size by using fewer bits to represent its parameters. Instead of using 32-bit precision floating points for most of the models, with quantization, we can use 12 bits to represent a number and consequently reduce the size of the model.\n",
    "\n",
    "Smaller models resulting from quantization are faster to deploy and have low latency in production.\n",
    "\n",
    "It has [been shown](https://github.com/huggingface/notebooks/blob/main/examples/onnx-export.ipynb) that you can improve the inference time by 75% by using an ONNX quantized model without a considerable loss in performance. <Find more evidence for this> \n",
    "\n",
    "\n",
    "For this tutorial, we will use quantization to reduce the size of our model for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from optimum.onnxruntime import ORTQuantizer, ORTModelForSeq2SeqLM\n",
    "from optimum.onnxruntime.configuration import AutoQuantizationConfig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_quantizer = ORTQuantizer.from_pretrained(base_model_onnx_dir, file_name=\"encoder_model.onnx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_quantizer = ORTQuantizer.from_pretrained(base_model_onnx_dir, file_name=\"decoder_model.onnx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_with_past_quantizer = ORTQuantizer.from_pretrained(base_model_onnx_dir, file_name=\"decoder_with_past_model.onnx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantizers = [encoder_quantizer, decoder_quantizer, decoder_with_past_quantizer]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use dynamic quantization to our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dynamic_quantization_config = AutoQuantizationConfig.avx512_vnni(is_static=False, per_channel=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantized_model_path = Path(\"onnx\").joinpath(f\"{MODEL_SUFFIX}_quantized/\")\n",
    "quantized_model_path.mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for quantizer in quantizers:\n",
    "    quantizer.quantize(quantization_config=dynamic_quantization_config, save_dir=quantized_model_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model are save as quantized version, we can now check the size of the quantized models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in quantized_model_path.glob(\"*.onnx\"):\n",
    "    print(f\"the size of {model.stem} the model in MB is: {model.stat().st_size / (1024 * 1024)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that we have managed to reduce the size of our initial models by two! From 1.6 Gb without quantization to 800 Mb with quantization. Let us see how to use the quantized model for inference."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the quantized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantized_model_path = base_model_onnx_dir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantized_model = ORTModelForSeq2SeqLM.from_pretrained(quantized_model_path, \n",
    "                                                       decoder_file_name='decoder_model_quantized.onnx',\n",
    "                                                       encoder_file_name='encoder_model_quantized.onnx',)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantized_pipeline = pipeline(\"translation_en_to_sw\", model=quantized_model, tokenizer=tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translated_text_quantized = quantized_pipeline(text_to_translate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(translated_text_quantized)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The quantization has reduced the size of the model, but it gave the same translation of our base text. We may need to run more extensive tests to find out what is the accuracy difference between our quantized model and the base model. Assuming the performance lost was not considerable, we move to the next step of our tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy the Model for inference\n",
    "\n",
    "At this point, we have our model quantized and saved in ONNX format. We will now deploy it to a production server using the triton inference server. \n",
    "In the first section, we will deploy with the triton server as a docker container, and then we will use Kserve to deploy it to the Kubernetes deployment environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Triton Server\n",
    "\n",
    "Triton is a software tool for deploying machine learning models for inference. It is designed to produce high-quality inference across different hardware platforms, either GPU or CPU. It also supports inference across cloud, data center, and embedded devices.\n",
    "One of the advantages of the triton server is that it supports dynamic batching and concurrent model execution.\n",
    "\n",
    "- Dynamic batching, for models that support batching, which is the case for deep learning models, triton implements scheduling and batching algorithms that combine individual requests to improve inference throughput.\n",
    "\n",
    "<Talk More about dynamic batching here...>\n",
    "    \n",
    "- Concurrency model execution is the capacity to run simultaneously multiple models on the same GPU or various GPUs.\n",
    "\n",
    "\n",
    "### Triton Server Backend\n",
    "\n",
    "Triton supports different backends to execute the model. A backend is a wrapper around a deep learning framework like Pytorch, TensorFlow, TensorRT, or ONNX Runtime.\n",
    "Two backend types interested us for this post: the Python Backend and the ONNX runtime backend. \n",
    "\n",
    "The ONNX runtime backend executes ONNX models, and the Python backend allows the writing of the model logic in Python. \n",
    "\n",
    "In this post, we will be focused on the ONNX and the Python backend.\n",
    "\n",
    "I decided to go with the Python backend because I struggled to deploy the encoder decode model using an ensemble of the ONNX model. I still have a question in progress on [StackOverlow](https://stackoverflow.com/q/76638766/4683950).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Uploading the Model to Repository.\n",
    "\n",
    "The first step before using our model is to upload it to the model repository. For this tutorial,   we will be using our local storage as a model repository but later we will use static storage such as google cloud or AWS S3 to host our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configuration\n",
    "\n",
    "The first step to deploy our model in triton is to configure it.\n",
    "\n",
    "The configuration sets up the model and defines the input shape and the output shape of our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ./triton_model_repository/m2m100_translation_model/config.pbtxt\n",
    "name: \"m2m100_translation_model\"\n",
    "backend: \"python\"\n",
    "max_batch_size: 0\n",
    "input [\n",
    "  {\n",
    "    name: \"input_ids\"\n",
    "    data_type: TYPE_INT64\n",
    "    dims: [ -1, -1 ]\n",
    "  },\n",
    "{\n",
    "    name: \"attention_mask\"\n",
    "    data_type: TYPE_INT64\n",
    "    dims: [ -1, -1 ]\n",
    "  }\n",
    "]\n",
    "output [\n",
    "    {\n",
    "    name: \"generated_indices\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [ -1, -1 ]\n",
    "  }\n",
    "]\n",
    "\n",
    "instance_group [\n",
    "    {\n",
    "      count: 1\n",
    "      kind: KIND_CPU\n",
    "    }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above configuration, we can see that the model is expecting two inputs:  the input ids and the attention masks, and it returns the generated input indices. \n",
    "Additionally, we can notice that the model is running on a 1 CPU. If we had a GPU available, we would put it in the instance settings.\n",
    "\n",
    "The input ids and the attention masks are the outputs from the tokenization process. The generated indices are the tokenized output that our tokenizer will decode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The configuration file needs to be save at the root folder  of our model repository."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the load model script\n",
    "\n",
    "The load model script is the python script that load our model before and run it for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ./triton_model_repository/m2m100_translation_model/1/model.py\n",
    "from typing import Dict, List\n",
    "import triton_python_backend_utils as pb_utils\n",
    "from pathlib import Path\n",
    "from optimum.onnxruntime import ORTModelForSeq2SeqLM\n",
    "import torch\n",
    "\n",
    "TOKENIZER_SW_LANG_CODE_TO_ID = 128088\n",
    "\n",
    "\n",
    "class TritonPythonModel:\n",
    "\n",
    "    def initialize(self, args: Dict[str, str]) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the tokenization process\n",
    "        :param args: arguments from Triton config file\n",
    "        \"\"\"\n",
    "        current_path: str = Path(args[\"model_repository\"]).parent.absolute()\n",
    "        model_path = current_path.joinpath(\"m2m100_translation_model\", \"1\", \"m2m100_418M_en_swa_rel_news_quantized\")\n",
    "        self.device = \"cpu\" if args[\"model_instance_kind\"] == \"CPU\" else \"cuda\"\n",
    "        # more variables in https://github.com/triton-inference-server/python_backend/blob/main/src/python.cc\n",
    "        self.model = ORTModelForSeq2SeqLM.from_pretrained(model_path,\n",
    "                                                          decoder_file_name=\"decoder_model_quantized.onnx\",\n",
    "                                                          encoder_file_name=\"encoder_model_quantized.onnx\")\n",
    "        if self.device == \"cuda\":\n",
    "            self.model = self.model.cuda()\n",
    "        print(\"TritonPythonModel initialized\")\n",
    "\n",
    "    def execute(self, requests) -> \"List[List[pb_utils.Tensor]]\":\n",
    "        \"\"\"\n",
    "        Parse and tokenize each request\n",
    "        :param requests: 1 or more requests received by Triton server.\n",
    "        :return: text as input tensors\n",
    "        \"\"\"\n",
    "        responses = []\n",
    "        # for loop for batch requests (disabled in our case)\n",
    "        for request in requests:\n",
    "            # binary data typed back to string\n",
    "            input_ids = pb_utils.get_input_tensor_by_name(request, \"input_ids\").as_numpy()\n",
    "            attention_masks = pb_utils.get_input_tensor_by_name(request, \"attention_mask\").as_numpy()\n",
    "            input_ids = torch.as_tensor(input_ids, dtype=torch.int64)\n",
    "            attention_masks = torch.as_tensor(attention_masks, dtype=torch.int64)\n",
    "            if self.device == \"cuda\":\n",
    "                input_ids = input_ids.to(\"cuda\")\n",
    "                attention_masks = attention_masks.to(\"cuda\")\n",
    "            model_inputs = {\"input_ids\": input_ids, \"attention_mask\": attention_masks}\n",
    "            generated_indices = self.model.generate(**model_inputs,\n",
    "                                                    forced_bos_token_id=TOKENIZER_SW_LANG_CODE_TO_ID)\n",
    "            tensor_output = pb_utils.Tensor(\"generated_indices\", generated_indices.numpy())\n",
    "            responses.append(tensor_output)\n",
    "        responses = [pb_utils.InferenceResponse(output_tensors=responses)]\n",
    "        return responses\n",
    "    \n",
    "    def finalize(self):\n",
    "        \"\"\"`finalize` is called only once when the model is being unloaded.\n",
    "        Implementing `finalize` function is optional. This function allows\n",
    "        the model to perform any necessary clean ups before exit.\n",
    "        \"\"\"\n",
    "        print('Cleaning up...')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model contains a class with two methods:\n",
    "\n",
    "- Initialize: The initialize method uses the ORT model to load the model in the memory!\n",
    "- The execute method parse and tokenize each request received by the triton server. It calls the generate method on the input of the request and returns the generated text indices. This text will be later decoded by the tokenizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If our configuration is done properly and the model is saved properly, we should have a model repository that looks like this:\n",
    "\n",
    "```\n",
    "triton_model_repository\n",
    "└── m2m100_translation_model\n",
    "    ├── 1\n",
    "    │   ├── m2m100_418M_en_swa_rel_news_quantized\n",
    "    │   │   ├── config.json\n",
    "    │   │   ├── decoder_model_quantized.onnx\n",
    "    │   │   ├── decoder_with_past_model_quantized.onnx\n",
    "    │   │   ├── encoder_model_quantized.onnx\n",
    "    │   │   └── ort_config.json\n",
    "    │   └── model.py\n",
    "    └── config.pbtxt\n",
    "```\n",
    "\n",
    "Make sure that you have the file located at the precise location as me in order to be able to run the code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Launching the docker image\n",
    "\n",
    "If you look carefully at the code for our Python model, you can see that the model is importing the ONNX runtime! However, that runtime is not installed in the base triton server image. The reason why we decided to build our own image based on the triton server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load Dockerfile\n",
    "# Use the base image\n",
    "FROM nvcr.io/nvidia/tritonserver:23.06-py3\n",
    "\n",
    "# Install the required Python packages\n",
    "RUN pip install optimum==1.9.0 onnxruntime==1.15.1 onnx==1.14.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code shows how we build our docker image.\n",
    "We use the base Tritonserver image, and then we add the different packages we need to run our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we can build our model using:\n",
    "\n",
    "`docker build -t espymur/triton-onnx:dev  -f Dockerfile .`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please note that the image is huge. Its size is around 15 GB. In the next post, I will try to optimize its size by using the technique suggested in the documentation.\n",
    "\n",
    "If our model build is finished, we can now run the docker container that serves the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "`docker run --rm -p 8000:8000 -p 8001:8001 -p 8002:8002  --shm-size 128M -v ${PWD}/triton_model_repository:/models  espymur/triton-onnx:dev tritonserver --model-repository=/models`\n",
    "\n",
    "- This command runs the docker container and map the port 8000, 8001, 8002 to 8000, 8001, and 8002 of our local machine.\n",
    "\n",
    "- It then creates a volume that maps the `${PWD}/triton_model_repository` path from our local machine to /models in the container.\n",
    "\n",
    "- It is also using a shared memory of 128 Mb.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this model we can see that our model is running and we can perform inference without any problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we have got our model running inside the docker container, the next step will be to make inference requests. Let see how we can achieve that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Inference Requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is now updated and saved as a Triton backend model. We will apply tokenization offline and query the model with the tokenized words and the attention mask. \n",
    "The model will return the indices of the translated test; we will use the tokenizer again to decode the indices and produce the output.\n",
    "\n",
    "We can later have the tokenizer as a separate service people can interact with using HTTP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"masakhane/m2m100_418M_en_swa_rel_news\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tritonclient.http as httpclient\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The HTTP client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = httpclient.InferenceServerClient(url=\"localhost:8000\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This line creates the client object we will be using to interact with our server. To create the client object, we are passing the URL of the inference service as parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = httpclient.InferInput(\"input_ids\", shape=(-1,1) , datatype=\"TYPE_INT64\",)\n",
    "attention_mask = httpclient.InferInput(\"attention_mask\", shape=(-1,1) , datatype=\"TYPE_INT64\",)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = httpclient.InferRequestedOutput(\"generated_indices\", binary_data=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To prepare our model input, we are using the Triton client library. \n",
    "The above code creates two objects for the input ID and the attention mask, respectively! We can specify the shape of the element and its datatype when creating the code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally to our inputs and outputs, we will need some utility function to perform the tokenization. Here are those functions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utilities Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokenizer(model_name):\n",
    "    \"\"\"Returns a tokenizer for a given model name\n",
    "\n",
    "    Args:\n",
    "        model_name (_type_): _description_\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    return tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, List\n",
    "\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def tokenize_text(tokenizer: AutoTokenizer, text:str) -> Tuple[np.ndarray , np.ndarray]:\n",
    "    tokenized_text = tokenizer(text, padding=True, return_tensors=\"np\")\n",
    "    return tokenized_text.input_ids, tokenized_text.attention_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_inference_input(input_ids: np.ndarray, attention_mask: np.ndarray) -> List[httpclient.InferInput]:\n",
    "    \"\"\"\n",
    "    Generate inference inputs for Triton server\n",
    "\n",
    "    Args:\n",
    "        input_ids (np.ndarray): _description_\n",
    "        attention_mask (np.ndarray): _description_\n",
    "\n",
    "    Returns:\n",
    "        List[httpclient.InferInput]: _description_\n",
    "    \"\"\"\n",
    "    inputs = []\n",
    "    inputs.append(httpclient.InferInput(\"input_ids\", input_ids.shape, \"INT64\"))\n",
    "    inputs.append(httpclient.InferInput(\"attention_mask\", attention_mask.shape, \"INT64\"))\n",
    "\n",
    "    inputs[0].set_data_from_numpy(input_ids.astype(np.int64), binary_data=False)\n",
    "    inputs[1].set_data_from_numpy(attention_mask.astype(np.int64), binary_data=False)\n",
    "    return inputs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [\"I am learning how to use Triton Server for Machine Learning\", \"Hello, my name is Espoir Murhabazi,  I am a Software Engineer from Congo DRC but living in UK\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer(MODEL_NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids, attention_mask = tokenize_text(tokenizer, text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_inputs = generate_inference_input(input_ids, attention_mask)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our input prepared we can now make an inference request to our server. Here is the code we will be using to make the inference request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = client.infer(model_name=\"m2m100_translation_model\", inputs=inference_inputs, outputs=[outputs])\n",
    "inference_output = results.as_numpy('generated_indices')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If everything goes as planned, we should be able to see the inference response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_output = tokenizer.batch_decode(inference_output, skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the decoded output, we can see that our inference server is working!\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "\n",
    "In this post, we saw how we can start form a raw translation model from huggingface, we then quantized it to reduce it's size, and finally deployed the model on a triton server to perform inference.\n",
    "In the second part of this blog we will learn how to scale the whole prototype and build an end to end pipeline using kubernetes and Kserve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7d420a2576d2f2cf4aee17bb1c719cb2b545f2d9fd7bdced2270e528bc643b9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
