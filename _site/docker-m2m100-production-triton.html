<!DOCTYPE html>
<html>
<head>
  <meta http-equiv="Content-type" content="text/html;charset=UTF-8" />
  

<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1">
<script
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"
  type="text/javascript">
</script>
<title>Deploy a Transformer models for Machine Translation in production using the Triton Server.</title>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.1.1/css/all.min.css" integrity="sha512-KfkfwYDsLkIlwQp6LFnl8zNdLGxu9YAA1QvwINks4PhcElQSvqcyVLLD9aMhXd13uQjoXtEKNosOWaZqXgel0g==" crossorigin="anonymous" referrerpolicy="no-referrer" />
<meta name="description" content="How to deploy a transformer models in production using the triton server">
<meta property="og:image" content=/assets/images/post-default-cover.jpeg>
<link rel="stylesheet" href="/assets/css/main.css">
<link rel="canonical" href="http://localhost:4000/docker-m2m100-production-triton">

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-CJV32Y553Z"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-CJV32Y553Z');
</script>


  <!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Deploy a Transformer models for Machine Translation in production using the Triton Server. | Espoir Murhabazi ideas’ home</title>
<meta name="generator" content="Jekyll v4.2.2" />
<meta property="og:title" content="Deploy a Transformer models for Machine Translation in production using the Triton Server." />
<meta name="author" content="Murhabazi Buzina, Espoir" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="How to deploy a transformer models in production using the triton server" />
<meta property="og:description" content="How to deploy a transformer models in production using the triton server" />
<link rel="canonical" href="http://localhost:4000/docker-m2m100-production-triton" />
<meta property="og:url" content="http://localhost:4000/docker-m2m100-production-triton" />
<meta property="og:site_name" content="Espoir Murhabazi ideas’ home" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2024-01-19T07:03:59+00:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Deploy a Transformer models for Machine Translation in production using the Triton Server." />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Murhabazi Buzina, Espoir","url":"https://murhabazi.com/"},"dateModified":"2024-01-19T07:03:59+00:00","datePublished":"2024-01-19T07:03:59+00:00","description":"How to deploy a transformer models in production using the triton server","headline":"Deploy a Transformer models for Machine Translation in production using the Triton Server.","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/docker-m2m100-production-triton"},"url":"http://localhost:4000/docker-m2m100-production-triton"}</script>
<!-- End Jekyll SEO tag -->

</head>
<body>
  <header class="site-header">
  <div class="container">
    <input type="checkbox" id="toggleNavbar">
    <h1 class="logo"><a href="/">esp<span>.py</span></a></h1>
    <label for="toggleNavbar" role="button" class="toggle-navbar-button">
      <i class="icon icon-menu"></i>
      <i class="icon icon-cross"></i>
    </label>
    <nav class="navbar">
      <ul>
        <li><a href="/" title="Home">127.0.0.1</a></li>
        
          <li><a href="/about" title="whoami();">whoami();</a></li>
        
          <li><a href="/blog" title="read();">read();</a></li>
        
          <li><a href="/drafts" title="drafts();">drafts();</a></li>
        
          <li><a href="/learnings" title="learnings();">learnings();</a></li>
        
      </ul>
    </nav>
  </div>
</header>


<main class="main-container">
  <div class="container">
    <article role="article" class="post">

  <div class="card">
    <header class="post-header">
      <h1 class="post-title">Deploy a Transformer models for Machine Translation in production using the Triton Server.</h1>
      <em class="post-meta">
        <time>Jan 19, 2024</time>
      </em>
    </header>

    <div class="post-content">
      
      <h3 id="how-to-prepare-an-encoder-decoder-transformer-model-to-production">How to prepare an encoder decoder transformer model to production.</h3>

<h4 id="optimize-the-m2m100-model-with-onnx">Optimize the M2M100 model with ONNX</h4>

<p>In this series of posts, we will learn how to productionalize  a machine translation model. We will start from a HuggingFace transformer model and learn how to deploy it in a production setting and make it accessible to users.</p>

<p>In the first notebook, we will learn how to prepare the model for production. We will load the model from the HuggingFace library and then quantize it; after quantization, we will use the triton server to deploy it in a docker container, and finally, we will learn how to make inference requests  to our model.</p>

<p>In the second post,  we will learn how to scale the model using Kserve and how to optimize the first version of our model.</p>

<p>This post is for Machine Learning Engineers/Enthusiasts with some knowledge of transformers models and Docker and who would like to learn how to deploy an encoder-decoder model in a production setting.</p>

<h2 id="environment-requirements">Environment requirements</h2>

<p>To run this code, you need to have <code class="language-bash highlighter-rouge">python 3.11</code> installed on your local machine.</p>

<p>You can install these libraries directly from your Python interpreter, or you can create a virtual environment to run Python. I would rather recommend using a Python interpreter from a virtual environment.</p>

<h3 id="install-libraries">Install libraries</h3>

<p>To install the useful libraries, you can use the following code:</p>

<p><code class="language-bash highlighter-rouge">pip <span class="nv">transformers</span><span class="o">==</span>4.30.2 <span class="nv">optimum</span><span class="o">==</span>1.9.0 <span class="nv">onnxruntime</span><span class="o">==</span>1.15.1 <span class="nv">onnx</span><span class="o">==</span>1.14.0</code></p>

<h1 id="a-brief-history-of-the-m2m100-model">A brief history of the M2M100 Model</h1>

<h3 id="encoder-decoder-model">Encoder-Decoder model</h3>

<p>Encoder-decoder models are large language models built with two components: the encoder and the decoder. They are used for natural language processing tasks that involve understanding input sequences and generating output sequences with different lengths and structures.</p>

<p>The encoder is a neural network that takes a variable-length sequence as an input and transforms it into a  vector representation.  For our machine translation task, the encoder takes the token in the source language and returns a vector representation of the source language.</p>

<p>The decoder, on the other hand, is also a neural network  that takes the vector representation of the source text and generates the translation in the target language.</p>

<p>IMAGE HERE</p>

<p>You can learn more about transformer models and encoder-decoder models, particularly <a href="https://jalammar.github.io/illustrated-transformer/">here</a>.</p>

<h3 id="the-m2m100-model">The M2M100 model:</h3>

<p>The M2M100 stands for Many to Many multilingual translation model that can translate between any pair of 100 languages it was trained on. It helps to alleviate the fact that most machine translation training is done from or to English. You can learn more about the M2M100 model <a href="https://huggingface.co/docs/transformers/model_doc/m2m_100">here</a>.</p>

<p>It was trained to translate English to Swahili. Why did I pick Swahili? Because I am a native Swahili speaker.</p>

<h2 id="testing-the-raw-model">Testing the raw model</h2>

<p>We will start by loading our model from the huggingface repository!
The below code will load the model from the HuggingFace library and perform a translation inference by using the generate method.</p>

<p><strong>Briefly talk about the encoder-decoder architecture and particularity  of M2M100.</strong></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">M2M100ForConditionalGeneration</span><span class="p">,</span> <span class="n">pipeline</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">MODEL_NAME</span> <span class="o">=</span> <span class="s">"masakhane/m2m100_418M_en_swa_rel_news"</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span><span class="p">:</span> <span class="n">M2M100ForConditionalGeneration</span> <span class="o">=</span> <span class="n">M2M100ForConditionalGeneration</span><span class="p">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">MODEL_NAME</span><span class="p">)</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="p">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">MODEL_NAME</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">text_to_translate</span> <span class="o">=</span> <span class="s">"Hello, my name is Espoir Murhabazi,  I am a Software Engineer from Congo DRC but living in UK"</span>

</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model_input</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">text_to_translate</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s">"pt"</span><span class="p">)</span>

</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model_input</span><span class="p">.</span><span class="n">keys</span><span class="p">()</span>

</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">generated_tokens</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">model_input</span><span class="p">,</span> <span class="n">forced_bos_token_id</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">.</span><span class="n">lang_code_to_id</span><span class="p">[</span><span class="s">"sw"</span><span class="p">])</span>

</code></pre></div></div>

<p>At this point our model have generate the translation token, the next step is to use our tokenizer to convert back the token to the text. This is called decoding.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">translated_text</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">.</span><span class="n">batch_decode</span><span class="p">(</span><span class="n">generated_tokens</span><span class="p">,</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">translated_text</span>

</code></pre></div></div>

<p>The translated test shows us that the model is working. The next step is to prepare the production model. 
To productionalize our model, we will deploy it to ONNX format.</p>

<h4 id="what-is-the-onnx-format">What is the ONNX format?</h4>

<p>ONNX stands for Open Neural Network Exchange. It is an open format built to represent machine learning models in a framework-agnostic way.</p>

<p>As you may know, neural networks are computation graphs with input, weights, and operations. [Cite the source here.]</p>

<p>ONNX format is a way of saving neural networks as computation graphs. That  computational graph represents the flow of data through the neural network.</p>

<p>The key benefits of saving neural networks in the ONNX format are interoperability and hardware access. Any deep learning platform can read a neural network saved in the ONNX format.  For example, a model trained in Pytorch can be exported to ONNX format and imported in Tensorflow and vice versa.</p>

<p>You don’t need to use Python to read a model saved as ONNX; you can use any programming language of your choice, such as Javascript, C, or C++.</p>

<p>ONNX makes the model easier to access hardware optimizations, and you can apply other optimizations, such as quantization, to your ONNX model.</p>

<p>Let us see how we can convert our model to ONNX format to use the full benefits of it.</p>

<p>Trying to export the model manually and see if we can load the model.</p>

<p>To export the model to onnx format we will be using the optimum cli from Huggingface.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="err">!</span> <span class="n">optimum</span><span class="o">-</span><span class="n">cli</span> <span class="n">export</span> <span class="n">onnx</span> <span class="o">--</span><span class="n">model</span> <span class="n">masakhane</span><span class="o">/</span><span class="n">m2m100_418M_en_swa_rel_news</span> <span class="o">--</span><span class="n">task</span> <span class="n">seq2seq</span><span class="o">-</span><span class="n">lm</span><span class="o">-</span><span class="k">with</span><span class="o">-</span><span class="n">past</span> <span class="o">--</span><span class="k">for</span><span class="o">-</span><span class="n">ort</span> <span class="n">onnx</span><span class="o">/</span><span class="n">m2m100_418M_en_swa_rel_news</span>

</code></pre></div></div>

<p>check if the model is correct</p>

<p>If the previous command was run successfully, we can see our model saved at <code class="language-bash highlighter-rouge">onnx/m2m100_418M_en_swa_rel_news</code>.</p>

<p>By checking the size we notice data our encoder model have 1.1 Gb, and our decoder model have 1.7Gb which make our model size to 2.8GB. Additionally, in the same folder we have the tokenizer data.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">pathlib</span> <span class="kn">import</span> <span class="n">Path</span>

</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">base_model_onnx_dir</span> <span class="o">=</span> <span class="n">Path</span><span class="p">.</span><span class="n">cwd</span><span class="p">().</span><span class="n">joinpath</span><span class="p">(</span><span class="s">'onnx'</span><span class="p">).</span><span class="n">joinpath</span><span class="p">(</span><span class="s">'m2m100_418M_en_swa_rel_news'</span><span class="p">)</span>

</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">base_model_onnx_dir</span><span class="p">.</span><span class="n">exists</span><span class="p">()</span>

</code></pre></div></div>

<h3 id="applying-quantization">Applying Quantization</h3>

<p>Quantization is the process of reducing the model size by using fewer bits to represent its parameters. Instead of using 32-bit precision floating points for most of the models, with quantization, we can use 12 bits to represent a number and consequently reduce the size of the model.</p>

<p>Smaller models resulting from quantization are faster to deploy and have low latency in production.</p>

<p>It has <a href="https://github.com/huggingface/notebooks/blob/main/examples/onnx-export.ipynb">been shown</a> that you can improve the inference time by 75% by using an ONNX quantized model without a considerable loss in performance. <Find more="" evidence="" for="" this=""></Find></p>

<p>For this tutorial, we will use quantization to reduce the size of our model for inference.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">optimum.onnxruntime</span> <span class="kn">import</span> <span class="n">ORTQuantizer</span><span class="p">,</span> <span class="n">ORTModelForSeq2SeqLM</span>
<span class="kn">from</span> <span class="nn">optimum.onnxruntime.configuration</span> <span class="kn">import</span> <span class="n">AutoQuantizationConfig</span>

</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">encoder_quantizer</span> <span class="o">=</span> <span class="n">ORTQuantizer</span><span class="p">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">base_model_onnx_dir</span><span class="p">,</span> <span class="n">file_name</span><span class="o">=</span><span class="s">"encoder_model.onnx"</span><span class="p">)</span>

</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">decoder_quantizer</span> <span class="o">=</span> <span class="n">ORTQuantizer</span><span class="p">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">base_model_onnx_dir</span><span class="p">,</span> <span class="n">file_name</span><span class="o">=</span><span class="s">"decoder_model.onnx"</span><span class="p">)</span>

</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">decoder_with_past_quantizer</span> <span class="o">=</span> <span class="n">ORTQuantizer</span><span class="p">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">base_model_onnx_dir</span><span class="p">,</span> <span class="n">file_name</span><span class="o">=</span><span class="s">"decoder_with_past_model.onnx"</span><span class="p">)</span>

</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">quantizers</span> <span class="o">=</span> <span class="p">[</span><span class="n">encoder_quantizer</span><span class="p">,</span> <span class="n">decoder_quantizer</span><span class="p">,</span> <span class="n">decoder_with_past_quantizer</span><span class="p">]</span>

</code></pre></div></div>

<p>We will use dynamic quantization to our model.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">dynamic_quantization_config</span> <span class="o">=</span> <span class="n">AutoQuantizationConfig</span><span class="p">.</span><span class="n">avx512_vnni</span><span class="p">(</span><span class="n">is_static</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">per_channel</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>

</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">quantized_model_path</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="s">"onnx"</span><span class="p">).</span><span class="n">joinpath</span><span class="p">(</span><span class="sa">f</span><span class="s">"</span><span class="si">{</span><span class="n">MODEL_SUFFIX</span><span class="si">}</span><span class="s">_quantized/"</span><span class="p">)</span>
<span class="n">quantized_model_path</span><span class="p">.</span><span class="n">mkdir</span><span class="p">(</span><span class="n">parents</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">exist_ok</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">for</span> <span class="n">quantizer</span> <span class="ow">in</span> <span class="n">quantizers</span><span class="p">:</span>
    <span class="n">quantizer</span><span class="p">.</span><span class="n">quantize</span><span class="p">(</span><span class="n">quantization_config</span><span class="o">=</span><span class="n">dynamic_quantization_config</span><span class="p">,</span> <span class="n">save_dir</span><span class="o">=</span><span class="n">quantized_model_path</span><span class="p">)</span>

</code></pre></div></div>

<p>Our model are save as quantized version, we can now check the size of the quantized models.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">for</span> <span class="n">model</span> <span class="ow">in</span> <span class="n">quantized_model_path</span><span class="p">.</span><span class="n">glob</span><span class="p">(</span><span class="s">"*.onnx"</span><span class="p">):</span>
    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"the size of </span><span class="si">{</span><span class="n">model</span><span class="p">.</span><span class="n">stem</span><span class="si">}</span><span class="s"> the model in MB is: </span><span class="si">{</span><span class="n">model</span><span class="p">.</span><span class="n">stat</span><span class="p">().</span><span class="n">st_size</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1024</span> <span class="o">*</span> <span class="mi">1024</span><span class="p">)</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>

</code></pre></div></div>

<p>We can see that we have managed to reduce the size of our initial models by two! From 1.6 Gb without quantization to 800 Mb with quantization. Let us see how to use the quantized model for inference.</p>

<h3 id="use-the-quantized-model">Use the quantized model</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">quantized_model_path</span> <span class="o">=</span> <span class="n">base_model_onnx_dir</span>

</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">quantized_model</span> <span class="o">=</span> <span class="n">ORTModelForSeq2SeqLM</span><span class="p">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">quantized_model_path</span><span class="p">,</span> 
                                                       <span class="n">decoder_file_name</span><span class="o">=</span><span class="s">'decoder_model_quantized.onnx'</span><span class="p">,</span>
                                                       <span class="n">encoder_file_name</span><span class="o">=</span><span class="s">'encoder_model_quantized.onnx'</span><span class="p">,)</span>

</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">quantized_pipeline</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">(</span><span class="s">"translation_en_to_sw"</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">quantized_model</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">)</span>

</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">translated_text_quantized</span> <span class="o">=</span> <span class="n">quantized_pipeline</span><span class="p">(</span><span class="n">text_to_translate</span><span class="p">)</span>

</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="n">translated_text_quantized</span><span class="p">)</span>

</code></pre></div></div>

<p>The quantization has reduced the size of the model, but it gave the same translation of our base text. We may need to run more extensive tests to find out what is the accuracy difference between our quantized model and the base model. Assuming the performance lost was not considerable, we move to the next step of our tutorial.</p>

<h2 id="deploy-the-model-for-inference">Deploy the Model for inference</h2>

<p>At this point, we have our model quantized and saved in ONNX format. We will now deploy it to a production server using the triton inference server. 
In the first section, we will deploy with the triton server as a docker container, and then we will use Kserve to deploy it to the Kubernetes deployment environment.</p>

<h2 id="triton-server">Triton Server</h2>

<p>Triton is a software tool for deploying machine learning models for inference. It is designed to produce high-quality inference across different hardware platforms, either GPU or CPU. It also supports inference across cloud, data center, and embedded devices.
One of the advantages of the triton server is that it supports dynamic batching and concurrent model execution.</p>

<ul>
  <li>Dynamic batching, for models that support batching, which is the case for deep learning models, triton implements scheduling and batching algorithms that combine individual requests to improve inference throughput.</li>
</ul>

<p><strong>Talk More about dynamic batching here…</strong></p>

<ul>
  <li>Concurrency model execution is the capacity to run simultaneously multiple models on the same GPU or various GPUs.</li>
</ul>

<h3 id="triton-server-backend">Triton Server Backend</h3>

<p>Triton supports different backends to execute the model. A backend is a wrapper around a deep learning framework like Pytorch, TensorFlow, TensorRT, or ONNX Runtime.
Two backend types interested us for this post: the Python Backend and the ONNX runtime backend.</p>

<p>The ONNX runtime backend executes ONNX models, and the Python backend allows the writing of the model logic in Python.</p>

<p>In this post, we will be focused on the ONNX and the Python backend.</p>

<p>I decided to go with the Python backend because I struggled to deploy the encoder decode model using an ensemble of the ONNX model. I still have a question in progress on <a href="https://stackoverflow.com/q/76638766/4683950">StackOverlow</a>.</p>

<h4 id="uploading-the-model-to-repository">Uploading the Model to Repository.</h4>

<p>The first step before using our model is to upload it to the model repository. For this tutorial,   we will be using our local storage as a model repository but later we will use static storage such as google cloud or AWS S3 to host our model.</p>

<h4 id="configuration">Configuration</h4>

<p>The first step to deploy our model in triton is to configure it.</p>

<p>The configuration sets up the model and defines the input shape and the output shape of our models.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># %load ./triton_model_repository/m2m100_translation_model/config.pbtxt
</span><span class="n">name</span><span class="p">:</span> <span class="s">"m2m100_translation_model"</span>
<span class="n">backend</span><span class="p">:</span> <span class="s">"python"</span>
<span class="n">max_batch_size</span><span class="p">:</span> <span class="mi">0</span>
<span class="nb">input</span> <span class="p">[</span>
  <span class="p">{</span>
    <span class="n">name</span><span class="p">:</span> <span class="s">"input_ids"</span>
    <span class="n">data_type</span><span class="p">:</span> <span class="n">TYPE_INT64</span>
    <span class="n">dims</span><span class="p">:</span> <span class="p">[</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span> <span class="p">]</span>
  <span class="p">},</span>
<span class="p">{</span>
    <span class="n">name</span><span class="p">:</span> <span class="s">"attention_mask"</span>
    <span class="n">data_type</span><span class="p">:</span> <span class="n">TYPE_INT64</span>
    <span class="n">dims</span><span class="p">:</span> <span class="p">[</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span> <span class="p">]</span>
  <span class="p">}</span>
<span class="p">]</span>
<span class="n">output</span> <span class="p">[</span>
    <span class="p">{</span>
    <span class="n">name</span><span class="p">:</span> <span class="s">"generated_indices"</span>
    <span class="n">data_type</span><span class="p">:</span> <span class="n">TYPE_FP32</span>
    <span class="n">dims</span><span class="p">:</span> <span class="p">[</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span> <span class="p">]</span>
  <span class="p">}</span>
<span class="p">]</span>

<span class="n">instance_group</span> <span class="p">[</span>
    <span class="p">{</span>
      <span class="n">count</span><span class="p">:</span> <span class="mi">1</span>
      <span class="n">kind</span><span class="p">:</span> <span class="n">KIND_CPU</span>
    <span class="p">}</span>
<span class="p">]</span>

</code></pre></div></div>

<p>In the above configuration, we can see that the model is expecting two inputs:  the input ids and the attention masks, and it returns the generated input indices. 
Additionally, we can notice that the model is running on a 1 CPU. If we had a GPU available, we would put it in the instance settings.</p>

<p>The input ids and the attention masks are the outputs from the tokenization process. The generated indices are the tokenized output that our tokenizer will decode.</p>

<p>The configuration file needs to be save at the root folder  of our model repository.</p>

<h4 id="create-the-load-model-script">Create the load model script</h4>

<p>The load model script is the python script that load our model before and run it for inference.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># %load ./triton_model_repository/m2m100_translation_model/1/model.py
</span><span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Dict</span><span class="p">,</span> <span class="n">List</span>
<span class="kn">import</span> <span class="nn">triton_python_backend_utils</span> <span class="k">as</span> <span class="n">pb_utils</span>
<span class="kn">from</span> <span class="nn">pathlib</span> <span class="kn">import</span> <span class="n">Path</span>
<span class="kn">from</span> <span class="nn">optimum.onnxruntime</span> <span class="kn">import</span> <span class="n">ORTModelForSeq2SeqLM</span>
<span class="kn">import</span> <span class="nn">torch</span>

<span class="n">TOKENIZER_SW_LANG_CODE_TO_ID</span> <span class="o">=</span> <span class="mi">128088</span>


<span class="k">class</span> <span class="nc">TritonPythonModel</span><span class="p">:</span>

    <span class="k">def</span> <span class="nf">initialize</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">args</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">str</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="s">"""
        Initialize the tokenization process
        :param args: arguments from Triton config file
        """</span>
        <span class="n">current_path</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="n">args</span><span class="p">[</span><span class="s">"model_repository"</span><span class="p">]).</span><span class="n">parent</span><span class="p">.</span><span class="n">absolute</span><span class="p">()</span>
        <span class="n">model_path</span> <span class="o">=</span> <span class="n">current_path</span><span class="p">.</span><span class="n">joinpath</span><span class="p">(</span><span class="s">"m2m100_translation_model"</span><span class="p">,</span> <span class="s">"1"</span><span class="p">,</span> <span class="s">"m2m100_418M_en_swa_rel_news_quantized"</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">device</span> <span class="o">=</span> <span class="s">"cpu"</span> <span class="k">if</span> <span class="n">args</span><span class="p">[</span><span class="s">"model_instance_kind"</span><span class="p">]</span> <span class="o">==</span> <span class="s">"CPU"</span> <span class="k">else</span> <span class="s">"cuda"</span>
        <span class="c1"># more variables in https://github.com/triton-inference-server/python_backend/blob/main/src/python.cc
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">ORTModelForSeq2SeqLM</span><span class="p">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_path</span><span class="p">,</span>
                                                          <span class="n">decoder_file_name</span><span class="o">=</span><span class="s">"decoder_model_quantized.onnx"</span><span class="p">,</span>
                                                          <span class="n">encoder_file_name</span><span class="o">=</span><span class="s">"encoder_model_quantized.onnx"</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">device</span> <span class="o">==</span> <span class="s">"cuda"</span><span class="p">:</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">model</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">model</span><span class="p">.</span><span class="n">cuda</span><span class="p">()</span>
        <span class="k">print</span><span class="p">(</span><span class="s">"TritonPythonModel initialized"</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">execute</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">requests</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s">"List[List[pb_utils.Tensor]]"</span><span class="p">:</span>
        <span class="s">"""
        Parse and tokenize each request
        :param requests: 1 or more requests received by Triton server.
        :return: text as input tensors
        """</span>
        <span class="n">responses</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="c1"># for loop for batch requests (disabled in our case)
</span>        <span class="k">for</span> <span class="n">request</span> <span class="ow">in</span> <span class="n">requests</span><span class="p">:</span>
            <span class="c1"># binary data typed back to string
</span>            <span class="n">input_ids</span> <span class="o">=</span> <span class="n">pb_utils</span><span class="p">.</span><span class="n">get_input_tensor_by_name</span><span class="p">(</span><span class="n">request</span><span class="p">,</span> <span class="s">"input_ids"</span><span class="p">).</span><span class="n">as_numpy</span><span class="p">()</span>
            <span class="n">attention_masks</span> <span class="o">=</span> <span class="n">pb_utils</span><span class="p">.</span><span class="n">get_input_tensor_by_name</span><span class="p">(</span><span class="n">request</span><span class="p">,</span> <span class="s">"attention_mask"</span><span class="p">).</span><span class="n">as_numpy</span><span class="p">()</span>
            <span class="n">input_ids</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">as_tensor</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">int64</span><span class="p">)</span>
            <span class="n">attention_masks</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">as_tensor</span><span class="p">(</span><span class="n">attention_masks</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">int64</span><span class="p">)</span>
            <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">device</span> <span class="o">==</span> <span class="s">"cuda"</span><span class="p">:</span>
                <span class="n">input_ids</span> <span class="o">=</span> <span class="n">input_ids</span><span class="p">.</span><span class="n">to</span><span class="p">(</span><span class="s">"cuda"</span><span class="p">)</span>
                <span class="n">attention_masks</span> <span class="o">=</span> <span class="n">attention_masks</span><span class="p">.</span><span class="n">to</span><span class="p">(</span><span class="s">"cuda"</span><span class="p">)</span>
            <span class="n">model_inputs</span> <span class="o">=</span> <span class="p">{</span><span class="s">"input_ids"</span><span class="p">:</span> <span class="n">input_ids</span><span class="p">,</span> <span class="s">"attention_mask"</span><span class="p">:</span> <span class="n">attention_masks</span><span class="p">}</span>
            <span class="n">generated_indices</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">model</span><span class="p">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">model_inputs</span><span class="p">,</span>
                                                    <span class="n">forced_bos_token_id</span><span class="o">=</span><span class="n">TOKENIZER_SW_LANG_CODE_TO_ID</span><span class="p">)</span>
            <span class="n">tensor_output</span> <span class="o">=</span> <span class="n">pb_utils</span><span class="p">.</span><span class="n">Tensor</span><span class="p">(</span><span class="s">"generated_indices"</span><span class="p">,</span> <span class="n">generated_indices</span><span class="p">.</span><span class="n">numpy</span><span class="p">())</span>
            <span class="n">responses</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">tensor_output</span><span class="p">)</span>
        <span class="n">responses</span> <span class="o">=</span> <span class="p">[</span><span class="n">pb_utils</span><span class="p">.</span><span class="n">InferenceResponse</span><span class="p">(</span><span class="n">output_tensors</span><span class="o">=</span><span class="n">responses</span><span class="p">)]</span>
        <span class="k">return</span> <span class="n">responses</span>
    
    <span class="k">def</span> <span class="nf">finalize</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="s">"""`finalize` is called only once when the model is being unloaded.
        Implementing `finalize` function is optional. This function allows
        the model to perform any necessary clean ups before exit.
        """</span>
        <span class="k">print</span><span class="p">(</span><span class="s">'Cleaning up...'</span><span class="p">)</span>

</code></pre></div></div>

<p>The model contains a class with two methods:</p>

<ul>
  <li>Initialize: The initialize method uses the ORT model to load the model in the memory!</li>
  <li>The execute method parse and tokenize each request received by the triton server. It calls the generate method on the input of the request and returns the generated text indices. This text will be later decoded by the tokenizer.</li>
</ul>

<p>If our configuration is done properly and the model is saved properly, we should have a model repository that looks like this:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>triton_model_repository
└── m2m100_translation_model
    ├── 1
    │   ├── m2m100_418M_en_swa_rel_news_quantized
    │   │   ├── config.json
    │   │   ├── decoder_model_quantized.onnx
    │   │   ├── decoder_with_past_model_quantized.onnx
    │   │   ├── encoder_model_quantized.onnx
    │   │   └── ort_config.json
    │   └── model.py
    └── config.pbtxt
</code></pre></div></div>

<p>Make sure that you have the file located at the precise location as me in order to be able to run the code.</p>

<h3 id="launching-the-docker-image">Launching the docker image</h3>

<p>If you look carefully at the code for our Python model, you can see that the model is importing the ONNX runtime! However, that runtime is not installed in the base triton server image. The reason why we decided to build our own image based on the triton server.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># %load Dockerfile
# Use the base image
</span><span class="n">FROM</span> <span class="n">nvcr</span><span class="p">.</span><span class="n">io</span><span class="o">/</span><span class="n">nvidia</span><span class="o">/</span><span class="n">tritonserver</span><span class="p">:</span><span class="mf">23.06</span><span class="o">-</span><span class="n">py3</span>

<span class="c1"># Install the required Python packages
</span><span class="n">RUN</span> <span class="n">pip</span> <span class="n">install</span> <span class="n">optimum</span><span class="o">==</span><span class="mf">1.9</span><span class="p">.</span><span class="mi">0</span> <span class="n">onnxruntime</span><span class="o">==</span><span class="mf">1.15</span><span class="p">.</span><span class="mi">1</span> <span class="n">onnx</span><span class="o">==</span><span class="mf">1.14</span><span class="p">.</span><span class="mi">0</span>

</code></pre></div></div>

<p>The above code shows how we build our docker image.
We use the base Tritonserver image, and then we add the different packages we need to run our model.</p>

<p>Next we can build our model using:</p>

<p><code class="language-bash highlighter-rouge">docker build <span class="nt">-t</span> espymur/triton-onnx:dev  <span class="nt">-f</span> Dockerfile .</code></p>

<p>Please note that the image is huge. Its size is around 15 GB. In the next post, I will try to optimize its size by using the technique suggested in the documentation.</p>

<p>If our model build is finished, we can now run the docker container that serves the model.</p>

<p><code class="language-bash highlighter-rouge">docker run <span class="nt">--rm</span> <span class="nt">-p</span> 8000:8000 <span class="nt">-p</span> 8001:8001 <span class="nt">-p</span> 8002:8002  <span class="nt">--shm-size</span> 128M <span class="nt">-v</span> <span class="k">${</span><span class="nv">PWD</span><span class="k">}</span>/triton_model_repository:/models  espymur/triton-onnx:dev tritonserver <span class="nt">--model-repository</span><span class="o">=</span>/models</code></p>

<ul>
  <li>
    <p>This command runs the docker container and map the port 8000, 8001, 8002 to 8000, 8001, and 8002 of our local machine.</p>
  </li>
  <li>
    <p>It then creates a volume that maps the <code class="language-bash highlighter-rouge"><span class="k">${</span><span class="nv">PWD</span><span class="k">}</span>/triton_model_repository</code> path from our local machine to /models in the container.</p>
  </li>
  <li>
    <p>It is also using a shared memory of 128 Mb.</p>
  </li>
</ul>

<p>With this model we can see that our model is running and we can perform inference without any problem.</p>

<p>At this point, we have got our model running inside the docker container, the next step will be to make inference requests. Let see how we can achieve that.</p>

<h3 id="making-inference-requests">Making Inference Requests</h3>

<p>The model is now updated and saved as a Triton backend model. We will apply tokenization offline and query the model with the tokenized words and the attention mask. 
The model will return the indices of the translated test; we will use the tokenizer again to decode the indices and produce the output.</p>

<p>We can later have the tokenizer as a separate service people can interact with using HTTP.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">MODEL_NAME</span> <span class="o">=</span> <span class="s">"masakhane/m2m100_418M_en_swa_rel_news"</span>

</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span>

</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="p">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">MODEL_NAME</span><span class="p">)</span>

</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">tritonclient.http</span> <span class="k">as</span> <span class="n">httpclient</span>

</code></pre></div></div>

<h4 id="the-http-client">The HTTP client</h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">client</span> <span class="o">=</span> <span class="n">httpclient</span><span class="p">.</span><span class="n">InferenceServerClient</span><span class="p">(</span><span class="n">url</span><span class="o">=</span><span class="s">"localhost:8000"</span><span class="p">)</span>

</code></pre></div></div>

<h4 id="the-inputs">The inputs</h4>

<p>This line creates the client object we will be using to interact with our server. To create the client object, we are passing the URL of the inference service as parameters.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">input_ids</span> <span class="o">=</span> <span class="n">httpclient</span><span class="p">.</span><span class="n">InferInput</span><span class="p">(</span><span class="s">"input_ids"</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span> <span class="p">,</span> <span class="n">datatype</span><span class="o">=</span><span class="s">"TYPE_INT64"</span><span class="p">,)</span>
<span class="n">attention_mask</span> <span class="o">=</span> <span class="n">httpclient</span><span class="p">.</span><span class="n">InferInput</span><span class="p">(</span><span class="s">"attention_mask"</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span> <span class="p">,</span> <span class="n">datatype</span><span class="o">=</span><span class="s">"TYPE_INT64"</span><span class="p">,)</span>

</code></pre></div></div>

<h3 id="the-outputs">The outputs.</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">outputs</span> <span class="o">=</span> <span class="n">httpclient</span><span class="p">.</span><span class="n">InferRequestedOutput</span><span class="p">(</span><span class="s">"generated_indices"</span><span class="p">,</span> <span class="n">binary_data</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>

</code></pre></div></div>

<p>To prepare our model input, we are using the Triton client library. 
The above code creates two objects for the input ID and the attention mask, respectively! We can specify the shape of the element and its datatype when creating the code.</p>

<p>Additionally to our inputs and outputs, we will need some utility function to perform the tokenization. Here are those functions:</p>

<h4 id="utilities-functions">Utilities Functions</h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">get_tokenizer</span><span class="p">(</span><span class="n">model_name</span><span class="p">):</span>
    <span class="s">"""Returns a tokenizer for a given model name

    Args:
        model_name (_type_): _description_

    Returns:
        _type_: _description_
    """</span>
    <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="p">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">tokenizer</span>

</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Tuple</span><span class="p">,</span> <span class="n">List</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>

</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span>

</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="k">def</span> <span class="nf">tokenize_text</span><span class="p">(</span><span class="n">tokenizer</span><span class="p">:</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">text</span><span class="p">:</span><span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="n">ndarray</span> <span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="n">ndarray</span><span class="p">]:</span>
    <span class="n">tokenized_text</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s">"np"</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">tokenized_text</span><span class="p">.</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">tokenized_text</span><span class="p">.</span><span class="n">attention_mask</span>

</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">generate_inference_input</span><span class="p">(</span><span class="n">input_ids</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">httpclient</span><span class="p">.</span><span class="n">InferInput</span><span class="p">]:</span>
    <span class="s">"""
    Generate inference inputs for Triton server

    Args:
        input_ids (np.ndarray): _description_
        attention_mask (np.ndarray): _description_

    Returns:
        List[httpclient.InferInput]: _description_
    """</span>
    <span class="n">inputs</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">inputs</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">httpclient</span><span class="p">.</span><span class="n">InferInput</span><span class="p">(</span><span class="s">"input_ids"</span><span class="p">,</span> <span class="n">input_ids</span><span class="p">.</span><span class="n">shape</span><span class="p">,</span> <span class="s">"INT64"</span><span class="p">))</span>
    <span class="n">inputs</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">httpclient</span><span class="p">.</span><span class="n">InferInput</span><span class="p">(</span><span class="s">"attention_mask"</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">.</span><span class="n">shape</span><span class="p">,</span> <span class="s">"INT64"</span><span class="p">))</span>

    <span class="n">inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">set_data_from_numpy</span><span class="p">(</span><span class="n">input_ids</span><span class="p">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">int64</span><span class="p">),</span> <span class="n">binary_data</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
    <span class="n">inputs</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="n">set_data_from_numpy</span><span class="p">(</span><span class="n">attention_mask</span><span class="p">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">int64</span><span class="p">),</span> <span class="n">binary_data</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">inputs</span>


</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">text</span> <span class="o">=</span> <span class="p">[</span><span class="s">"I am learning how to use Triton Server for Machine Learning"</span><span class="p">,</span> <span class="s">"Hello, my name is Espoir Murhabazi,  I am a Software Engineer from Congo DRC but living in UK"</span><span class="p">]</span>

</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">get_tokenizer</span><span class="p">(</span><span class="n">MODEL_NAME</span><span class="p">)</span>

</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">input_ids</span><span class="p">,</span> <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">tokenize_text</span><span class="p">(</span><span class="n">tokenizer</span><span class="p">,</span> <span class="n">text</span><span class="p">)</span>

</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">inference_inputs</span> <span class="o">=</span> <span class="n">generate_inference_input</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">)</span>

</code></pre></div></div>

<p>With our input prepared we can now make an inference request to our server. Here is the code we will be using to make the inference request.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">results</span> <span class="o">=</span> <span class="n">client</span><span class="p">.</span><span class="n">infer</span><span class="p">(</span><span class="n">model_name</span><span class="o">=</span><span class="s">"m2m100_translation_model"</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="n">inference_inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="n">outputs</span><span class="p">])</span>
<span class="n">inference_output</span> <span class="o">=</span> <span class="n">results</span><span class="p">.</span><span class="n">as_numpy</span><span class="p">(</span><span class="s">'generated_indices'</span><span class="p">)</span>

</code></pre></div></div>

<p>If everything goes as planned, we should be able to see the inference response.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">inference_output</span>

</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">decoded_output</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">.</span><span class="n">batch_decode</span><span class="p">(</span><span class="n">inference_output</span><span class="p">,</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">decoded_output</span>

</code></pre></div></div>

<p>With the decoded output, we can see that our inference server is working!</p>

<h3 id="conclusion">Conclusion</h3>

<p>In this post, we saw how we can start form a raw translation model from huggingface, we then quantized it to reduce it’s size, and finally deployed the model on a triton server to perform inference.
In the second part of this blog we will learn how to scale the whole prototype and build an end to end pipeline using kubernetes and Kserve.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
</code></pre></div></div>

    </div>

    
<hr>

<aside id="comments" class="disqus">
  <h3><i class="icon icon-comments-o"></i> Comments</h3>
  <div id="disqus_thread"></div>
  <script>
    var disqus_config = function() {
      this.page.url = 'http://localhost:4000/docker-m2m100-production-triton';
      this.page.identifier = '/docker-tutorial-using-triton-server';
    };
    (function() {
      var d = document,
      s = d.createElement('script');
      s.src = '//nandothemes.disqus.com/embed.js';
      s.setAttribute('data-timestamp', +new Date());
      (d.head || d.body).appendChild(s);
    })();
  </script>
  <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>
</aside>


  </div>

</article>

  </div>
</main>

<footer class="site-footer">
  <div class="container">
    <ul class="social">
  <li><a href="https://github.com/espoirMur" target="_blank"><i class="icon icon-github"></i></a></li>
  <li><a href="https://twitter.com/esp_py" target="_blank"><i class="icon icon-twitter"></i></a></li>
  <li><a href="https://www.linkedin.com/in/espoirMur" target="_blank"><i class="icon icon-linkedin"></i></a></li>
  <li><a href="https://stackoverflow.com/users/4683950/espoir-murhabazi" target="_blank"><i class="fa-brands fa-stack-overflow"></i></a></li>
</ul>

    <p class="txt-medium-gray">
      <small>&copy;2024 All rights reserved. Made with <a href="http://jekyllrb.com/" target="_blank">Jekyll</a> and ♥</small>
    </p>
  </div>
</footer>


</body>
</html>
