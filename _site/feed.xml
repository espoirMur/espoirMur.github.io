<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Espoir Murhabazi ideas' home</title>
    <description>Murhabazi Buzina Espoir Home on the internet</description>
    <link>http://localhost:4000/</link>
    <atom:link href="http://localhost:4000/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Fri, 19 Jan 2024 09:00:09 +0000</pubDate>
    <lastBuildDate>Fri, 19 Jan 2024 09:00:09 +0000</lastBuildDate>
    <generator>Jekyll v4.2.2</generator>
    
      <item>
        <title>Deploy a Transformer models for Machine Translation in production using the Triton Server.</title>
        <description>&lt;h3 id=&quot;how-to-prepare-an-encoder-decoder-transformer-model-to-production&quot;&gt;How to prepare an encoder decoder transformer model to production.&lt;/h3&gt;

&lt;h4 id=&quot;optimize-the-m2m100-model-with-onnx&quot;&gt;Optimize the M2M100 model with ONNX&lt;/h4&gt;

&lt;p&gt;In this series of posts, we will learn how to productionalize  a machine translation model. We will start from a HuggingFace transformer model and learn how to deploy it in a production setting and make it accessible to users.&lt;/p&gt;

&lt;p&gt;In the first notebook, we will learn how to prepare the model for production. We will load the model from the HuggingFace library and then quantize it; after quantization, we will use the triton server to deploy it in a docker container, and finally, we will learn how to make inference requests  to our model.&lt;/p&gt;

&lt;p&gt;In the second post,  we will learn how to scale the model using Kserve and how to optimize the first version of our model.&lt;/p&gt;

&lt;p&gt;This post is for Machine Learning Engineers/Enthusiasts with some knowledge of transformers models and Docker and who would like to learn how to deploy an encoder-decoder model in a production setting.&lt;/p&gt;

&lt;h2 id=&quot;environment-requirements&quot;&gt;Environment requirements&lt;/h2&gt;

&lt;p&gt;To run this code, you need to have &lt;code class=&quot;language-bash highlighter-rouge&quot;&gt;python 3.11&lt;/code&gt; installed on your local machine.&lt;/p&gt;

&lt;p&gt;You can install these libraries directly from your Python interpreter, or you can create a virtual environment to run Python. I would rather recommend using a Python interpreter from a virtual environment.&lt;/p&gt;

&lt;h3 id=&quot;install-libraries&quot;&gt;Install libraries&lt;/h3&gt;

&lt;p&gt;To install the useful libraries, you can use the following code:&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-bash highlighter-rouge&quot;&gt;pip &lt;span class=&quot;nv&quot;&gt;transformers&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;4.30.2 &lt;span class=&quot;nv&quot;&gt;optimum&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;1.9.0 &lt;span class=&quot;nv&quot;&gt;onnxruntime&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;1.15.1 &lt;span class=&quot;nv&quot;&gt;onnx&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;1.14.0&lt;/code&gt;&lt;/p&gt;

&lt;h1 id=&quot;a-brief-history-of-the-m2m100-model&quot;&gt;A brief history of the M2M100 Model&lt;/h1&gt;

&lt;h3 id=&quot;encoder-decoder-model&quot;&gt;Encoder-Decoder model&lt;/h3&gt;

&lt;p&gt;Encoder-decoder models are large language models built with two components: the encoder and the decoder. They are used for natural language processing tasks that involve understanding input sequences and generating output sequences with different lengths and structures.&lt;/p&gt;

&lt;p&gt;The encoder is a neural network that takes a variable-length sequence as an input and transforms it into a  vector representation.  For our machine translation task, the encoder takes the token in the source language and returns a vector representation of the source language.&lt;/p&gt;

&lt;p&gt;The decoder, on the other hand, is also a neural network  that takes the vector representation of the source text and generates the translation in the target language.&lt;/p&gt;

&lt;p&gt;IMAGE HERE&lt;/p&gt;

&lt;p&gt;You can learn more about transformer models and encoder-decoder models, particularly &lt;a href=&quot;https://jalammar.github.io/illustrated-transformer/&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;the-m2m100-model&quot;&gt;The M2M100 model:&lt;/h3&gt;

&lt;p&gt;The M2M100 stands for Many to Many multilingual translation model that can translate between any pair of 100 languages it was trained on. It helps to alleviate the fact that most machine translation training is done from or to English. You can learn more about the M2M100 model &lt;a href=&quot;https://huggingface.co/docs/transformers/model_doc/m2m_100&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;It was trained to translate English to Swahili. Why did I pick Swahili? Because I am a native Swahili speaker.&lt;/p&gt;

&lt;h2 id=&quot;testing-the-raw-model&quot;&gt;Testing the raw model&lt;/h2&gt;

&lt;p&gt;We will start by loading our model from the huggingface repository!
The below code will load the model from the HuggingFace library and perform a translation inference by using the generate method.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Briefly talk about the encoder-decoder architecture and particularity  of M2M100.&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;transformers&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;AutoTokenizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;M2M100ForConditionalGeneration&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pipeline&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;MODEL_NAME&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;masakhane/m2m100_418M_en_swa_rel_news&quot;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;M2M100ForConditionalGeneration&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;M2M100ForConditionalGeneration&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;from_pretrained&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;MODEL_NAME&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;tokenizer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;AutoTokenizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;from_pretrained&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;MODEL_NAME&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;text_to_translate&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;Hello, my name is Espoir Murhabazi,  I am a Software Engineer from Congo DRC but living in UK&quot;&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;model_input&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tokenizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;text_to_translate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;return_tensors&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;pt&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;model_input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;keys&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;generated_tokens&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;generate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model_input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;forced_bos_token_id&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tokenizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lang_code_to_id&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;sw&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;At this point our model have generate the translation token, the next step is to use our tokenizer to convert back the token to the text. This is called decoding.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;translated_text&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tokenizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;batch_decode&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;generated_tokens&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;skip_special_tokens&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;translated_text&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The translated test shows us that the model is working. The next step is to prepare the production model. 
To productionalize our model, we will deploy it to ONNX format.&lt;/p&gt;

&lt;h4 id=&quot;what-is-the-onnx-format&quot;&gt;What is the ONNX format?&lt;/h4&gt;

&lt;p&gt;ONNX stands for Open Neural Network Exchange. It is an open format built to represent machine learning models in a framework-agnostic way.&lt;/p&gt;

&lt;p&gt;As you may know, neural networks are computation graphs with input, weights, and operations. [Cite the source here.]&lt;/p&gt;

&lt;p&gt;ONNX format is a way of saving neural networks as computation graphs. That  computational graph represents the flow of data through the neural network.&lt;/p&gt;

&lt;p&gt;The key benefits of saving neural networks in the ONNX format are interoperability and hardware access. Any deep learning platform can read a neural network saved in the ONNX format.  For example, a model trained in Pytorch can be exported to ONNX format and imported in Tensorflow and vice versa.&lt;/p&gt;

&lt;p&gt;You don’t need to use Python to read a model saved as ONNX; you can use any programming language of your choice, such as Javascript, C, or C++.&lt;/p&gt;

&lt;p&gt;ONNX makes the model easier to access hardware optimizations, and you can apply other optimizations, such as quantization, to your ONNX model.&lt;/p&gt;

&lt;p&gt;Let us see how we can convert our model to ONNX format to use the full benefits of it.&lt;/p&gt;

&lt;p&gt;Trying to export the model manually and see if we can load the model.&lt;/p&gt;

&lt;p&gt;To export the model to onnx format we will be using the optimum cli from Huggingface.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;err&quot;&gt;!&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;optimum&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cli&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;export&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;onnx&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;masakhane&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;m2m100_418M_en_swa_rel_news&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;task&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;seq2seq&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lm&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;with&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;past&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ort&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;onnx&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;m2m100_418M_en_swa_rel_news&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;check if the model is correct&lt;/p&gt;

&lt;p&gt;If the previous command was run successfully, we can see our model saved at &lt;code class=&quot;language-bash highlighter-rouge&quot;&gt;onnx/m2m100_418M_en_swa_rel_news&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;By checking the size we notice data our encoder model have 1.1 Gb, and our decoder model have 1.7Gb which make our model size to 2.8GB. Additionally, in the same folder we have the tokenizer data.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;pathlib&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Path&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;base_model_onnx_dir&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cwd&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;().&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;joinpath&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'onnx'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;joinpath&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'m2m100_418M_en_swa_rel_news'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;base_model_onnx_dir&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;exists&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;applying-quantization&quot;&gt;Applying Quantization&lt;/h3&gt;

&lt;p&gt;Quantization is the process of reducing the model size by using fewer bits to represent its parameters. Instead of using 32-bit precision floating points for most of the models, with quantization, we can use 12 bits to represent a number and consequently reduce the size of the model.&lt;/p&gt;

&lt;p&gt;Smaller models resulting from quantization are faster to deploy and have low latency in production.&lt;/p&gt;

&lt;p&gt;It has &lt;a href=&quot;https://github.com/huggingface/notebooks/blob/main/examples/onnx-export.ipynb&quot;&gt;been shown&lt;/a&gt; that you can improve the inference time by 75% by using an ONNX quantized model without a considerable loss in performance. &lt;Find more=&quot;&quot; evidence=&quot;&quot; for=&quot;&quot; this=&quot;&quot;&gt;&lt;/Find&gt;&lt;/p&gt;

&lt;p&gt;For this tutorial, we will use quantization to reduce the size of our model for inference.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;optimum.onnxruntime&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ORTQuantizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ORTModelForSeq2SeqLM&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;optimum.onnxruntime.configuration&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;AutoQuantizationConfig&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;encoder_quantizer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ORTQuantizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;from_pretrained&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;base_model_onnx_dir&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;file_name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;encoder_model.onnx&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;decoder_quantizer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ORTQuantizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;from_pretrained&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;base_model_onnx_dir&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;file_name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;decoder_model.onnx&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;decoder_with_past_quantizer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ORTQuantizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;from_pretrained&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;base_model_onnx_dir&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;file_name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;decoder_with_past_model.onnx&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;quantizers&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;encoder_quantizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;decoder_quantizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;decoder_with_past_quantizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We will use dynamic quantization to our model.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;dynamic_quantization_config&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;AutoQuantizationConfig&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;avx512_vnni&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;is_static&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;per_channel&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;quantized_model_path&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;onnx&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;joinpath&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;MODEL_SUFFIX&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;_quantized/&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;quantized_model_path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mkdir&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;parents&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;exist_ok&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;quantizer&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;quantizers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;quantizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;quantize&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;quantization_config&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dynamic_quantization_config&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;save_dir&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;quantized_model_path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Our model are save as quantized version, we can now check the size of the quantized models.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;quantized_model_path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;glob&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;*.onnx&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;the size of &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stem&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; the model in MB is: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;().&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;st_size&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1024&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1024&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We can see that we have managed to reduce the size of our initial models by two! From 1.6 Gb without quantization to 800 Mb with quantization. Let us see how to use the quantized model for inference.&lt;/p&gt;

&lt;h3 id=&quot;use-the-quantized-model&quot;&gt;Use the quantized model&lt;/h3&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;quantized_model_path&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;base_model_onnx_dir&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;quantized_model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ORTModelForSeq2SeqLM&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;from_pretrained&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;quantized_model_path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
                                                       &lt;span class=&quot;n&quot;&gt;decoder_file_name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'decoder_model_quantized.onnx'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                                                       &lt;span class=&quot;n&quot;&gt;encoder_file_name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'encoder_model_quantized.onnx'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,)&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;quantized_pipeline&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pipeline&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;translation_en_to_sw&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;quantized_model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tokenizer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tokenizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;translated_text_quantized&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;quantized_pipeline&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;text_to_translate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;translated_text_quantized&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The quantization has reduced the size of the model, but it gave the same translation of our base text. We may need to run more extensive tests to find out what is the accuracy difference between our quantized model and the base model. Assuming the performance lost was not considerable, we move to the next step of our tutorial.&lt;/p&gt;

&lt;h2 id=&quot;deploy-the-model-for-inference&quot;&gt;Deploy the Model for inference&lt;/h2&gt;

&lt;p&gt;At this point, we have our model quantized and saved in ONNX format. We will now deploy it to a production server using the triton inference server. 
In the first section, we will deploy with the triton server as a docker container, and then we will use Kserve to deploy it to the Kubernetes deployment environment.&lt;/p&gt;

&lt;h2 id=&quot;triton-server&quot;&gt;Triton Server&lt;/h2&gt;

&lt;p&gt;Triton is a software tool for deploying machine learning models for inference. It is designed to produce high-quality inference across different hardware platforms, either GPU or CPU. It also supports inference across cloud, data center, and embedded devices.
One of the advantages of the triton server is that it supports dynamic batching and concurrent model execution.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Dynamic batching, for models that support batching, which is the case for deep learning models, triton implements scheduling and batching algorithms that combine individual requests to improve inference throughput.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Talk More about dynamic batching here…&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Concurrency model execution is the capacity to run simultaneously multiple models on the same GPU or various GPUs.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;triton-server-backend&quot;&gt;Triton Server Backend&lt;/h3&gt;

&lt;p&gt;Triton supports different backends to execute the model. A backend is a wrapper around a deep learning framework like Pytorch, TensorFlow, TensorRT, or ONNX Runtime.
Two backend types interested us for this post: the Python Backend and the ONNX runtime backend.&lt;/p&gt;

&lt;p&gt;The ONNX runtime backend executes ONNX models, and the Python backend allows the writing of the model logic in Python.&lt;/p&gt;

&lt;p&gt;In this post, we will be focused on the ONNX and the Python backend.&lt;/p&gt;

&lt;p&gt;I decided to go with the Python backend because I struggled to deploy the encoder decode model using an ensemble of the ONNX model. I still have a question in progress on &lt;a href=&quot;https://stackoverflow.com/q/76638766/4683950&quot;&gt;StackOverlow&lt;/a&gt;.&lt;/p&gt;

&lt;h4 id=&quot;uploading-the-model-to-repository&quot;&gt;Uploading the Model to Repository.&lt;/h4&gt;

&lt;p&gt;The first step before using our model is to upload it to the model repository. For this tutorial,   we will be using our local storage as a model repository but later we will use static storage such as google cloud or AWS S3 to host our model.&lt;/p&gt;

&lt;h4 id=&quot;configuration&quot;&gt;Configuration&lt;/h4&gt;

&lt;p&gt;The first step to deploy our model in triton is to configure it.&lt;/p&gt;

&lt;p&gt;The configuration sets up the model and defines the input shape and the output shape of our models.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# %load ./triton_model_repository/m2m100_translation_model/config.pbtxt
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;m2m100_translation_model&quot;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;backend&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;python&quot;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;max_batch_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;input&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;input_ids&quot;&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;data_type&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;TYPE_INT64&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;dims&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;attention_mask&quot;&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;data_type&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;TYPE_INT64&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;dims&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;output&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;generated_indices&quot;&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;data_type&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;TYPE_FP32&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;dims&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;instance_group&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
      &lt;span class=&quot;n&quot;&gt;count&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
      &lt;span class=&quot;n&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;KIND_CPU&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;In the above configuration, we can see that the model is expecting two inputs:  the input ids and the attention masks, and it returns the generated input indices. 
Additionally, we can notice that the model is running on a 1 CPU. If we had a GPU available, we would put it in the instance settings.&lt;/p&gt;

&lt;p&gt;The input ids and the attention masks are the outputs from the tokenization process. The generated indices are the tokenized output that our tokenizer will decode.&lt;/p&gt;

&lt;p&gt;The configuration file needs to be save at the root folder  of our model repository.&lt;/p&gt;

&lt;h4 id=&quot;create-the-load-model-script&quot;&gt;Create the load model script&lt;/h4&gt;

&lt;p&gt;The load model script is the python script that load our model before and run it for inference.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# %load ./triton_model_repository/m2m100_translation_model/1/model.py
&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;typing&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Dict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;List&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;triton_python_backend_utils&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pb_utils&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;pathlib&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Path&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;optimum.onnxruntime&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ORTModelForSeq2SeqLM&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;TOKENIZER_SW_LANG_CODE_TO_ID&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;128088&lt;/span&gt;


&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;TritonPythonModel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;initialize&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;args&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Dict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot;
        Initialize the tokenization process
        :param args: arguments from Triton config file
        &quot;&quot;&quot;&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;current_path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;args&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;model_repository&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;parent&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;absolute&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;model_path&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;current_path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;joinpath&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;m2m100_translation_model&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;1&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;m2m100_418M_en_swa_rel_news_quantized&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;device&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;cpu&quot;&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;args&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;model_instance_kind&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;CPU&quot;&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;cuda&quot;&lt;/span&gt;
        &lt;span class=&quot;c1&quot;&gt;# more variables in https://github.com/triton-inference-server/python_backend/blob/main/src/python.cc
&lt;/span&gt;        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ORTModelForSeq2SeqLM&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;from_pretrained&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model_path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                                                          &lt;span class=&quot;n&quot;&gt;decoder_file_name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;decoder_model_quantized.onnx&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                                                          &lt;span class=&quot;n&quot;&gt;encoder_file_name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;encoder_model_quantized.onnx&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;device&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;cuda&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cuda&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;TritonPythonModel initialized&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;execute&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;requests&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;List[List[pb_utils.Tensor]]&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot;
        Parse and tokenize each request
        :param requests: 1 or more requests received by Triton server.
        :return: text as input tensors
        &quot;&quot;&quot;&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;responses&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
        &lt;span class=&quot;c1&quot;&gt;# for loop for batch requests (disabled in our case)
&lt;/span&gt;        &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;request&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;requests&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;c1&quot;&gt;# binary data typed back to string
&lt;/span&gt;            &lt;span class=&quot;n&quot;&gt;input_ids&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pb_utils&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_input_tensor_by_name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;request&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;input_ids&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;as_numpy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;attention_masks&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pb_utils&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_input_tensor_by_name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;request&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;attention_mask&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;as_numpy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;input_ids&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;as_tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;input_ids&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dtype&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;int64&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;attention_masks&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;as_tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;attention_masks&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dtype&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;int64&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;device&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;cuda&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;input_ids&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;input_ids&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;to&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;cuda&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;attention_masks&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;attention_masks&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;to&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;cuda&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;model_inputs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;input_ids&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;input_ids&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;attention_mask&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;attention_masks&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;generated_indices&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;generate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model_inputs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                                                    &lt;span class=&quot;n&quot;&gt;forced_bos_token_id&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;TOKENIZER_SW_LANG_CODE_TO_ID&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;tensor_output&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pb_utils&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;generated_indices&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;generated_indices&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;numpy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;responses&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tensor_output&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;responses&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pb_utils&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;InferenceResponse&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;output_tensors&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;responses&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;responses&lt;/span&gt;
    
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;finalize&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot;`finalize` is called only once when the model is being unloaded.
        Implementing `finalize` function is optional. This function allows
        the model to perform any necessary clean ups before exit.
        &quot;&quot;&quot;&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Cleaning up...'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The model contains a class with two methods:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Initialize: The initialize method uses the ORT model to load the model in the memory!&lt;/li&gt;
  &lt;li&gt;The execute method parse and tokenize each request received by the triton server. It calls the generate method on the input of the request and returns the generated text indices. This text will be later decoded by the tokenizer.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;If our configuration is done properly and the model is saved properly, we should have a model repository that looks like this:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;triton_model_repository
└── m2m100_translation_model
    ├── 1
    │   ├── m2m100_418M_en_swa_rel_news_quantized
    │   │   ├── config.json
    │   │   ├── decoder_model_quantized.onnx
    │   │   ├── decoder_with_past_model_quantized.onnx
    │   │   ├── encoder_model_quantized.onnx
    │   │   └── ort_config.json
    │   └── model.py
    └── config.pbtxt
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Make sure that you have the file located at the precise location as me in order to be able to run the code.&lt;/p&gt;

&lt;h3 id=&quot;launching-the-docker-image&quot;&gt;Launching the docker image&lt;/h3&gt;

&lt;p&gt;If you look carefully at the code for our Python model, you can see that the model is importing the ONNX runtime! However, that runtime is not installed in the base triton server image. The reason why we decided to build our own image based on the triton server.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# %load Dockerfile
# Use the base image
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;FROM&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nvcr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;io&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nvidia&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tritonserver&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;23.06&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;py3&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Install the required Python packages
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;RUN&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pip&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;install&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;optimum&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1.9&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;onnxruntime&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1.15&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;onnx&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1.14&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The above code shows how we build our docker image.
We use the base Tritonserver image, and then we add the different packages we need to run our model.&lt;/p&gt;

&lt;p&gt;Next we can build our model using:&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-bash highlighter-rouge&quot;&gt;docker build &lt;span class=&quot;nt&quot;&gt;-t&lt;/span&gt; espymur/triton-onnx:dev  &lt;span class=&quot;nt&quot;&gt;-f&lt;/span&gt; Dockerfile .&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Please note that the image is huge. Its size is around 15 GB. In the next post, I will try to optimize its size by using the technique suggested in the documentation.&lt;/p&gt;

&lt;p&gt;If our model build is finished, we can now run the docker container that serves the model.&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-bash highlighter-rouge&quot;&gt;docker run &lt;span class=&quot;nt&quot;&gt;--rm&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-p&lt;/span&gt; 8000:8000 &lt;span class=&quot;nt&quot;&gt;-p&lt;/span&gt; 8001:8001 &lt;span class=&quot;nt&quot;&gt;-p&lt;/span&gt; 8002:8002  &lt;span class=&quot;nt&quot;&gt;--shm-size&lt;/span&gt; 128M &lt;span class=&quot;nt&quot;&gt;-v&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;PWD&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt;/triton_model_repository:/models  espymur/triton-onnx:dev tritonserver &lt;span class=&quot;nt&quot;&gt;--model-repository&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;/models&lt;/code&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;This command runs the docker container and map the port 8000, 8001, 8002 to 8000, 8001, and 8002 of our local machine.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;It then creates a volume that maps the &lt;code class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;PWD&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt;/triton_model_repository&lt;/code&gt; path from our local machine to /models in the container.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;It is also using a shared memory of 128 Mb.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;With this model we can see that our model is running and we can perform inference without any problem.&lt;/p&gt;

&lt;p&gt;At this point, we have got our model running inside the docker container, the next step will be to make inference requests. Let see how we can achieve that.&lt;/p&gt;

&lt;h3 id=&quot;making-inference-requests&quot;&gt;Making Inference Requests&lt;/h3&gt;

&lt;p&gt;The model is now updated and saved as a Triton backend model. We will apply tokenization offline and query the model with the tokenized words and the attention mask. 
The model will return the indices of the translated test; we will use the tokenizer again to decode the indices and produce the output.&lt;/p&gt;

&lt;p&gt;We can later have the tokenizer as a separate service people can interact with using HTTP.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;MODEL_NAME&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;masakhane/m2m100_418M_en_swa_rel_news&quot;&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;transformers&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;AutoTokenizer&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;tokenizer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;AutoTokenizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;from_pretrained&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;MODEL_NAME&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;tritonclient.http&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;httpclient&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;the-http-client&quot;&gt;The HTTP client&lt;/h4&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;client&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;httpclient&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;InferenceServerClient&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;url&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;localhost:8000&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;the-inputs&quot;&gt;The inputs&lt;/h4&gt;

&lt;p&gt;This line creates the client object we will be using to interact with our server. To create the client object, we are passing the URL of the inference service as parameters.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;input_ids&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;httpclient&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;InferInput&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;input_ids&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;datatype&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;TYPE_INT64&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;attention_mask&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;httpclient&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;InferInput&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;attention_mask&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;datatype&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;TYPE_INT64&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,)&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;the-outputs&quot;&gt;The outputs.&lt;/h3&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;outputs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;httpclient&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;InferRequestedOutput&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;generated_indices&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;binary_data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;To prepare our model input, we are using the Triton client library. 
The above code creates two objects for the input ID and the attention mask, respectively! We can specify the shape of the element and its datatype when creating the code.&lt;/p&gt;

&lt;p&gt;Additionally to our inputs and outputs, we will need some utility function to perform the tokenization. Here are those functions:&lt;/p&gt;

&lt;h4 id=&quot;utilities-functions&quot;&gt;Utilities Functions&lt;/h4&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;get_tokenizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model_name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot;Returns a tokenizer for a given model name

    Args:
        model_name (_type_): _description_

    Returns:
        _type_: _description_
    &quot;&quot;&quot;&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;tokenizer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;AutoTokenizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;from_pretrained&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model_name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tokenizer&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;typing&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Tuple&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;List&lt;/span&gt;

&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;transformers&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;AutoTokenizer&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;tokenize_text&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tokenizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;AutoTokenizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;text&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Tuple&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ndarray&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ndarray&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;tokenized_text&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tokenizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;text&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;padding&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;return_tensors&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;np&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tokenized_text&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;input_ids&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tokenized_text&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;attention_mask&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;generate_inference_input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;input_ids&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ndarray&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;attention_mask&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ndarray&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;List&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;httpclient&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;InferInput&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]:&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot;
    Generate inference inputs for Triton server

    Args:
        input_ids (np.ndarray): _description_
        attention_mask (np.ndarray): _description_

    Returns:
        List[httpclient.InferInput]: _description_
    &quot;&quot;&quot;&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;inputs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;inputs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;httpclient&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;InferInput&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;input_ids&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;input_ids&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;INT64&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;inputs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;httpclient&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;InferInput&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;attention_mask&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;attention_mask&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;INT64&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;inputs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;set_data_from_numpy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;input_ids&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;astype&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;int64&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;binary_data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;inputs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;set_data_from_numpy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;attention_mask&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;astype&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;int64&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;binary_data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;inputs&lt;/span&gt;


&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;text&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;I am learning how to use Triton Server for Machine Learning&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;Hello, my name is Espoir Murhabazi,  I am a Software Engineer from Congo DRC but living in UK&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;tokenizer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;get_tokenizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;MODEL_NAME&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;input_ids&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;attention_mask&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tokenize_text&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tokenizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;text&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;inference_inputs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;generate_inference_input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;input_ids&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;attention_mask&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;With our input prepared we can now make an inference request to our server. Here is the code we will be using to make the inference request.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;results&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;client&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;infer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model_name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;m2m100_translation_model&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;inputs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;inference_inputs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;outputs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;outputs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;inference_output&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;results&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;as_numpy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'generated_indices'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;If everything goes as planned, we should be able to see the inference response.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;inference_output&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;decoded_output&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tokenizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;batch_decode&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;inference_output&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;skip_special_tokens&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;decoded_output&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;With the decoded output, we can see that our inference server is working!&lt;/p&gt;

&lt;h3 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h3&gt;

&lt;p&gt;In this post, we saw how we can start form a raw translation model from huggingface, we then quantized it to reduce it’s size, and finally deployed the model on a triton server to perform inference.
In the second part of this blog we will learn how to scale the whole prototype and build an end to end pipeline using kubernetes and Kserve.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
</description>
        <pubDate>Fri, 19 Jan 2024 07:03:59 +0000</pubDate>
        <link>http://localhost:4000/docker-m2m100-production-triton</link>
        <guid isPermaLink="true">http://localhost:4000/docker-m2m100-production-triton</guid>
        
        <category>docker,</category>
        
        <category>transformer,</category>
        
        <category>devops,</category>
        
        <category>translation</category>
        
        
      </item>
    
      <item>
        <title>Docker Tutorial Using Triton Server</title>
        <description>{
 &quot;cells&quot;: [
  {
   &quot;cell_type&quot;: &quot;markdown&quot;,
   &quot;metadata&quot;: {},
   &quot;source&quot;: [
    &quot;### How to prepare an encoder decoder transformer model to production.&quot;
   ]
  },
  {
   &quot;attachments&quot;: {},
   &quot;cell_type&quot;: &quot;markdown&quot;,
   &quot;metadata&quot;: {},
   &quot;source&quot;: [
    &quot;#### Optimize the M2M100 model with ONNX\n&quot;,
    &quot;\n&quot;,
    &quot;\n&quot;,
    &quot;In this series of posts, we will learn how to productionalize  a machine translation model. We will start from a HuggingFace transformer model and learn how to deploy it in a production setting and make it accessible to users.\n&quot;,
    &quot;\n&quot;,
    &quot;In the first notebook, we will learn how to prepare the model for production. We will load the model from the HuggingFace library and then quantize it; after quantization, we will use the triton server to deploy it in a docker container, and finally, we will learn how to make inference requests  to our model.\n&quot;,
    &quot;\n&quot;,
    &quot;In the second post,  we will learn how to scale the model using Kserve and how to optimize the first version of our model.\n&quot;,
    &quot;\n&quot;,
    &quot;This post is for Machine Learning Engineers/Enthusiasts with some knowledge of transformers models and Docker and who would like to learn how to deploy an encoder-decoder model in a production setting.\n&quot;,
    &quot;\n&quot;
   ]
  },
  {
   &quot;cell_type&quot;: &quot;markdown&quot;,
   &quot;metadata&quot;: {},
   &quot;source&quot;: [
    &quot;## Environment requirements\n&quot;,
    &quot;\n&quot;,
    &quot;To run this code, you need to have `python 3.11` installed on your local machine. \n&quot;,
    &quot;\n&quot;,
    &quot;You can install these libraries directly from your Python interpreter, or you can create a virtual environment to run Python. I would rather recommend using a Python interpreter from a virtual environment.\n&quot;,
    &quot;\n&quot;,
    &quot;### Install libraries\n&quot;,
    &quot;\n&quot;,
    &quot;To install the useful libraries, you can use the following code: \n&quot;,
    &quot;\n&quot;,
    &quot;`pip transformers==4.30.2 optimum==1.9.0 onnxruntime==1.15.1 onnx==1.14.0`\n&quot;,
    &quot;\n&quot;,
    &quot;# A brief history of the M2M100 Model\n&quot;,
    &quot;\n&quot;,
    &quot;### Encoder-Decoder model \n&quot;,
    &quot;\n&quot;,
    &quot;Encoder-decoder models are large language models built with two components: the encoder and the decoder. They are used for natural language processing tasks that involve understanding input sequences and generating output sequences with different lengths and structures.\n&quot;,
    &quot;\n&quot;,
    &quot;The encoder is a neural network that takes a variable-length sequence as an input and transforms it into a  vector representation.  For our machine translation task, the encoder takes the token in the source language and returns a vector representation of the source language. \n&quot;,
    &quot;\n&quot;,
    &quot;The decoder, on the other hand, is also a neural network  that takes the vector representation of the source text and generates the translation in the target language.\n&quot;,
    &quot;\n&quot;,
    &quot;&lt;Put the Image here.&gt;\n&quot;,
    &quot;You can learn more about transformer models and encoder-decoder models, particularly [here](https://jalammar.github.io/illustrated-transformer/)\n&quot;,
    &quot;\n&quot;,
    &quot;\n&quot;,
    &quot;### The M2M100 model:\n&quot;,
    &quot;\n&quot;,
    &quot;The M2M100 stands for Many to Many multilingual translation model that can translate between any pair of 100 languages it was trained on. It helps to alleviate the fact that most machine translation training is done from or to English. You can learn more about the M2M100 model [here](https://huggingface.co/docs/transformers/model_doc/m2m_100).\n&quot;,
    &quot;\n&quot;,
    &quot;\n&quot;,
    &quot;\n&quot;,
    &quot;\n&quot;,
    &quot;It was trained to translate English to Swahili. Why did I pick Swahili? Because I am a native Swahili speaker. \n&quot;,
    &quot;\n&quot;,
    &quot;## Testing the raw model\n&quot;,
    &quot;\n&quot;,
    &quot;We will start by loading our model from the huggingface repository!\n&quot;,
    &quot;The below code will load the model from the HuggingFace library and perform a translation inference by using the generate method.\n&quot;,
    &quot;\n&quot;,
    &quot;&lt;Briefly talk about the encoder-decoder architecture and particularity  of M2M100.&gt;&quot;
   ]
  },
  {
   &quot;cell_type&quot;: &quot;code&quot;,
   &quot;execution_count&quot;: null,
   &quot;metadata&quot;: {},
   &quot;outputs&quot;: [],
   &quot;source&quot;: [
    &quot;from transformers import AutoTokenizer, M2M100ForConditionalGeneration, pipeline\n&quot;
   ]
  },
  {
   &quot;cell_type&quot;: &quot;code&quot;,
   &quot;execution_count&quot;: null,
   &quot;metadata&quot;: {},
   &quot;outputs&quot;: [],
   &quot;source&quot;: [
    &quot;MODEL_NAME = \&quot;masakhane/m2m100_418M_en_swa_rel_news\&quot;\n&quot;
   ]
  },
  {
   &quot;cell_type&quot;: &quot;code&quot;,
   &quot;execution_count&quot;: null,
   &quot;metadata&quot;: {},
   &quot;outputs&quot;: [],
   &quot;source&quot;: [
    &quot;model: M2M100ForConditionalGeneration = M2M100ForConditionalGeneration.from_pretrained(MODEL_NAME)\n&quot;,
    &quot;tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n&quot;
   ]
  },
  {
   &quot;cell_type&quot;: &quot;code&quot;,
   &quot;execution_count&quot;: null,
   &quot;metadata&quot;: {},
   &quot;outputs&quot;: [],
   &quot;source&quot;: [
    &quot;text_to_translate = \&quot;Hello, my name is Espoir Murhabazi,  I am a Software Engineer from Congo DRC but living in UK\&quot;\n&quot;
   ]
  },
  {
   &quot;cell_type&quot;: &quot;code&quot;,
   &quot;execution_count&quot;: null,
   &quot;metadata&quot;: {},
   &quot;outputs&quot;: [],
   &quot;source&quot;: [
    &quot;model_input = tokenizer(text_to_translate, return_tensors=\&quot;pt\&quot;)\n&quot;
   ]
  },
  {
   &quot;cell_type&quot;: &quot;code&quot;,
   &quot;execution_count&quot;: null,
   &quot;metadata&quot;: {},
   &quot;outputs&quot;: [],
   &quot;source&quot;: [
    &quot;model_input.keys()\n&quot;
   ]
  },
  {
   &quot;cell_type&quot;: &quot;code&quot;,
   &quot;execution_count&quot;: null,
   &quot;metadata&quot;: {},
   &quot;outputs&quot;: [],
   &quot;source&quot;: [
    &quot;generated_tokens = model.generate(**model_input, forced_bos_token_id=tokenizer.lang_code_to_id[\&quot;sw\&quot;])\n&quot;
   ]
  },
  {
   &quot;cell_type&quot;: &quot;markdown&quot;,
   &quot;metadata&quot;: {},
   &quot;source&quot;: [
    &quot;At this point our model have generate the translation token, the next step is to use our tokenizer to convert back the token to the text. This is called decoding.&quot;
   ]
  },
  {
   &quot;cell_type&quot;: &quot;code&quot;,
   &quot;execution_count&quot;: null,
   &quot;metadata&quot;: {},
   &quot;outputs&quot;: [],
   &quot;source&quot;: [
    &quot;translated_text = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n&quot;
   ]
  },
  {
   &quot;cell_type&quot;: &quot;code&quot;,
   &quot;execution_count&quot;: null,
   &quot;metadata&quot;: {},
   &quot;outputs&quot;: [],
   &quot;source&quot;: [
    &quot;translated_text\n&quot;
   ]
  },
  {
   &quot;cell_type&quot;: &quot;markdown&quot;,
   &quot;metadata&quot;: {},
   &quot;source&quot;: [
    &quot;The translated test shows us that the model is working. The next step is to prepare the production model. \n&quot;,
    &quot;To productionalize our model, we will deploy it to ONNX format.\n&quot;,
    &quot;\n&quot;,
    &quot;#### What is the ONNX format?\n&quot;,
    &quot;\n&quot;,
    &quot;ONNX stands for Open Neural Network Exchange. It is an open format built to represent machine learning models in a framework-agnostic way.\n&quot;,
    &quot;\n&quot;,
    &quot;As you may know, neural networks are computation graphs with input, weights, and operations. [Cite the source here.]\n&quot;,
    &quot;\n&quot;,
    &quot;ONNX format is a way of saving neural networks as computation graphs. That  computational graph represents the flow of data through the neural network.\n&quot;,
    &quot;\n&quot;,
    &quot;\n&quot;,
    &quot;The key benefits of saving neural networks in the ONNX format are interoperability and hardware access. Any deep learning platform can read a neural network saved in the ONNX format.  For example, a model trained in Pytorch can be exported to ONNX format and imported in Tensorflow and vice versa.\n&quot;,
    &quot;\n&quot;,
    &quot;You don't need to use Python to read a model saved as ONNX; you can use any programming language of your choice, such as Javascript, C, or C++. \n&quot;,
    &quot;\n&quot;,
    &quot;ONNX makes the model easier to access hardware optimizations, and you can apply other optimizations, such as quantization, to your ONNX model.\n&quot;,
    &quot;\n&quot;,
    &quot;Let us see how we can convert our model to ONNX format to use the full benefits of it.&quot;
   ]
  },
  {
   &quot;attachments&quot;: {},
   &quot;cell_type&quot;: &quot;markdown&quot;,
   &quot;metadata&quot;: {},
   &quot;source&quot;: [
    &quot;Trying to export the model manually and see if we can load the model.&quot;
   ]
  },
  {
   &quot;cell_type&quot;: &quot;markdown&quot;,
   &quot;metadata&quot;: {},
   &quot;source&quot;: [
    &quot;To export the model to onnx format we will be using the optimum cli from Huggingface.&quot;
   ]
  },
  {
   &quot;cell_type&quot;: &quot;code&quot;,
   &quot;execution_count&quot;: null,
   &quot;metadata&quot;: {},
   &quot;outputs&quot;: [],
   &quot;source&quot;: [
    &quot;! optimum-cli export onnx --model masakhane/m2m100_418M_en_swa_rel_news --task seq2seq-lm-with-past --for-ort onnx/m2m100_418M_en_swa_rel_news\n&quot;
   ]
  },
  {
   &quot;attachments&quot;: {},
   &quot;cell_type&quot;: &quot;markdown&quot;,
   &quot;metadata&quot;: {},
   &quot;source&quot;: [
    &quot;check if the model is correct&quot;
   ]
  },
  {
   &quot;cell_type&quot;: &quot;markdown&quot;,
   &quot;metadata&quot;: {},
   &quot;source&quot;: [
    &quot;If the previous command was run successfully, we can see our model saved at `onnx/m2m100_418M_en_swa_rel_news`. \n&quot;,
    &quot;\n&quot;,
    &quot;By checking the size we notice data our encoder model have 1.1 Gb, and our decoder model have 1.7Gb which make our model size to 2.8GB. Additionally, in the same folder we have the tokenizer data.&quot;
   ]
  },
  {
   &quot;cell_type&quot;: &quot;code&quot;,
   &quot;execution_count&quot;: null,
   &quot;metadata&quot;: {},
   &quot;outputs&quot;: [],
   &quot;source&quot;: [
    &quot;from pathlib import Path\n&quot;
   ]
  },
  {
   &quot;cell_type&quot;: &quot;code&quot;,
   &quot;execution_count&quot;: null,
   &quot;metadata&quot;: {},
   &quot;outputs&quot;: [],
   &quot;source&quot;: [
    &quot;base_model_onnx_dir = Path.cwd().joinpath('onnx').joinpath('m2m100_418M_en_swa_rel_news')\n&quot;
   ]
  },
  {
   &quot;cell_type&quot;: &quot;code&quot;,
   &quot;execution_count&quot;: null,
   &quot;metadata&quot;: {},
   &quot;outputs&quot;: [],
   &quot;source&quot;: [
    &quot;base_model_onnx_dir.exists()\n&quot;
   ]
  },
  {
   &quot;attachments&quot;: {},
   &quot;cell_type&quot;: &quot;markdown&quot;,
   &quot;metadata&quot;: {},
   &quot;source&quot;: [
    &quot;### Applying Quantization&quot;
   ]
  },
  {
   &quot;attachments&quot;: {},
   &quot;cell_type&quot;: &quot;markdown&quot;,
   &quot;metadata&quot;: {},
   &quot;source&quot;: [
    &quot;Quantization is the process of reducing the model size by using fewer bits to represent its parameters. Instead of using 32-bit precision floating points for most of the models, with quantization, we can use 12 bits to represent a number and consequently reduce the size of the model.\n&quot;,
    &quot;\n&quot;,
    &quot;Smaller models resulting from quantization are faster to deploy and have low latency in production.\n&quot;,
    &quot;\n&quot;,
    &quot;It has [been shown](https://github.com/huggingface/notebooks/blob/main/examples/onnx-export.ipynb) that you can improve the inference time by 75% by using an ONNX quantized model without a considerable loss in performance. &lt;Find more evidence for this&gt; \n&quot;,
    &quot;\n&quot;,
    &quot;\n&quot;,
    &quot;For this tutorial, we will use quantization to reduce the size of our model for inference.&quot;
   ]
  },
  {
   &quot;cell_type&quot;: &quot;code&quot;,
   &quot;execution_count&quot;: null,
   &quot;metadata&quot;: {},
   &quot;outputs&quot;: [],
   &quot;source&quot;: [
    &quot;from optimum.onnxruntime import ORTQuantizer, ORTModelForSeq2SeqLM\n&quot;,
    &quot;from optimum.onnxruntime.configuration import AutoQuantizationConfig\n&quot;
   ]
  },
  {
   &quot;cell_type&quot;: &quot;code&quot;,
   &quot;execution_count&quot;: null,
   &quot;metadata&quot;: {},
   &quot;outputs&quot;: [],
   &quot;source&quot;: [
    &quot;encoder_quantizer = ORTQuantizer.from_pretrained(base_model_onnx_dir, file_name=\&quot;encoder_model.onnx\&quot;)\n&quot;
   ]
  },
  {
   &quot;cell_type&quot;: &quot;code&quot;,
   &quot;execution_count&quot;: null,
   &quot;metadata&quot;: {},
   &quot;outputs&quot;: [],
   &quot;source&quot;: [
    &quot;decoder_quantizer = ORTQuantizer.from_pretrained(base_model_onnx_dir, file_name=\&quot;decoder_model.onnx\&quot;)\n&quot;
   ]
  },
  {
   &quot;cell_type&quot;: &quot;code&quot;,
   &quot;execution_count&quot;: null,
   &quot;metadata&quot;: {},
   &quot;outputs&quot;: [],
   &quot;source&quot;: [
    &quot;decoder_with_past_quantizer = ORTQuantizer.from_pretrained(base_model_onnx_dir, file_name=\&quot;decoder_with_past_model.onnx\&quot;)\n&quot;
   ]
  },
  {
   &quot;cell_type&quot;: &quot;code&quot;,
   &quot;execution_count&quot;: null,
   &quot;metadata&quot;: {},
   &quot;outputs&quot;: [],
   &quot;source&quot;: [
    &quot;quantizers = [encoder_quantizer, decoder_quantizer, decoder_with_past_quantizer]\n&quot;
   ]
  },
  {
   &quot;cell_type&quot;: &quot;markdown&quot;,
   &quot;metadata&quot;: {},
   &quot;source&quot;: [
    &quot;We will use dynamic quantization to our model.&quot;
   ]
  },
  {
   &quot;cell_type&quot;: &quot;code&quot;,
   &quot;execution_count&quot;: null,
   &quot;metadata&quot;: {},
   &quot;outputs&quot;: [],
   &quot;source&quot;: [
    &quot;dynamic_quantization_config = AutoQuantizationConfig.avx512_vnni(is_static=False, per_channel=False)\n&quot;
   ]
  },
  {
   &quot;cell_type&quot;: &quot;code&quot;,
   &quot;execution_count&quot;: null,
   &quot;metadata&quot;: {},
   &quot;outputs&quot;: [],
   &quot;source&quot;: [
    &quot;quantized_model_path = Path(\&quot;onnx\&quot;).joinpath(f\&quot;{MODEL_SUFFIX}_quantized/\&quot;)\n&quot;,
    &quot;quantized_model_path.mkdir(parents=True, exist_ok=True)\n&quot;
   ]
  },
  {
   &quot;cell_type&quot;: &quot;code&quot;,
   &quot;execution_count&quot;: null,
   &quot;metadata&quot;: {},
   &quot;outputs&quot;: [],
   &quot;source&quot;: [
    &quot;for quantizer in quantizers:\n&quot;,
    &quot;    quantizer.quantize(quantization_config=dynamic_quantization_config, save_dir=quantized_model_path)\n&quot;
   ]
  },
  {
   &quot;cell_type&quot;: &quot;markdown&quot;,
   &quot;metadata&quot;: {},
   &quot;source&quot;: [
    &quot;Our model are save as quantized version, we can now check the size of the quantized models.&quot;
   ]
  },
  {
   &quot;cell_type&quot;: &quot;code&quot;,
   &quot;execution_count&quot;: null,
   &quot;metadata&quot;: {},
   &quot;outputs&quot;: [],
   &quot;source&quot;: [
    &quot;for model in quantized_model_path.glob(\&quot;*.onnx\&quot;):\n&quot;,
    &quot;    print(f\&quot;the size of {model.stem} the model in MB is: {model.stat().st_size / (1024 * 1024)}\&quot;)\n&quot;
   ]
  },
  {
   &quot;cell_type&quot;: &quot;markdown&quot;,
   &quot;metadata&quot;: {},
   &quot;source&quot;: [
    &quot;We can see that we have managed to reduce the size of our initial models by two! From 1.6 Gb without quantization to 800 Mb with quantization. Let us see how to use the quantized model for inference.&quot;
   ]
  },
  {
   &quot;attachments&quot;: {},
   &quot;cell_type&quot;: &quot;markdown&quot;,
   &quot;metadata&quot;: {},
   &quot;source&quot;: [
    &quot;### Use the quantized model&quot;
   ]
  },
  {
   &quot;cell_type&quot;: &quot;code&quot;,
   &quot;execution_count&quot;: null,
   &quot;metadata&quot;: {},
   &quot;outputs&quot;: [],
   &quot;source&quot;: [
    &quot;quantized_model_path = base_model_onnx_dir\n&quot;
   ]
  },
  {
   &quot;cell_type&quot;: &quot;code&quot;,
   &quot;execution_count&quot;: null,
   &quot;metadata&quot;: {},
   &quot;outputs&quot;: [],
   &quot;source&quot;: [
    &quot;quantized_model = ORTModelForSeq2SeqLM.from_pretrained(quantized_model_path, \n&quot;,
    &quot;                                                       decoder_file_name='decoder_model_quantized.onnx',\n&quot;,
    &quot;                                                       encoder_file_name='encoder_model_quantized.onnx',)\n&quot;
   ]
  },
  {
   &quot;cell_type&quot;: &quot;code&quot;,
   &quot;execution_count&quot;: null,
   &quot;metadata&quot;: {},
   &quot;outputs&quot;: [],
   &quot;source&quot;: [
    &quot;quantized_pipeline = pipeline(\&quot;translation_en_to_sw\&quot;, model=quantized_model, tokenizer=tokenizer)\n&quot;
   ]
  },
  {
   &quot;cell_type&quot;: &quot;code&quot;,
   &quot;execution_count&quot;: null,
   &quot;metadata&quot;: {},
   &quot;outputs&quot;: [],
   &quot;source&quot;: [
    &quot;translated_text_quantized = quantized_pipeline(text_to_translate)\n&quot;
   ]
  },
  {
   &quot;cell_type&quot;: &quot;code&quot;,
   &quot;execution_count&quot;: null,
   &quot;metadata&quot;: {},
   &quot;outputs&quot;: [],
   &quot;source&quot;: [
    &quot;print(translated_text_quantized)\n&quot;
   ]
  },
  {
   &quot;attachments&quot;: {},
   &quot;cell_type&quot;: &quot;markdown&quot;,
   &quot;metadata&quot;: {},
   &quot;source&quot;: [
    &quot;The quantization has reduced the size of the model, but it gave the same translation of our base text. We may need to run more extensive tests to find out what is the accuracy difference between our quantized model and the base model. Assuming the performance lost was not considerable, we move to the next step of our tutorial.&quot;
   ]
  },
  {
   &quot;cell_type&quot;: &quot;markdown&quot;,
   &quot;metadata&quot;: {},
   &quot;source&quot;: []
  },
  {
   &quot;cell_type&quot;: &quot;markdown&quot;,
   &quot;metadata&quot;: {},
   &quot;source&quot;: [
    &quot;## Deploy the Model for inference\n&quot;,
    &quot;\n&quot;,
    &quot;At this point, we have our model quantized and saved in ONNX format. We will now deploy it to a production server using the triton inference server. \n&quot;,
    &quot;In the first section, we will deploy with the triton server as a docker container, and then we will use Kserve to deploy it to the Kubernetes deployment environment.&quot;
   ]
  },
  {
   &quot;cell_type&quot;: &quot;markdown&quot;,
   &quot;metadata&quot;: {},
   &quot;source&quot;: [
    &quot;## Triton Server\n&quot;,
    &quot;\n&quot;,
    &quot;Triton is a software tool for deploying machine learning models for inference. It is designed to produce high-quality inference across different hardware platforms, either GPU or CPU. It also supports inference across cloud, data center, and embedded devices.\n&quot;,
    &quot;One of the advantages of the triton server is that it supports dynamic batching and concurrent model execution.\n&quot;,
    &quot;\n&quot;,
    &quot;- Dynamic batching, for models that support batching, which is the case for deep learning models, triton implements scheduling and batching algorithms that combine individual requests to improve inference throughput.\n&quot;,
    &quot;\n&quot;,
    &quot;&lt;Talk More about dynamic batching here...&gt;\n&quot;,
    &quot;    \n&quot;,
    &quot;- Concurrency model execution is the capacity to run simultaneously multiple models on the same GPU or various GPUs.\n&quot;,
    &quot;\n&quot;,
    &quot;\n&quot;,
    &quot;### Triton Server Backend\n&quot;,
    &quot;\n&quot;,
    &quot;Triton supports different backends to execute the model. A backend is a wrapper around a deep learning framework like Pytorch, TensorFlow, TensorRT, or ONNX Runtime.\n&quot;,
    &quot;Two backend types interested us for this post: the Python Backend and the ONNX runtime backend. \n&quot;,
    &quot;\n&quot;,
    &quot;The ONNX runtime backend executes ONNX models, and the Python backend allows the writing of the model logic in Python. \n&quot;,
    &quot;\n&quot;,
    &quot;In this post, we will be focused on the ONNX and the Python backend.\n&quot;,
    &quot;\n&quot;,
    &quot;I decided to go with the Python backend because I struggled to deploy the encoder decode model using an ensemble of the ONNX model. I still have a question in progress on [StackOverlow](https://stackoverflow.com/q/76638766/4683950).  &quot;
   ]
  },
  {
   &quot;cell_type&quot;: &quot;markdown&quot;,
   &quot;metadata&quot;: {},
   &quot;source&quot;: [
    &quot;#### Uploading the Model to Repository.\n&quot;,
    &quot;\n&quot;,
    &quot;The first step before using our model is to upload it to the model repository. For this tutorial,   we will be using our local storage as a model repository but later we will use static storage such as google cloud or AWS S3 to host our model.&quot;
   ]
  },
  {
   &quot;cell_type&quot;: &quot;markdown&quot;,
   &quot;metadata&quot;: {},
   &quot;source&quot;: [
    &quot;#### Configuration\n&quot;,
    &quot;\n&quot;,
    &quot;The first step to deploy our model in triton is to configure it.\n&quot;,
    &quot;\n&quot;,
    &quot;The configuration sets up the model and defines the input shape and the output shape of our models.&quot;
   ]
  },
  {
   &quot;cell_type&quot;: &quot;code&quot;,
   &quot;execution_count&quot;: null,
   &quot;metadata&quot;: {},
   &quot;outputs&quot;: [],
   &quot;source&quot;: [
    &quot;# %load ./triton_model_repository/m2m100_translation_model/config.pbtxt\n&quot;,
    &quot;name: \&quot;m2m100_translation_model\&quot;\n&quot;,
    &quot;backend: \&quot;python\&quot;\n&quot;,
    &quot;max_batch_size: 0\n&quot;,
    &quot;input [\n&quot;,
    &quot;  {\n&quot;,
    &quot;    name: \&quot;input_ids\&quot;\n&quot;,
    &quot;    data_type: TYPE_INT64\n&quot;,
    &quot;    dims: [ -1, -1 ]\n&quot;,
    &quot;  },\n&quot;,
    &quot;{\n&quot;,
    &quot;    name: \&quot;attention_mask\&quot;\n&quot;,
    &quot;    data_type: TYPE_INT64\n&quot;,
    &quot;    dims: [ -1, -1 ]\n&quot;,
    &quot;  }\n&quot;,
    &quot;]\n&quot;,
    &quot;output [\n&quot;,
    &quot;    {\n&quot;,
    &quot;    name: \&quot;generated_indices\&quot;\n&quot;,
    &quot;    data_type: TYPE_FP32\n&quot;,
    &quot;    dims: [ -1, -1 ]\n&quot;,
    &quot;  }\n&quot;,
    &quot;]\n&quot;,
    &quot;\n&quot;,
    &quot;instance_group [\n&quot;,
    &quot;    {\n&quot;,
    &quot;      count: 1\n&quot;,
    &quot;      kind: KIND_CPU\n&quot;,
    &quot;    }\n&quot;,
    &quot;]\n&quot;
   ]
  },
  {
   &quot;cell_type&quot;: &quot;markdown&quot;,
   &quot;metadata&quot;: {},
   &quot;source&quot;: [
    &quot;In the above configuration, we can see that the model is expecting two inputs:  the input ids and the attention masks, and it returns the generated input indices. \n&quot;,
    &quot;Additionally, we can notice that the model is running on a 1 CPU. If we had a GPU available, we would put it in the instance settings.\n&quot;,
    &quot;\n&quot;,
    &quot;The input ids and the attention masks are the outputs from the tokenization process. The generated indices are the tokenized output that our tokenizer will decode.&quot;
   ]
  },
  {
   &quot;cell_type&quot;: &quot;markdown&quot;,
   &quot;metadata&quot;: {},
   &quot;source&quot;: [
    &quot;The configuration file needs to be save at the root folder  of our model repository.&quot;
   ]
  },
  {
   &quot;cell_type&quot;: &quot;markdown&quot;,
   &quot;metadata&quot;: {},
   &quot;source&quot;: [
    &quot;#### Create the load model script\n&quot;,
    &quot;\n&quot;,
    &quot;The load model script is the python script that load our model before and run it for inference.&quot;
   ]
  },
  {
   &quot;cell_type&quot;: &quot;code&quot;,
   &quot;execution_count&quot;: null,
   &quot;metadata&quot;: {},
   &quot;outputs&quot;: [],
   &quot;source&quot;: [
    &quot;# %load ./triton_model_repository/m2m100_translation_model/1/model.py\n&quot;,
    &quot;from typing import Dict, List\n&quot;,
    &quot;import triton_python_backend_utils as pb_utils\n&quot;,
    &quot;from pathlib import Path\n&quot;,
    &quot;from optimum.onnxruntime import ORTModelForSeq2SeqLM\n&quot;,
    &quot;import torch\n&quot;,
    &quot;\n&quot;,
    &quot;TOKENIZER_SW_LANG_CODE_TO_ID = 128088\n&quot;,
    &quot;\n&quot;,
    &quot;\n&quot;,
    &quot;class TritonPythonModel:\n&quot;,
    &quot;\n&quot;,
    &quot;    def initialize(self, args: Dict[str, str]) -&gt; None:\n&quot;,
    &quot;        \&quot;\&quot;\&quot;\n&quot;,
    &quot;        Initialize the tokenization process\n&quot;,
    &quot;        :param args: arguments from Triton config file\n&quot;,
    &quot;        \&quot;\&quot;\&quot;\n&quot;,
    &quot;        current_path: str = Path(args[\&quot;model_repository\&quot;]).parent.absolute()\n&quot;,
    &quot;        model_path = current_path.joinpath(\&quot;m2m100_translation_model\&quot;, \&quot;1\&quot;, \&quot;m2m100_418M_en_swa_rel_news_quantized\&quot;)\n&quot;,
    &quot;        self.device = \&quot;cpu\&quot; if args[\&quot;model_instance_kind\&quot;] == \&quot;CPU\&quot; else \&quot;cuda\&quot;\n&quot;,
    &quot;        # more variables in https://github.com/triton-inference-server/python_backend/blob/main/src/python.cc\n&quot;,
    &quot;        self.model = ORTModelForSeq2SeqLM.from_pretrained(model_path,\n&quot;,
    &quot;                                                          decoder_file_name=\&quot;decoder_model_quantized.onnx\&quot;,\n&quot;,
    &quot;                                                          encoder_file_name=\&quot;encoder_model_quantized.onnx\&quot;)\n&quot;,
    &quot;        if self.device == \&quot;cuda\&quot;:\n&quot;,
    &quot;            self.model = self.model.cuda()\n&quot;,
    &quot;        print(\&quot;TritonPythonModel initialized\&quot;)\n&quot;,
    &quot;\n&quot;,
    &quot;    def execute(self, requests) -&gt; \&quot;List[List[pb_utils.Tensor]]\&quot;:\n&quot;,
    &quot;        \&quot;\&quot;\&quot;\n&quot;,
    &quot;        Parse and tokenize each request\n&quot;,
    &quot;        :param requests: 1 or more requests received by Triton server.\n&quot;,
    &quot;        :return: text as input tensors\n&quot;,
    &quot;        \&quot;\&quot;\&quot;\n&quot;,
    &quot;        responses = []\n&quot;,
    &quot;        # for loop for batch requests (disabled in our case)\n&quot;,
    &quot;        for request in requests:\n&quot;,
    &quot;            # binary data typed back to string\n&quot;,
    &quot;            input_ids = pb_utils.get_input_tensor_by_name(request, \&quot;input_ids\&quot;).as_numpy()\n&quot;,
    &quot;            attention_masks = pb_utils.get_input_tensor_by_name(request, \&quot;attention_mask\&quot;).as_numpy()\n&quot;,
    &quot;            input_ids = torch.as_tensor(input_ids, dtype=torch.int64)\n&quot;,
    &quot;            attention_masks = torch.as_tensor(attention_masks, dtype=torch.int64)\n&quot;,
    &quot;            if self.device == \&quot;cuda\&quot;:\n&quot;,
    &quot;                input_ids = input_ids.to(\&quot;cuda\&quot;)\n&quot;,
    &quot;                attention_masks = attention_masks.to(\&quot;cuda\&quot;)\n&quot;,
    &quot;            model_inputs = {\&quot;input_ids\&quot;: input_ids, \&quot;attention_mask\&quot;: attention_masks}\n&quot;,
    &quot;            generated_indices = self.model.generate(**model_inputs,\n&quot;,
    &quot;                                                    forced_bos_token_id=TOKENIZER_SW_LANG_CODE_TO_ID)\n&quot;,
    &quot;            tensor_output = pb_utils.Tensor(\&quot;generated_indices\&quot;, generated_indices.numpy())\n&quot;,
    &quot;            responses.append(tensor_output)\n&quot;,
    &quot;        responses = [pb_utils.InferenceResponse(output_tensors=responses)]\n&quot;,
    &quot;        return responses\n&quot;,
    &quot;    \n&quot;,
    &quot;    def finalize(self):\n&quot;,
    &quot;        \&quot;\&quot;\&quot;`finalize` is called only once when the model is being unloaded.\n&quot;,
    &quot;        Implementing `finalize` function is optional. This function allows\n&quot;,
    &quot;        the model to perform any necessary clean ups before exit.\n&quot;,
    &quot;        \&quot;\&quot;\&quot;\n&quot;,
    &quot;        print('Cleaning up...')\n&quot;
   ]
  },
  {
   &quot;cell_type&quot;: &quot;markdown&quot;,
   &quot;metadata&quot;: {},
   &quot;source&quot;: [
    &quot;The model contains a class with two methods:\n&quot;,
    &quot;\n&quot;,
    &quot;- Initialize: The initialize method uses the ORT model to load the model in the memory!\n&quot;,
    &quot;- The execute method parse and tokenize each request received by the triton server. It calls the generate method on the input of the request and returns the generated text indices. This text will be later decoded by the tokenizer.&quot;
   ]
  },
  {
   &quot;cell_type&quot;: &quot;markdown&quot;,
   &quot;metadata&quot;: {},
   &quot;source&quot;: [
    &quot;If our configuration is done properly and the model is saved properly, we should have a model repository that looks like this:\n&quot;,
    &quot;\n&quot;,
    &quot;```\n&quot;,
    &quot;triton_model_repository\n&quot;,
    &quot;└── m2m100_translation_model\n&quot;,
    &quot;    ├── 1\n&quot;,
    &quot;    │   ├── m2m100_418M_en_swa_rel_news_quantized\n&quot;,
    &quot;    │   │   ├── config.json\n&quot;,
    &quot;    │   │   ├── decoder_model_quantized.onnx\n&quot;,
    &quot;    │   │   ├── decoder_with_past_model_quantized.onnx\n&quot;,
    &quot;    │   │   ├── encoder_model_quantized.onnx\n&quot;,
    &quot;    │   │   └── ort_config.json\n&quot;,
    &quot;    │   └── model.py\n&quot;,
    &quot;    └── config.pbtxt\n&quot;,
    &quot;```\n&quot;,
    &quot;\n&quot;,
    &quot;Make sure that you have the file located at the precise location as me in order to be able to run the code.&quot;
   ]
  },
  {
   &quot;cell_type&quot;: &quot;markdown&quot;,
   &quot;metadata&quot;: {},
   &quot;source&quot;: [
    &quot;\n&quot;,
    &quot;### Launching the docker image\n&quot;,
    &quot;\n&quot;,
    &quot;If you look carefully at the code for our Python model, you can see that the model is importing the ONNX runtime! However, that runtime is not installed in the base triton server image. The reason why we decided to build our own image based on the triton server.&quot;
   ]
  },
  {
   &quot;cell_type&quot;: &quot;code&quot;,
   &quot;execution_count&quot;: null,
   &quot;metadata&quot;: {},
   &quot;outputs&quot;: [],
   &quot;source&quot;: [
    &quot;# %load Dockerfile\n&quot;,
    &quot;# Use the base image\n&quot;,
    &quot;FROM nvcr.io/nvidia/tritonserver:23.06-py3\n&quot;,
    &quot;\n&quot;,
    &quot;# Install the required Python packages\n&quot;,
    &quot;RUN pip install optimum==1.9.0 onnxruntime==1.15.1 onnx==1.14.0\n&quot;
   ]
  },
  {
   &quot;cell_type&quot;: &quot;markdown&quot;,
   &quot;metadata&quot;: {},
   &quot;source&quot;: [
    &quot;The above code shows how we build our docker image.\n&quot;,
    &quot;We use the base Tritonserver image, and then we add the different packages we need to run our model.&quot;
   ]
  },
  {
   &quot;cell_type&quot;: &quot;markdown&quot;,
   &quot;metadata&quot;: {},
   &quot;source&quot;: [
    &quot;Next we can build our model using:\n&quot;,
    &quot;\n&quot;,
    &quot;`docker build -t espymur/triton-onnx:dev  -f Dockerfile .`&quot;
   ]
  },
  {
   &quot;cell_type&quot;: &quot;markdown&quot;,
   &quot;metadata&quot;: {},
   &quot;source&quot;: [
    &quot;Please note that the image is huge. Its size is around 15 GB. In the next post, I will try to optimize its size by using the technique suggested in the documentation.\n&quot;,
    &quot;\n&quot;,
    &quot;If our model build is finished, we can now run the docker container that serves the model.&quot;
   ]
  },
  {
   &quot;cell_type&quot;: &quot;markdown&quot;,
   &quot;metadata&quot;: {},
   &quot;source&quot;: [
    &quot;\n&quot;,
    &quot;`docker run --rm -p 8000:8000 -p 8001:8001 -p 8002:8002  --shm-size 128M -v ${PWD}/triton_model_repository:/models  espymur/triton-onnx:dev tritonserver --model-repository=/models`\n&quot;,
    &quot;\n&quot;,
    &quot;- This command runs the docker container and map the port 8000, 8001, 8002 to 8000, 8001, and 8002 of our local machine.\n&quot;,
    &quot;\n&quot;,
    &quot;- It then creates a volume that maps the `${PWD}/triton_model_repository` path from our local machine to /models in the container.\n&quot;,
    &quot;\n&quot;,
    &quot;- It is also using a shared memory of 128 Mb.\n&quot;
   ]
  },
  {
   &quot;cell_type&quot;: &quot;markdown&quot;,
   &quot;metadata&quot;: {},
   &quot;source&quot;: [
    &quot;With this model we can see that our model is running and we can perform inference without any problem.&quot;
   ]
  },
  {
   &quot;cell_type&quot;: &quot;markdown&quot;,
   &quot;metadata&quot;: {},
   &quot;source&quot;: [
    &quot;At this point, we have got our model running inside the docker container, the next step will be to make inference requests. Let see how we can achieve that.&quot;
   ]
  },
  {
   &quot;cell_type&quot;: &quot;markdown&quot;,
   &quot;metadata&quot;: {},
   &quot;source&quot;: [
    &quot;### Making Inference Requests&quot;
   ]
  },
  {
   &quot;cell_type&quot;: &quot;markdown&quot;,
   &quot;metadata&quot;: {},
   &quot;source&quot;: [
    &quot;The model is now updated and saved as a Triton backend model. We will apply tokenization offline and query the model with the tokenized words and the attention mask. \n&quot;,
    &quot;The model will return the indices of the translated test; we will use the tokenizer again to decode the indices and produce the output.\n&quot;,
    &quot;\n&quot;,
    &quot;We can later have the tokenizer as a separate service people can interact with using HTTP.&quot;
   ]
  },
  {
   &quot;cell_type&quot;: &quot;code&quot;,
   &quot;execution_count&quot;: null,
   &quot;metadata&quot;: {},
   &quot;outputs&quot;: [],
   &quot;source&quot;: [
    &quot;MODEL_NAME = \&quot;masakhane/m2m100_418M_en_swa_rel_news\&quot;\n&quot;
   ]
  },
  {
   &quot;cell_type&quot;: &quot;code&quot;,
   &quot;execution_count&quot;: null,
   &quot;metadata&quot;: {},
   &quot;outputs&quot;: [],
   &quot;source&quot;: [
    &quot;from transformers import AutoTokenizer\n&quot;
   ]
  },
  {
   &quot;cell_type&quot;: &quot;code&quot;,
   &quot;execution_count&quot;: null,
   &quot;metadata&quot;: {},
   &quot;outputs&quot;: [],
   &quot;source&quot;: [
    &quot;tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n&quot;
   ]
  },
  {
   &quot;cell_type&quot;: &quot;code&quot;,
   &quot;execution_count&quot;: null,
   &quot;metadata&quot;: {},
   &quot;outputs&quot;: [],
   &quot;source&quot;: [
    &quot;import numpy as np\n&quot;,
    &quot;import tritonclient.http as httpclient\n&quot;
   ]
  },
  {
   &quot;cell_type&quot;: &quot;markdown&quot;,
   &quot;metadata&quot;: {},
   &quot;source&quot;: [
    &quot;#### The HTTP client&quot;
   ]
  },
  {
   &quot;cell_type&quot;: &quot;code&quot;,
   &quot;execution_count&quot;: null,
   &quot;metadata&quot;: {},
   &quot;outputs&quot;: [],
   &quot;source&quot;: [
    &quot;client = httpclient.InferenceServerClient(url=\&quot;localhost:8000\&quot;)\n&quot;
   ]
  },
  {
   &quot;cell_type&quot;: &quot;markdown&quot;,
   &quot;metadata&quot;: {},
   &quot;source&quot;: [
    &quot;#### The inputs&quot;
   ]
  },
  {
   &quot;cell_type&quot;: &quot;markdown&quot;,
   &quot;metadata&quot;: {},
   &quot;source&quot;: [
    &quot;This line creates the client object we will be using to interact with our server. To create the client object, we are passing the URL of the inference service as parameters.&quot;
   ]
  },
  {
   &quot;cell_type&quot;: &quot;code&quot;,
   &quot;execution_count&quot;: null,
   &quot;metadata&quot;: {},
   &quot;outputs&quot;: [],
   &quot;source&quot;: [
    &quot;input_ids = httpclient.InferInput(\&quot;input_ids\&quot;, shape=(-1,1) , datatype=\&quot;TYPE_INT64\&quot;,)\n&quot;,
    &quot;attention_mask = httpclient.InferInput(\&quot;attention_mask\&quot;, shape=(-1,1) , datatype=\&quot;TYPE_INT64\&quot;,)\n&quot;
   ]
  },
  {
   &quot;cell_type&quot;: &quot;markdown&quot;,
   &quot;metadata&quot;: {},
   &quot;source&quot;: [
    &quot;### The outputs.&quot;
   ]
  },
  {
   &quot;cell_type&quot;: &quot;code&quot;,
   &quot;execution_count&quot;: null,
   &quot;metadata&quot;: {},
   &quot;outputs&quot;: [],
   &quot;source&quot;: [
    &quot;outputs = httpclient.InferRequestedOutput(\&quot;generated_indices\&quot;, binary_data=False)\n&quot;
   ]
  },
  {
   &quot;cell_type&quot;: &quot;markdown&quot;,
   &quot;metadata&quot;: {},
   &quot;source&quot;: [
    &quot;To prepare our model input, we are using the Triton client library. \n&quot;,
    &quot;The above code creates two objects for the input ID and the attention mask, respectively! We can specify the shape of the element and its datatype when creating the code.&quot;
   ]
  },
  {
   &quot;cell_type&quot;: &quot;markdown&quot;,
   &quot;metadata&quot;: {},
   &quot;source&quot;: [
    &quot;Additionally to our inputs and outputs, we will need some utility function to perform the tokenization. Here are those functions:&quot;
   ]
  },
  {
   &quot;cell_type&quot;: &quot;markdown&quot;,
   &quot;metadata&quot;: {},
   &quot;source&quot;: [
    &quot;#### Utilities Functions&quot;
   ]
  },
  {
   &quot;cell_type&quot;: &quot;code&quot;,
   &quot;execution_count&quot;: null,
   &quot;metadata&quot;: {},
   &quot;outputs&quot;: [],
   &quot;source&quot;: [
    &quot;def get_tokenizer(model_name):\n&quot;,
    &quot;    \&quot;\&quot;\&quot;Returns a tokenizer for a given model name\n&quot;,
    &quot;\n&quot;,
    &quot;    Args:\n&quot;,
    &quot;        model_name (_type_): _description_\n&quot;,
    &quot;\n&quot;,
    &quot;    Returns:\n&quot;,
    &quot;        _type_: _description_\n&quot;,
    &quot;    \&quot;\&quot;\&quot;\n&quot;,
    &quot;    tokenizer = AutoTokenizer.from_pretrained(model_name)\n&quot;,
    &quot;    return tokenizer\n&quot;
   ]
  },
  {
   &quot;cell_type&quot;: &quot;code&quot;,
   &quot;execution_count&quot;: null,
   &quot;metadata&quot;: {},
   &quot;outputs&quot;: [],
   &quot;source&quot;: [
    &quot;from typing import Tuple, List\n&quot;,
    &quot;\n&quot;,
    &quot;import numpy as np\n&quot;
   ]
  },
  {
   &quot;cell_type&quot;: &quot;code&quot;,
   &quot;execution_count&quot;: null,
   &quot;metadata&quot;: {},
   &quot;outputs&quot;: [],
   &quot;source&quot;: [
    &quot;from transformers import AutoTokenizer\n&quot;
   ]
  },
  {
   &quot;cell_type&quot;: &quot;code&quot;,
   &quot;execution_count&quot;: null,
   &quot;metadata&quot;: {},
   &quot;outputs&quot;: [],
   &quot;source&quot;: [
    &quot;\n&quot;,
    &quot;def tokenize_text(tokenizer: AutoTokenizer, text:str) -&gt; Tuple[np.ndarray , np.ndarray]:\n&quot;,
    &quot;    tokenized_text = tokenizer(text, padding=True, return_tensors=\&quot;np\&quot;)\n&quot;,
    &quot;    return tokenized_text.input_ids, tokenized_text.attention_mask\n&quot;
   ]
  },
  {
   &quot;cell_type&quot;: &quot;code&quot;,
   &quot;execution_count&quot;: null,
   &quot;metadata&quot;: {},
   &quot;outputs&quot;: [],
   &quot;source&quot;: [
    &quot;def generate_inference_input(input_ids: np.ndarray, attention_mask: np.ndarray) -&gt; List[httpclient.InferInput]:\n&quot;,
    &quot;    \&quot;\&quot;\&quot;\n&quot;,
    &quot;    Generate inference inputs for Triton server\n&quot;,
    &quot;\n&quot;,
    &quot;    Args:\n&quot;,
    &quot;        input_ids (np.ndarray): _description_\n&quot;,
    &quot;        attention_mask (np.ndarray): _description_\n&quot;,
    &quot;\n&quot;,
    &quot;    Returns:\n&quot;,
    &quot;        List[httpclient.InferInput]: _description_\n&quot;,
    &quot;    \&quot;\&quot;\&quot;\n&quot;,
    &quot;    inputs = []\n&quot;,
    &quot;    inputs.append(httpclient.InferInput(\&quot;input_ids\&quot;, input_ids.shape, \&quot;INT64\&quot;))\n&quot;,
    &quot;    inputs.append(httpclient.InferInput(\&quot;attention_mask\&quot;, attention_mask.shape, \&quot;INT64\&quot;))\n&quot;,
    &quot;\n&quot;,
    &quot;    inputs[0].set_data_from_numpy(input_ids.astype(np.int64), binary_data=False)\n&quot;,
    &quot;    inputs[1].set_data_from_numpy(attention_mask.astype(np.int64), binary_data=False)\n&quot;,
    &quot;    return inputs\n&quot;,
    &quot;\n&quot;
   ]
  },
  {
   &quot;cell_type&quot;: &quot;code&quot;,
   &quot;execution_count&quot;: null,
   &quot;metadata&quot;: {},
   &quot;outputs&quot;: [],
   &quot;source&quot;: [
    &quot;text = [\&quot;I am learning how to use Triton Server for Machine Learning\&quot;, \&quot;Hello, my name is Espoir Murhabazi,  I am a Software Engineer from Congo DRC but living in UK\&quot;]\n&quot;
   ]
  },
  {
   &quot;cell_type&quot;: &quot;code&quot;,
   &quot;execution_count&quot;: null,
   &quot;metadata&quot;: {},
   &quot;outputs&quot;: [],
   &quot;source&quot;: [
    &quot;tokenizer = get_tokenizer(MODEL_NAME)\n&quot;
   ]
  },
  {
   &quot;cell_type&quot;: &quot;code&quot;,
   &quot;execution_count&quot;: null,
   &quot;metadata&quot;: {},
   &quot;outputs&quot;: [],
   &quot;source&quot;: [
    &quot;input_ids, attention_mask = tokenize_text(tokenizer, text)\n&quot;
   ]
  },
  {
   &quot;cell_type&quot;: &quot;code&quot;,
   &quot;execution_count&quot;: null,
   &quot;metadata&quot;: {},
   &quot;outputs&quot;: [],
   &quot;source&quot;: [
    &quot;inference_inputs = generate_inference_input(input_ids, attention_mask)\n&quot;
   ]
  },
  {
   &quot;cell_type&quot;: &quot;markdown&quot;,
   &quot;metadata&quot;: {},
   &quot;source&quot;: []
  },
  {
   &quot;cell_type&quot;: &quot;markdown&quot;,
   &quot;metadata&quot;: {},
   &quot;source&quot;: [
    &quot;With our input prepared we can now make an inference request to our server. Here is the code we will be using to make the inference request.&quot;
   ]
  },
  {
   &quot;cell_type&quot;: &quot;code&quot;,
   &quot;execution_count&quot;: null,
   &quot;metadata&quot;: {},
   &quot;outputs&quot;: [],
   &quot;source&quot;: [
    &quot;results = client.infer(model_name=\&quot;m2m100_translation_model\&quot;, inputs=inference_inputs, outputs=[outputs])\n&quot;,
    &quot;inference_output = results.as_numpy('generated_indices')\n&quot;
   ]
  },
  {
   &quot;cell_type&quot;: &quot;markdown&quot;,
   &quot;metadata&quot;: {},
   &quot;source&quot;: [
    &quot;If everything goes as planned, we should be able to see the inference response.&quot;
   ]
  },
  {
   &quot;cell_type&quot;: &quot;code&quot;,
   &quot;execution_count&quot;: null,
   &quot;metadata&quot;: {},
   &quot;outputs&quot;: [],
   &quot;source&quot;: [
    &quot;inference_output\n&quot;
   ]
  },
  {
   &quot;cell_type&quot;: &quot;code&quot;,
   &quot;execution_count&quot;: null,
   &quot;metadata&quot;: {},
   &quot;outputs&quot;: [],
   &quot;source&quot;: [
    &quot;decoded_output = tokenizer.batch_decode(inference_output, skip_special_tokens=True)\n&quot;
   ]
  },
  {
   &quot;cell_type&quot;: &quot;code&quot;,
   &quot;execution_count&quot;: null,
   &quot;metadata&quot;: {},
   &quot;outputs&quot;: [],
   &quot;source&quot;: [
    &quot;decoded_output\n&quot;
   ]
  },
  {
   &quot;cell_type&quot;: &quot;markdown&quot;,
   &quot;metadata&quot;: {},
   &quot;source&quot;: [
    &quot;With the decoded output, we can see that our inference server is working!\n&quot;,
    &quot;\n&quot;,
    &quot;### Conclusion\n&quot;,
    &quot;\n&quot;,
    &quot;\n&quot;,
    &quot;In this post, we saw how we can start form a raw translation model from huggingface, we then quantized it to reduce it's size, and finally deployed the model on a triton server to perform inference.\n&quot;,
    &quot;In the second part of this blog we will learn how to scale the whole prototype and build an end to end pipeline using kubernetes and Kserve.&quot;
   ]
  },
  {
   &quot;cell_type&quot;: &quot;code&quot;,
   &quot;execution_count&quot;: null,
   &quot;metadata&quot;: {},
   &quot;outputs&quot;: [],
   &quot;source&quot;: []
  }
 ],
 &quot;metadata&quot;: {
  &quot;kernelspec&quot;: {
   &quot;display_name&quot;: &quot;Python 3 (ipykernel)&quot;,
   &quot;language&quot;: &quot;python&quot;,
   &quot;name&quot;: &quot;python3&quot;
  },
  &quot;language_info&quot;: {
   &quot;codemirror_mode&quot;: {
    &quot;name&quot;: &quot;ipython&quot;,
    &quot;version&quot;: 3
   },
   &quot;file_extension&quot;: &quot;.py&quot;,
   &quot;mimetype&quot;: &quot;text/x-python&quot;,
   &quot;name&quot;: &quot;python&quot;,
   &quot;nbconvert_exporter&quot;: &quot;python&quot;,
   &quot;pygments_lexer&quot;: &quot;ipython3&quot;,
   &quot;version&quot;: &quot;3.11.4&quot;
  },
  &quot;vscode&quot;: {
   &quot;interpreter&quot;: {
    &quot;hash&quot;: &quot;e7d420a2576d2f2cf4aee17bb1c719cb2b545f2d9fd7bdced2270e528bc643b9&quot;
   }
  }
 },
 &quot;nbformat&quot;: 4,
 &quot;nbformat_minor&quot;: 4
}
</description>
        <pubDate>Fri, 19 Jan 2024 00:00:00 +0000</pubDate>
        <link>http://localhost:4000/blog/2024/docker-tutorial-using-triton-server/</link>
        <guid isPermaLink="true">http://localhost:4000/blog/2024/docker-tutorial-using-triton-server/</guid>
        
        
      </item>
    
      <item>
        <title>Getting Started with Seldon-core and Kubernetes, Part 1: My Struggles with Kubernetes</title>
        <description>
&lt;figure&gt;
  &lt;p&gt;&lt;img src=&quot;/assets/posts/2023-03-07-struggles-with-docker/container-repair.png&quot; /&gt;&lt;/p&gt;
  &lt;figcaption&gt; Mechanic fixing container image by : &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Learning Kubernetes has been on my bucket list for years. I always said that when I had the time, I would learn it because it is one of those tools missing from my developer toolbox.&lt;/p&gt;

&lt;p&gt;In January, I decided to start my learning journey. I watched this amazing video, which gave me a basic overview of Kubernetes components, and I managed to install it on my local laptop.&lt;/p&gt;

&lt;p&gt;To practice, I used Kubernetes to deploy a machine learning model. After researching, I found out that there are currently two tools used to deploy machine learning models using Kubernetes: Kserve and Seldon-core. After struggling to run Kserve on my machine, I decided to go with Seldon-core because it was well-documented and seemed more mature compared to Kserve. While following the getting started tutorial on Seldon-core, I encountered some bugs that tested my knowledge of Kubernetes. In this post, I will write about some of them, how I encountered them, and the lessons I learned from them.&lt;/p&gt;

&lt;p&gt;The tutorial describes how to create a machine learning service on top of Kubernetes that will be used to make predictions.&lt;/p&gt;

&lt;p&gt;My troubles and bugs started when I ran the following command to create the Seldon deployment:&lt;/p&gt;

&lt;figure&gt;
  &lt;p&gt;&lt;img src=&quot;/assets/posts/2023-03-07-struggles-with-docker/seldon-core.png&quot; /&gt;&lt;/p&gt;
  &lt;figcaption&gt;Overview of Seldon Core Components&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;yaml
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;kubectl apply &lt;span class=&quot;nt&quot;&gt;-f&lt;/span&gt; - &lt;span class=&quot;o&quot;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;no&quot;&gt;END&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;
apiVersion: machinelearning.seldon.io/v1
kind: SeldonDeployment
metadata:
  name: iris-model
  namespace: seldon
spec:
  name: iris
  predictors:
  - graph:
      implementation: SKLEARN_SERVER
      modelUri: gs://seldon-models/v1.16.0-dev/sklearn/iris
      name: classifier
    name: default
    replicas: 1
&lt;/span&gt;&lt;span class=&quot;no&quot;&gt;END
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This command was supposed to start a SeldonDeployment, which consists of a deployment, a service, and a pod running the model. I hoped that the command would run successfully, but it didn’t. Over the past three days, I faced different errors that made me learn more about Kubernetes. Let me talk about the first one.&lt;/p&gt;

&lt;h2 id=&quot;downgrading-kubernetes-version-with-docker-desktop&quot;&gt;Downgrading Kubernetes version with Docker Desktop.&lt;/h2&gt;
&lt;p&gt;I don’t remember many details about the first bug I faced because I didn’t document it much. I remember using Docker Desktop as a backend for Kubernetes, and it was using Kubernetes 1.26. The fix for the issue was to use a lower version of Kubernetes, such as 1.24, but with Docker Desktop, there is no way to downgrade the version of Kubernetes. I had to switch to using Minikube; with it, I could specify the version of Kubernetes to use. Here is the command I used to downgrade it:&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-bash highlighter-rouge&quot;&gt;minikube start &lt;span class=&quot;nt&quot;&gt;--kubernetes-version&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;v1.24.1&lt;/code&gt;&lt;/p&gt;

&lt;h2 id=&quot;unable-to-pull-the-large-images-in-the-pod&quot;&gt;Unable to pull the large images in the pod.&lt;/h2&gt;

&lt;p&gt;After solving the first issue, I faced another one. I noticed my pod was not starting, so I decided to debug the pod to find out what was going wrong. When I checked the pod’s status, I found that it was stuck with this message: Error: ImagePullBackOff. I ran the following command:&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-bash highlighter-rouge&quot;&gt;kubectl describe pod PodName &lt;span class=&quot;nt&quot;&gt;-n&lt;/span&gt; podNamespace.&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;And I ended up with the following error message:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
  Normal   Scheduled  13m   default-scheduler  Successfully assigned seldon/xgboost-default-0-classifier-786f456bd4-jxjm6 to minikube
  Normal   Pulled     13m   kubelet   Container image &lt;span class=&quot;s2&quot;&gt;&quot;seldonio/rclone-storage-initializer:1.15.0&quot;&lt;/span&gt; already present on machine
  Normal   Created    13m   kubelet   Created container classifier-model-initializer
  Normal   Started    13m   kubelet   Started container classifier-model-initializer
  Warning  Failed     10m   kubelet   Error: ErrImagePull
  Warning  Failed     10m   kubelet   Failed to pull image &lt;span class=&quot;s2&quot;&gt;&quot;seldonio/xgboostserver:1.15.0&quot;&lt;/span&gt;: rpc error: code &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; Unknown desc &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; context deadline exceeded
  Normal   Pulled     10m   kubelet   Container image &lt;span class=&quot;s2&quot;&gt;&quot;docker.io/seldonio/seldon-core-executor:1.15.0&quot;&lt;/span&gt; already present on machine
  Normal   Created    10m   kubelet   Created container seldon-container-engine
  Normal   Started    10m   kubelet   Started container seldon-container-engine
  Normal   BackOff    9m58s &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;x2 over 9m59s&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;   kubelet   Back-off pulling image &lt;span class=&quot;s2&quot;&gt;&quot;seldonio/xgboostserver:1.15.0&quot;&lt;/span&gt;
  Warning  Failed     9m58s &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;x2 over 9m59s&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;   kubelet   Error: ImagePullBackOff
  Normal   Pulling    9m45s &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;x2 over 13m&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;     kubelet   Pulling image &lt;span class=&quot;s2&quot;&gt;&quot;seldonio/xgboostserver:1.15.0&quot;&lt;/span&gt;
  Warning  Unhealthy  3m14s &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;x84 over 9m39s&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;  kubelet   Readiness probe failed: HTTP probe failed with statuscode: 503
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The first lines of the logs show that Kubernetes could not pull the image for my container:&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-bash highlighter-rouge&quot;&gt;kubelet Failed to pull image &lt;span class=&quot;s2&quot;&gt;&quot;seldonio/xgboostserver:1.15.0&quot;&lt;/span&gt;: rpc error: code &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; Unknown desc &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; context deadline exceeded&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Initially, I thought that my pod was not connected to the internet because it could not pull the container image, but that was not the case. On closer inspection, I found that the pod could pull the &lt;code class=&quot;language-bash highlighter-rouge&quot;&gt;docker.io/seldonio/seldon-core-executor:1.15.0&lt;/code&gt; container image and was starting the executor container but not the MLServer image.&lt;/p&gt;

&lt;p&gt;After several hours of debugging, I discovered that the error was due to the size of my image and the timeout while pulling the image for the first time. The container was trying to pull the images, but it took a long time to pull, and the container timed out. After Googling, two possible workarounds were suggested:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Increase the size of the runtime timeout to a larger time and hope it will work.&lt;/li&gt;
  &lt;li&gt;Download the images with a separate command and run the container once the image is downloaded in the machine.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;To apply the first workaround, I had to run the following command:&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-bash highlighter-rouge&quot;&gt;minikube ssh &lt;span class=&quot;s2&quot;&gt;&quot;sudo sed -i 's/KUBELET_ARGS=/KUBELET_ARGS=--runtime-request-timeout=TIME /g' /etc/systemd/system/kubelet.service.d/10-kubeadm.conf &amp;amp;&amp;amp; sudo systemctl daemon-reload &amp;amp;&amp;amp; sudo systemctl restart kubelet&quot;&lt;/span&gt;&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;I tried it, but it didn’t work in my case, so I had to use the second method, which consists of downloading the image separately inside Minikube. I used the following command:&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-bash highlighter-rouge&quot;&gt;minikube image load seldonio/sklearnserver:1.15.0&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;The image was large and took approximately 10 minutes to download, maybe because my internet connection this weekend was not at its best. But after that, I passed that issue, but that was not all. There was another bug waiting for me:&lt;/p&gt;

&lt;figure&gt;
  &lt;p&gt;&lt;img src=&quot;/assets/posts/2023-03-07-struggles-with-docker/car-fails-to-start.jpeg&quot; /&gt;&lt;/p&gt;
  &lt;figcaption&gt;Car failing to start&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h2 id=&quot;readiness-probe-failed&quot;&gt;Readiness probe failed&lt;/h2&gt;

&lt;p&gt;When I described my pod, I found that the image was pulled and the container was running for a few seconds, and then it stopped with this message:&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-bash highlighter-rouge&quot;&gt;kubelet Readiness probe failed: HTTP probe failed with statuscode: 503&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;When I read about the error message, I found that Kubernetes uses readiness probes to know if a container is ready to accept traffic. The service keeps sending the request to the pod until the pod is ready to accept the traffic. So it was not passing that status. It is an error on the container side. But the container was stopped; how could I log in to a stopped container? I found this command:&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-bash highlighter-rouge&quot;&gt;kubectl logs podName &lt;span class=&quot;nt&quot;&gt;-c&lt;/span&gt; ContainerName &lt;span class=&quot;nt&quot;&gt;-n&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;seldon&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;By checking the log of the container to my satisfaction, I found that the container was failing because of an error with the Python image. I fixed it by changing the container image I used in my deployment. Then I had everything running.&lt;/p&gt;

&lt;p&gt;With the pod running and the service working, how could I connect to the container inside a cluster? I had to create an ingress component connecting to the external service serving my pod on port 9000, where the model was running. I did everything to set up the ingress, but I could not connect to the internal service on my Mac.&lt;/p&gt;

&lt;p&gt;I spent quite some time learning about Kubernetes networking and how services work, but networking is out of the scope of this tutorial. In the short term, I had to use tunneling to access my ingress from outside the container.&lt;/p&gt;

&lt;p&gt;Thanks to this &lt;a href=&quot;https://stackoverflow.com/a/73735009/4683950&quot;&gt;stackoverflow question&lt;/a&gt;, which provided the steps to solve the issue, I finally managed to access the deployment. However, when I attempted to access the URL, I discovered that it was not working and all of the endpoints on the server were returning a 404 error. Although I have not yet solved the issue, I plan to do so soon.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;Yes, I did struggles a lot, but this was a good learning lesson for me.  I learned how to debug containers on kubernetes and how minikube works with kuberenetes. I also learn some bit of kubernetes networking. I hope this post will serve my future self if I am facing the same issue as well as anyone else who is struggling with those bugs. My journey is not completed yet, I haven’t managed to deploy a large language model on Kubernetes, I am still struggling with that. In part two of this post I will talk about how I managed to deploy a transformer model with kubernetes.&lt;/p&gt;
</description>
        <pubDate>Tue, 07 Mar 2023 10:03:59 +0000</pubDate>
        <link>http://localhost:4000/struggles-with-docker</link>
        <guid isPermaLink="true">http://localhost:4000/struggles-with-docker</guid>
        
        <category>docker,</category>
        
        <category>kubernetes,</category>
        
        <category>devops</category>
        
        
      </item>
    
      <item>
        <title>Information Retrieval on the COVID-19 Open Research Dataset (CORD-19) Part one: TF-IDF and Cosine Similarity</title>
        <description>&lt;h2 id=&quot;scenario&quot;&gt;Scenario&lt;/h2&gt;

&lt;p&gt;In response to the COVID-19 pandemic, the White House and a coalition of leading research groups have prepared the &lt;a href=&quot;https://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge&quot;&gt;COVID-19&lt;/a&gt; Open Research Dataset (CORD-19). CORD-19 is a resource of over 181,000 scholarly articles, including over 80,000 with full text, about COVID-19, SARS-CoV-2, and related coronaviruses. This freely available dataset is provided to the global research community to apply recent advances in Information Retrieval and other AI techniques to generate new insights in support of the ongoing fight against this infectious disease. There is a growing urgency for these approaches because of the rapid acceleration in new coronavirus literature, making it difficult for the medical research community to keep up.&lt;/p&gt;

&lt;h2 id=&quot;the-task&quot;&gt;The task&lt;/h2&gt;
&lt;p&gt;For this tutorial, we will write an Information Retrieval pipeline that helps anyone to query the cord-19 dataset and find relevant articles for their queries.&lt;/p&gt;

&lt;p&gt;We will show two different approaches to building that pipeline. The first approach uses the TF-IDF and cosine similarity between the query and the articles. We will use Elasticsearch with the python API to search our articles for the second one.&lt;/p&gt;

&lt;p&gt;For the first step, we will leverage the TF-IDF vectorizer from Sklearn. In contrast, for the second one, we will leverage the python &lt;a href=&quot;https://github.com/elastic/elasticsearch-dsl-py&quot;&gt;elastic search dsl&lt;/a&gt;.  If you are  familiar with ORMs, this library is like an object-relational framework to interact with the elastic search database. If you have a background in python and some NLP experience, this is a fantastic tool for a smooth interaction with elastic search.&lt;/p&gt;

&lt;p&gt;This series is a part of an assignment I did for my Information Retrieval course. I decided to publish it online because of the lack of relevant tutorials on this topic.&lt;/p&gt;

&lt;p&gt;This will be a two parts tutorial.&lt;/p&gt;

&lt;p&gt;For the first part:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Data Collection:&lt;/strong&gt; In this section, we will go through downloading the dataset from Kaggle and saving it as a csv file.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Data Cleaning:&lt;/strong&gt; We will pre-process and clean the text.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Keyword selection:&lt;/strong&gt; We will use TF-IDF to find the appropriate keywords for each document.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Querying&lt;/strong&gt; : In this section we will query the article using TF-IDF.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For the second part:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Model Creation and Indexing: We will build our elastic search model and create our index in the database.&lt;/li&gt;
  &lt;li&gt;Querying: We will show to perform some simple queries on our database and preprocess the results.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For this series, I am assuming the following:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;You have elastic-search installed and running on your computer.&lt;/li&gt;
  &lt;li&gt;You have Python 3.7 installed.&lt;/li&gt;
  &lt;li&gt;You are familiar with the basics of text processing in Python and Object-Relational Mapping(ORM).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;If that is the case for you, let’s get started.&lt;/p&gt;

&lt;figure&gt;
  &lt;p&gt;&lt;img src=&quot;/assets/posts/2022-04-07-information-retrieval-on-medical-research-papers-about-covid19/information-retrieval-system.png&quot; /&gt;&lt;/p&gt;
  &lt;figcaption&gt;Our Information Retrieval pipeline&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h2 id=&quot;data-collection&quot;&gt;Data Collection&lt;/h2&gt;

&lt;p&gt;In this section, we will go through downloading the dataset from Kaggle and how to process it in chunks.&lt;/p&gt;

&lt;p&gt;The dataset is huge, and for this tutorial, we will use only the metadata release alongside the dataset.&lt;/p&gt;

&lt;h3 id=&quot;downloading-the-metadata-file&quot;&gt;Downloading the metadata file&lt;/h3&gt;

&lt;p&gt;The simplest way to download the data is to head to the &lt;a href=&quot;https://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge?select=metadata.csv&quot;&gt;competition link&lt;/a&gt; then select the metadata.csv file, and finally, click on the download button to download it and save it in a place in your local machine.&lt;/p&gt;

&lt;p&gt;The data comes as a zip file; you will have to unzip it to start using it.&lt;/p&gt;

&lt;h3 id=&quot;reading-the-dataset-with-dask&quot;&gt;Reading the dataset with Dask.&lt;/h3&gt;

&lt;p&gt;The dataset is huge. It has more than 1.5Gb. We will use dask and pandas to read the file to make our life easier.&lt;/p&gt;

&lt;p&gt;Make sure to have &lt;a href=&quot;https://docs.dask.org/en/stable/&quot;&gt;dask&lt;/a&gt; and &lt;a href=&quot;https://pandas.pydata.org/&quot;&gt;pandas&lt;/a&gt; installed in your environment for this project.&lt;/p&gt;

&lt;p&gt;With the file downloaded in your local machine, dask and pandas installed, let us write the first code to read the dataset.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt;  &lt;span class=&quot;nn&quot;&gt;pandas&lt;/span&gt;  &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt;  &lt;span class=&quot;nn&quot;&gt;pathlib&lt;/span&gt;  &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;Path&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt;  &lt;span class=&quot;nn&quot;&gt;dask.dataframe&lt;/span&gt;  &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;dd&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt;  &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt;  &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;If the libraries are well imported, let us read the file.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;DATA_PATH&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cwd&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;().&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;joinpath&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;data&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;metadata_path&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;DATA_PATH&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;joinpath&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;metadata.csv&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dd&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;read_csv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;metadata_path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;dtype&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;pubmed_id&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;object0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;s&quot;&gt;'arxiv_id'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;  &lt;span class=&quot;s&quot;&gt;'object'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;s&quot;&gt;'who_covidence_id'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;  &lt;span class=&quot;s&quot;&gt;'object'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;})&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Those lines, call the read_csv from function from dask to read the file and specify some columns data mapping.
Under the assumption that you have a data folder on the same level as this jupyter notebook and it is named &lt;code class=&quot;language-bash highlighter-rouge&quot;&gt;metadata.csv&lt;/code&gt;. If that is not the case , you can pass the exact path of your csv file to the &lt;code class=&quot;language-bash highlighter-rouge&quot;&gt;read_csv&lt;/code&gt; function.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Why is Dask faster than Pandas?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Remember, we are dealing with a large dataset. Pandas would like to load the whole data in memory. But for a small computer, 1.7 Gb is a lot of data to fit in memory, which would take us a lot of time.&lt;/p&gt;

&lt;p&gt;A workaround for this problem would be to read our dataset in different chunks with pandas. Dask is doing almost the same, it read the dataset in parallel using chunk but transparently for the end users.&lt;/p&gt;

&lt;p&gt;It avoids us writing a lot of code that will read the data in chunks, process each chunk in parallel, and combine the whole data in one dataset. Check this tutorial &lt;a href=&quot;https://pythonspeed.com/articles/faster-pandas-dask/&quot;&gt;here&lt;/a&gt; for more details about how dask is faster than pandas.&lt;/p&gt;

&lt;p&gt;Once our data is read, let us select a subset for our tutorial.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;important_columns&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;pubmed_id&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;s&quot;&gt;&quot;title&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;s&quot;&gt;&quot;abstract&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;s&quot;&gt;&quot;journal&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;s&quot;&gt;&quot;authors&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;s&quot;&gt;&quot;publish_time&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The dataset has 59k rows but for this exercice will will only work this the a sample of 1000 rows which is roughly the 1/60 of the whole dataframe&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;sample_df&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sample&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;frac&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;float&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;300&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;sample_df&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sample_df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dropna&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;subset&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;important_columns&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;sample_df&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sample_df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;important_columns&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We are still using a dask dataframe; let us convert it to a pandas dataframe.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;data_df&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sample_df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;compute&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This line creates the pandas dataframe we will be working with for the rest of the tutorial.
The bellow lines will drop null values in the abstract of the dataset and&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;data_df&lt;/span&gt;  &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;data_df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dropna&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;subset&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'abstract'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;rows&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;data_df&lt;/span&gt;  &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;data_df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;set_index&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;pubmed_id&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;data_df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;head&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;pubmed_id&lt;/th&gt;
      &lt;th&gt;title&lt;/th&gt;
      &lt;th&gt;abstract&lt;/th&gt;
      &lt;th&gt;journal&lt;/th&gt;
      &lt;th&gt;authors&lt;/th&gt;
      &lt;th&gt;publish_time&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;32165633&lt;/td&gt;
      &lt;td&gt;Acid ceramidase of macrophages traps herpes si…&lt;/td&gt;
      &lt;td&gt;Macrophages have important protective function…&lt;/td&gt;
      &lt;td&gt;Nat Commun&lt;/td&gt;
      &lt;td&gt;Lang, Judith; Bohn, Patrick; Bhat, Hilal; Jast…&lt;/td&gt;
      &lt;td&gt;2020-03-12&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;18325284&lt;/td&gt;
      &lt;td&gt;Resource Allocation during an Influenza Pandemic&lt;/td&gt;
      &lt;td&gt;Resource Allocation during an Influenza Pandemic&lt;/td&gt;
      &lt;td&gt;Emerg Infect Dis&lt;/td&gt;
      &lt;td&gt;Paranthaman, Karthikeyan; Conlon, Christopher …&lt;/td&gt;
      &lt;td&gt;2008-03-01&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;30073452&lt;/td&gt;
      &lt;td&gt;Analysis of pig trading networks and practices…&lt;/td&gt;
      &lt;td&gt;East Africa is undergoing rapid expansion of p…&lt;/td&gt;
      &lt;td&gt;Trop Anim Health Prod&lt;/td&gt;
      &lt;td&gt;Atherstone, C.; Galiwango, R. G.; Grace, D.; A…&lt;/td&gt;
      &lt;td&gt;2018-08-02&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;35017151&lt;/td&gt;
      &lt;td&gt;Pembrolizumab and decitabine for refractory or…&lt;/td&gt;
      &lt;td&gt;BACKGROUND: The powerful ‘graft versus leukemi…&lt;/td&gt;
      &lt;td&gt;J Immunother Cancer&lt;/td&gt;
      &lt;td&gt;Goswami, Meghali; Gui, Gege; Dillon, Laura W; …&lt;/td&gt;
      &lt;td&gt;2022-01-11&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;34504521&lt;/td&gt;
      &lt;td&gt;Performance Evaluation of Enterprise Supply Ch…&lt;/td&gt;
      &lt;td&gt;In order to make up for the shortcomings of cu…&lt;/td&gt;
      &lt;td&gt;Comput Intell Neurosci&lt;/td&gt;
      &lt;td&gt;Bu, Miaoling&lt;/td&gt;
      &lt;td&gt;2021-08-30&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;The dataframe contains the following columns :&lt;/p&gt;

&lt;p&gt;The pub med id is a unique id to identify the article.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Title: the title of the research paper&lt;/li&gt;
  &lt;li&gt;authors: the author of the paper&lt;/li&gt;
  &lt;li&gt;publish time: the date the paper was published 
The abstract is the abstract of the paper, which is the text we will use for indexing purposes for this work.
With our pandas dataset in place, let us move to the next part: text processing.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;text-processing&quot;&gt;Text Processing&lt;/h2&gt;

&lt;figure&gt;
  &lt;p&gt;&lt;img src=&quot;/assets/posts/2022-04-07-information-retrieval-on-medical-research-papers-about-covid19/text-processing-part.png&quot; /&gt;&lt;/p&gt;
  &lt;figcaption&gt;The text Processing Part&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;In this part, we will perform text processing. Our input is the raw text for the paper abstract for this section. Our output is the cleaned version of the input text.
Our preprocessing consists of the following steps: tokenization, lemmatization, stop word removal, lowercasing, special characters, and number removal.&lt;/p&gt;

&lt;p&gt;For this step, we will be using &lt;a href=&quot;https://spacy.io/usage&quot;&gt;Spacy&lt;/a&gt;,&lt;a href=&quot;https://www.nltk.org/install.html&quot;&gt;NLTK&lt;/a&gt;, and regular expressions to perform our preprocessing. Ensure you have spacy and NLTK installed before running the code in this section. Check also if you have installed the &lt;code class=&quot;language-bash highlighter-rouge&quot;&gt;en_core_web_sm&lt;/code&gt; package. If not install it from this &lt;a href=&quot;https://spacy.io/models/en#en_core_web_sm&quot;&gt;link&lt;/a&gt;&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt;  &lt;span class=&quot;nn&quot;&gt;spacy&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt;  &lt;span class=&quot;nn&quot;&gt;nltk&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt;  &lt;span class=&quot;nn&quot;&gt;re&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;nlp&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;spacy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;load&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'en_core_web_sm'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;stopwords_list&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nltk&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;corpus&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stopwords&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;words&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'english'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;tokenization&quot;&gt;Tokenization&lt;/h3&gt;

&lt;p&gt;Tokenization is the process of splitting a document into tokens, basically splitting a bunch of text into words. Spacy has a built-in tokenizer that helps us with this.&lt;/p&gt;

&lt;h3 id=&quot;stopwords-removal&quot;&gt;Stopwords removal.&lt;/h3&gt;

&lt;p&gt;Stop words are words that have no special significance when analyzing the text. Those words are frequent in the corpus but are useless for our analysis and example of them are &lt;strong&gt;&lt;em&gt;a&lt;/em&gt;&lt;/strong&gt;, &lt;strong&gt;&lt;em&gt;an&lt;/em&gt;&lt;/strong&gt;, &lt;strong&gt;&lt;em&gt;the&lt;/em&gt;&lt;/strong&gt;, &lt;strong&gt;&lt;em&gt;and&lt;/em&gt;&lt;/strong&gt; the like.&lt;/p&gt;

&lt;p&gt;The following function will perform stop word removal for us :&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt;  &lt;span class=&quot;nf&quot;&gt;remove_stopwords&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;text&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;is_lower_case&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;tokens&lt;/span&gt;  &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;nltk&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;word_tokenize&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;text&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;tokens&lt;/span&gt;  &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;  &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;token&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;strip&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;token&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tokens&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;is_lower_case&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;filtered_tokens&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;token&lt;/span&gt;  &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;token&lt;/span&gt;  &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;tokens&lt;/span&gt;  &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;token&lt;/span&gt;  &lt;span class=&quot;ow&quot;&gt;not&lt;/span&gt;  &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;stopwords_list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;filtered_tokens&lt;/span&gt;  &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;  &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;token&lt;/span&gt;  &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;token&lt;/span&gt;  &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;tokens&lt;/span&gt;  &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;token&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lower&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;  &lt;span class=&quot;ow&quot;&gt;not&lt;/span&gt;  &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;stopwords_list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;filtered_text&lt;/span&gt;  &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;  &lt;span class=&quot;s&quot;&gt;'  '&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;join&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;filtered_tokens&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;filtered_text&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;special-characters-and-number-removal&quot;&gt;Special characters and number removal.&lt;/h3&gt;

&lt;p&gt;Special characters and symbols are usually non-alphanumeric or occasionally numeric characters which add extra noise in unstructured text. For our problem, since our corpus is built with articles from the biomedical field, there are a lot of numbers denoting quantities and dosages. We have decided to remove them to simplify the tutorial.&lt;/p&gt;

&lt;h3 id=&quot;lematization&quot;&gt;Lematization&lt;/h3&gt;

&lt;p&gt;In this step, we will use lemmatization instead of stemming,&lt;/p&gt;

&lt;p&gt;Chirstopher Maning define lemmatization as :&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Lemmatization usually refers to doing things properly with the use of a vocabulary and morphological analysis of words, normally aiming to remove inflectional endings only and to return the base or dictionary form of a word, which is known as the lemma . If confronted with the token saw, stemming might return just s, whereas lemmatization would attempt to return either see or saw depending on whether the use of the token was as a verb or a noun. The two may also differ in that stemming most commonly collapses derivationally related words, whereas lemmatization commonly only collapses the different inflectional forms of a lemma.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;A good lemmatizer will replace words such as foot by feet; chosen, choose, by choice.; etc.&lt;/p&gt;

&lt;p&gt;This approach has some advantages because it will help not spread the information between different word forms derived from the same lemma. Therefore, it will lead to an accurate TF-IDF because the same semantic information is assembled in one place.&lt;/p&gt;

&lt;p&gt;The code for lemmatization is as follow :&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt;  &lt;span class=&quot;nf&quot;&gt;lemmatize_text&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;text&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;text&lt;/span&gt;  &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;nlp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;text&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;text&lt;/span&gt;  &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;  &lt;span class=&quot;s&quot;&gt;'  '&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;join&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;word&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lemma_&lt;/span&gt;  &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;word&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lemma_&lt;/span&gt;  &lt;span class=&quot;o&quot;&gt;!=&lt;/span&gt;  &lt;span class=&quot;s&quot;&gt;'-PRON-'&lt;/span&gt;  &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;word&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;text&lt;/span&gt;  &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;word&lt;/span&gt;  &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;text&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;text&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Finally, we apply the preprocessing function to our dataset to generate a cleaned version for each abstract.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;preprocess_text&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;text&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;text_lower_case&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;text_lemmatization&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;stopword_removal&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;  
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;text_lower_case&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;text&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;text&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lower&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;().&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;strip&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    
    &lt;span class=&quot;n&quot;&gt;text&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;re&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sub&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;r&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'[^\w\s]'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;''&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;text&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;text&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;re&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sub&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;r&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'[\r|\n|\r\n]+'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;' '&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;text&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;text&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;re&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sub&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;r&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'\d+'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;''&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;text&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;text_lemmatization&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;text&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lemmatize_text&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;text&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;text&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;re&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sub&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;' +'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;' '&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;text&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;c1&quot;&gt;# remove stopwords
&lt;/span&gt;    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;stopword_removal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;text&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;remove_stopwords&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;text&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;is_lower_case&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;text_lower_case&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;text&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;data_df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'abstract_cleaned'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;  &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;data_df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'abstract'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;apply&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;preprocess_text&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;With our text cleaned, we can move to our tutorial’s next section, which generates the most relevant keywords for each abstract.&lt;/p&gt;

&lt;h3 id=&quot;keyword-generation-using-term-inverse---document-frequency-tf-idf&quot;&gt;Keyword Generation using Term Inverse - Document Frequency (Tf-IDF)&lt;/h3&gt;

&lt;p&gt;To generate keywords for each paper, we have to find a heuristic that finds the most relevant words while penalizing the common phrase in our corpus. Practitioners have widely used the Term Frequency-Inverse Document Frequency (TF-IDF) to generate important keywords in documents in Information Retrieval. But what is TF-IDF? It combines two metrics, the Term frequency and the Inverse Document Frequency.&lt;/p&gt;

&lt;h4 id=&quot;term-frequency&quot;&gt;Term Frequency&lt;/h4&gt;

&lt;p&gt;[K. Sparck Jones.] defines the term frequency (TF) as a numerical statistic that reflects how important a word is to document in a collection or a corpus. It is the relative frequency of term w within the document d.&lt;/p&gt;

&lt;p&gt;It is computed  using the following formula :&lt;/p&gt;

\[\begin{equation}
    tf(w,d) = \frac{f_{w,d}}{\sum_{t\ast}^{d}f_{w\ast,d}}
\end{equation}\]

&lt;p&gt;with \(f_{w,d}\) defined as the raw count of the word w in the document d, and \(\sum_{t\ast}^{d}f_{w\ast,d}\) as the total number of terms in document d (counting each occurrence of the same term separately).&lt;/p&gt;

&lt;h4 id=&quot;inverse-document-frequency&quot;&gt;Inverse Document Frequency&lt;/h4&gt;

&lt;p&gt;The inverse document frequency is defined as the log of the ratio between the total number of documents in the 
corpus and the number of documents with the word. It is a measure the amount of information provided by the word.
\(\begin{equation}
    idf(w, d) = log\frac{N}{1 + (\left | d \in D : w \in d \right |)}
\end{equation}\)
with&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;N: total number of documents in the corpus N  and the  denominator represent the number of documents with the word w.
This helps to penalize the most common word in a corpus. Those words carry fewer values for in the corpus.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For the curious who want to know why we use the log in the IDF, check out &lt;a href=&quot;https://stackoverflow.com/a/33429876/4683950&quot;&gt;this answer&lt;/a&gt; from StackOverflow.&lt;/p&gt;

&lt;p&gt;The TF-IDF combines both the Term Frequency and the Inverse Document Frequency.&lt;/p&gt;

\[(tf_idf)_{t,d} = Idf_t * TF_{w, d}\]

&lt;h4 id=&quot;applying-tf-idf-to-our-corpus&quot;&gt;Applying TF-IDF to our corpus&lt;/h4&gt;

&lt;p&gt;To apply TF-IDF we will leverage the sklearn implementation of the algorithm.
Before running the bellow code, make sure you have &lt;a href=&quot;https://scikit-learn.org/stable/about.html&quot;&gt;sklearn&lt;/a&gt; installed.&lt;/p&gt;

&lt;p&gt;If the sklearn is installed, you can import it with the bellow code.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt;  &lt;span class=&quot;nn&quot;&gt;sklearn.feature_extraction.text&lt;/span&gt;  &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;TfidfVectorizer&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;create_tfidf_features&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;corpus&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;max_features&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;max_df&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.95&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;min_df&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot; Creates a tf-idf matrix for the `corpus` using sklearn. &quot;&quot;&quot;&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;tfidf_vectorizor&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;TfidfVectorizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;decode_error&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'replace'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;strip_accents&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'unicode'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;analyzer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'word'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
                                       &lt;span class=&quot;n&quot;&gt;stop_words&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'english'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ngram_range&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;max_features&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;max_features&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
                                       &lt;span class=&quot;n&quot;&gt;norm&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'l2'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;use_idf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;smooth_idf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sublinear_tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                                       &lt;span class=&quot;n&quot;&gt;max_df&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;max_df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;min_df&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;min_df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tfidf_vectorizor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit_transform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;corpus&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'tfidf matrix successfully created.'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tfidf_vectorizor&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;&lt;a href=&quot;https://melaniewalsh.github.io/Intro-Cultural-Analytics/05-Text-Analysis/03-TF-IDF-Scikit-Learn.html&quot;&gt;This article&lt;/a&gt; recommended  to use  &lt;code class=&quot;language-bash highlighter-rouge&quot;&gt;TfidfVectorizer&lt;/code&gt;  with smoothing (&lt;code class=&quot;language-bash highlighter-rouge&quot;&gt;smooth_idf  &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;  True&lt;/code&gt;) and normalization (&lt;code class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;span class=&quot;nv&quot;&gt;norm&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'l2&quot;) turned on. These parameters will better account for text length differences and produce more meaningful to–IDF scores. Smoothing and L2 normalization are the default settings for &lt;/span&gt;&lt;/code&gt;TfidfVectorizer,` so you don’t need to include any extra code at all to turn them on.&lt;/p&gt;

&lt;p&gt;On top of the &lt;code class=&quot;language-bash highlighter-rouge&quot;&gt;smoth_idf&lt;/code&gt;  and &lt;code class=&quot;language-bash highlighter-rouge&quot;&gt;norm&lt;/code&gt; hyperparameters, the other keys hyperparameters are :&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-bash highlighter-rouge&quot;&gt;max_features&lt;/code&gt; which denotes the max number of words to keep in our vocabulary&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-bash highlighter-rouge&quot;&gt;max_df&lt;/code&gt;: When building the vocabulary, ignore terms that have a document frequency strictly higher than the given threshold&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-bash highlighter-rouge&quot;&gt;min_df:&lt;/code&gt; When building the vocabulary, ignore terms that have a document frequency strictly lower than the given threshold. This value is also called a cut-off in the literature.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-bash highlighter-rouge&quot;&gt;n_gram_range&lt;/code&gt; is the number of n-grams to consider when building our vocabulary; for this task, we consider nonograms, bigrams, and trigrams.&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;data_df&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data_df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reset_index&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;tf_idf_matrix&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf_idf_vectorizer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;create_tfidf_features&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data_df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'abstract_cleaned'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;After applying the tf_if vectorizer on to our corpus, it will result in the following two objects :&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;The &lt;code class=&quot;language-bash highlighter-rouge&quot;&gt;tf_ifd matrix&lt;/code&gt; , is a matrix where rows are the documents and columns are the words in our vocabulary.&lt;/li&gt;
  &lt;li&gt;The &lt;code class=&quot;language-bash highlighter-rouge&quot;&gt;tf_idf_vectorizer&lt;/code&gt; is an object that will help us to transform a new document to the TF-IDF version.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The value at the ith row and jth column is the TF-IDF score of the word j in document i.&lt;/p&gt;

&lt;p&gt;For better analysis we converted the &lt;code class=&quot;language-bash highlighter-rouge&quot;&gt;tf_idf_matrix&lt;/code&gt; into a pandas dataframe using the following code :&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;tfidf_df&lt;/span&gt;  &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;DataFrame&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf_idf_matrix&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;toarray&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;columns&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf_idf_vectorizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_feature_names&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;index&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data_df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;index&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The next step is to generate the top 20 keywords for each document, those word are the word with the highest tf-idf score within the document.&lt;/p&gt;

&lt;p&gt;Before doing that, let’s reorganize the DataFrame so that the words are in rows rather than columns.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;tfidf_df&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tfidf_df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sort_index&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;().&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;round&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;decimals&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;tfidf_df_stacked&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tfidf_df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stack&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;().&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reset_index&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;tfidf_df_stacked&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tfidf_df_stacked&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rename&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;columns&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'tfidf'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'level_1'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'term'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;level_0&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;doc_id&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;})&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We sort by document and tfidf score and then groupby document and take the first 20 values.
Once we have sorted and find the top keywords we can save them in a dictionary where the keys are the the document id and the values are the another dictionary of the term and their tf-idf score.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;tfidf_df_stacked&lt;/span&gt;  &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;tfidf_df_stacked&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reset_index&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;().&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rename&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;columns&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'level_1'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'term'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;})&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;document_tfidf&lt;/span&gt;  &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;tfidf_df_stacked&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;groupby&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'doc_id'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]).&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;apply&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;lambda&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'term'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;s&quot;&gt;&quot;tfidf&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]].&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;set_index&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;term&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;to_dict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;().&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'tfidf'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;With our documents and the top keyword mappings, we can now visualize what our corpus looks like to have an idea on each paper on the document.&lt;/p&gt;

&lt;p&gt;I recently come across a good piece of code that makes visualization for a document using TF-IDF.&lt;/p&gt;

&lt;p&gt;I grabbed it from this &lt;a href=&quot;https://melaniewalsh.github.io/Intro-Cultural-Analytics/05-Text-Analysis/03-TF-IDF-Scikit-Learn.html&quot;&gt;article&lt;/a&gt;.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;altair&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alt&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Terms in this list will get a red dot in the visualization
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;term_list&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;covid&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'traitement'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'ebola'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# adding a little randomness to break ties in term ranking
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;top_tfidf_plusRand&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tfidf_df_stacked&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;800&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;top_tfidf_plusRand&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'tfidf'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;top_tfidf_plusRand&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'tfidf'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rand&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tfidf_df_stacked&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;800&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.0001&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# base for all visualizations, with rank calculation
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;base&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Chart&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;top_tfidf_plusRand&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;encode&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'rank:O'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'doc_id:N'&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;transform_window&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;rank&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;rank()&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;sort&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;alt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;SortField&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;tfidf&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;order&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;descending&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)],&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;groupby&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;doc_id&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# heatmap specification
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;heatmap&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;base&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mark_rect&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;().&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;encode&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;color&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'tfidf:Q'&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# red circle over terms in above list
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;circle&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;base&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mark_circle&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;encode&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;color&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;condition&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;alt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;FieldOneOfPredicate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;field&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'term'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;oneOf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;term_list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;alt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'red'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;alt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'#FFFFFF00'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;        
    &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# text labels, white for darker heatmap colors
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;text&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;base&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mark_text&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;baseline&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'middle'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;encode&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;text&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'term:N'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;color&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;condition&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;alt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;datum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tfidf&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.23&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'white'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'black'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# display the three superimposed visualizations
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;heatmap&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;circle&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;text&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;properties&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;width&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1200&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;figure&gt;
  &lt;p&gt;&lt;img src=&quot;/assets/posts/2022-04-07-information-retrieval-on-medical-research-papers-about-covid19/tf-idf-chart.png&quot; /&gt;&lt;/p&gt;
  &lt;figcaption&gt;each document and top 10 terms&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h4 id=&quot;querying-using-tf-idf&quot;&gt;Querying using tf-idf&lt;/h4&gt;

&lt;figure&gt;
  &lt;p&gt;&lt;img src=&quot;/assets/posts/2022-04-07-information-retrieval-on-medical-research-papers-about-covid19/query-processing-tfidf.png&quot; /&gt;&lt;/p&gt;
  &lt;figcaption&gt;The text Processing Part&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;With our TF-IDF we could easily use it to run search and make queries that use the TF-IDF score and cosine similarty.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn.metrics.pairwise&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cosine_similarity&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;calculate_similarity&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;query&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf_idf_matrix&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vectorizor&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf_idf_vectorizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;top_k&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot; Vectorizes the `query` via `vectorizor` and calculates the cosine similarity of
    the `query` and `X` (all the documents) and returns the `top_k` similar documents.
    &quot;&quot;&quot;&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# Vectorize the query to the same length as documents
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;query_vec&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vectorizor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;transform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;query&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# Compute the cosine similarity between query_vec and all the documents
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;cosine_similarities&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cosine_similarity&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;query_vec&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;flatten&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# Sort the similar documents from the most similar to less similar and return the indices
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;most_similar_doc_indices&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;argsort&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cosine_similarities&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)[:&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;top_k&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;most_similar_doc_indices&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cosine_similarities&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;show_similar_documents&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cosine_similarities&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;similar_doc_indices&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot; Prints the most similar documents using indices in the `similar_doc_indices` vector.&quot;&quot;&quot;&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;counter&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;index&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;similar_doc_indices&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Top-{}, Similarity = {}'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;counter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cosine_similarities&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;index&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]))&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;pubmed_id&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;index&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'pubmed_id'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'the pubmed id : {}, '&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pubmed_id&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'title {}'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data_df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;index&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;title&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]))&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;abstract {}&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data_df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;index&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;abstract&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;50&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]))&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;counter&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'**=='&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The above code, get our new query, generate it TF-IDF  vector. Then it computes the cosine similarity between the vectors and all our documents in the TF-IDF matrix.&lt;/p&gt;

&lt;p&gt;As the result, it returns the top n rows in the matrix which are similar to our query vector.&lt;/p&gt;

&lt;p&gt;Let us try to check how it works in practice.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;time&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;query&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'are gorillas responsible of ebola'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;search_start&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;time&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;time&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;sim_vecs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cosine_similarities&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;calculate_similarity&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;query&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;search_time&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;time&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;time&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;search_start&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;search time: {:.2f} ms&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;search_time&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;show_similar_documents&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data_df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cosine_similarities&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sim_vecs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;search &lt;span class=&quot;nb&quot;&gt;time&lt;/span&gt;: 7.60 ms

Top-1, Similarity &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; 0.25484833157402037
the pubmed &lt;span class=&quot;nb&quot;&gt;id&lt;/span&gt; : 32287784, 
title Wuhan virus spreads
abstract We now know the virus responsible &lt;span class=&quot;k&quot;&gt;for &lt;/span&gt;deaths and i
&lt;span class=&quot;k&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;
Top-2, Similarity &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; 0.2332808978421972
the pubmed &lt;span class=&quot;nb&quot;&gt;id&lt;/span&gt; : 32162604, 
title How Is the World Responding to the Novel Coronavirus Disease &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;COVID-19&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; Compared with the 2014 West African Ebola Epidemic? The Importance of China as a Player &lt;span class=&quot;k&quot;&gt;in &lt;/span&gt;the Global Economy
abstract This article describes similarities and difference
&lt;span class=&quot;k&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;
Top-3, Similarity &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; 0.1286925639778678
the pubmed &lt;span class=&quot;nb&quot;&gt;id&lt;/span&gt; : 19297495, 
title Aquareovirus effects syncytiogenesis by using a novel member of the FAST protein family translated from a noncanonical translation start site.
abstract As nonenveloped viruses, the aquareoviruses and or
&lt;span class=&quot;k&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;
Top-4, Similarity &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; 0.11561157619559267
the pubmed &lt;span class=&quot;nb&quot;&gt;id&lt;/span&gt; : 27325914, 
title Consortia critical role &lt;span class=&quot;k&quot;&gt;in &lt;/span&gt;developing medical countermeasures &lt;span class=&quot;k&quot;&gt;for &lt;/span&gt;re-emerging viral infections: a USA perspective.
abstract Viral infections, such as Ebola, severe acute resp
&lt;span class=&quot;k&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;
Top-5, Similarity &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; 0.10755790364315998
the pubmed &lt;span class=&quot;nb&quot;&gt;id&lt;/span&gt; : 33360484, 
title Neuropathological explanation of minimal COVID-19 infection rate &lt;span class=&quot;k&quot;&gt;in &lt;/span&gt;newborns, infants and children – a mystery so far. New insight into the role of Substance P
abstract Sars-Cov-2 or Novel coronavirus infection &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;COVID-1
&lt;span class=&quot;k&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;That is all for the first part of this tutorial , We have learned how to build TF-IDF vectors, and how to leverage the cosine similarity to compute and retrieve documents that matches a query. In the second part of the tutorial we will learn how to use elasticsearch to perform the same task.&lt;/p&gt;
</description>
        <pubDate>Thu, 07 Apr 2022 13:03:59 +0100</pubDate>
        <link>http://localhost:4000/information-retrieval-on-medical-research-papers-about-covid19</link>
        <guid isPermaLink="true">http://localhost:4000/information-retrieval-on-medical-research-papers-about-covid19</guid>
        
        <category>python,</category>
        
        <category>information-retrieval,</category>
        
        <category>elastic-search,</category>
        
        <category>covid19,</category>
        
        <category>cord-19</category>
        
        
      </item>
    
      <item>
        <title>Congolese 🇨🇩 , Africans young Software Engineers, and Computer Scientists graduates - let’s get that master's degree! 🚀 🚀🚀🎓</title>
        <description>
&lt;figure&gt;
  &lt;p&gt;&lt;img src=&quot;/assets/posts/2022-04-06-master-options-africans/cmu-africa-graduation.png&quot; /&gt;&lt;/p&gt;
  &lt;figcaption&gt;CMU Africa Graduation ceremony class of 2019, I need someone to put the 🇨🇩 on that picture next time&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;I had been dreaming about attending a high-ranked university for ages. The interaction with brilliant professors, lecturers, the smell of old books in a library, and the social aspect of university life have always fascinated me.&lt;/p&gt;

&lt;p&gt;After graduating from my local university in my native country - Congo, I decided to gain some real-world experience by working instead of going straight for a master’s degree. Six years passed since I graduated, and I yearned to build a strong career in the Software Engineering industry. I wanted to realize my dream, and I decided to apply for a Master in Data Science in the UK. Fortunately, I got an offer from the University of Essex, thanks for the Chevening Scholarship. For those who don’t know I am currently doing my master at that university, my experience about it are mixed but that is out of the scope of this blog  🤪.&lt;/p&gt;

&lt;p&gt;As a Software Engineer, there are different ways to learn. I have gone through all of them:  tutorials, coding Bootcamp, Youtube videos, MOOC, personnel projects, and the academic way. Nevertheless, I can attest, that as long as we live in today’s world and economy, a &lt;strong&gt;high-ranked&lt;/strong&gt; university is still the best, the most legit, and the most credible way to learn and acquire new skills, especially when it comes to research. I am not opening the old debate on the best way to learn as a Software Engineer, and it is beyond the scope of this blog. Instead, I will share some opportunities for young talented graduates from the continent in general and DRCongo particularly, on attending high-ranked universities globally.&lt;/p&gt;

&lt;p&gt;As a young African growing up in DRCongo, there are fewer options for good universities than someone born and raised in Europe or North America.&lt;/p&gt;

&lt;p&gt;Fortunately, there have been initiatives to improve inclusivity for people from developing countries or of disadvantaged backgrounds in academia over the past decade.&lt;/p&gt;

&lt;p&gt;Following up on a recent discussion with an old friend of mine in a WhatsApp group, I decided to put some options young graduates with a similar background as me can access  to get a good master’s in computer science or related field.&lt;/p&gt;

&lt;p&gt;I will talk about some of them below, and the list is not exhaustive. In conclusion, I will share some of my tips to get opportunities in my career.&lt;/p&gt;

&lt;h2 id=&quot;carnegie-mellon-university--africa&quot;&gt;&lt;a href=&quot;https://www.cmu.edu/africa/&quot;&gt;Carnegie Mellon University  AFRICA&lt;/a&gt;&lt;/h2&gt;

&lt;figure&gt;
  &lt;p&gt;&lt;img src=&quot;/assets/posts/2022-04-06-master-options-africans/cmu-africa-campus.jpeg&quot; /&gt;&lt;/p&gt;
  &lt;figcaption&gt;CMU Africa campus, I am sure there are some Europeans universities campuses which are not as beautiful as this campus&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;The CMU Africa is an extension of the main &lt;a href=&quot;https://www.cmu.edu/&quot;&gt;CMU,&lt;/a&gt; which is one of the best computer science universities in the world. In 2011, following up on a recent partnership with the (African Bank of Development)BAD and the World Bank, CMU has decided to open its door in Rwanda to bring high-quality education to Africa. It is the only U.S. research university offering its master’s degrees with a full-time faculty, staff, and operations in Africa.&lt;/p&gt;

&lt;p&gt;Since 2018, I have been interacting with CMU graduates from all over Africa. I can attest it is one of the best masters out there. CMU is the pillar of the Rwanda and East African tech scene. Their graduates are working for top-notch tech companies locally. Some of my CMU Alumni friends are working for Google, IBM, or doing Ph.D. in the main CMU campus or Oxford.&lt;/p&gt;

&lt;p&gt;Here are some of the alumni from the university I got a chance to interact with :&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.linkedin.com/in/kesaoluwafunmilola/&quot;&gt;Oluwafunmilola Kesa&lt;/a&gt;, Software Engineer at Microsoft, Vancouver Canada&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.cs.ox.ac.uk/people/daniel.omeiza/&quot;&gt;Daniel Omeiza&lt;/a&gt;, Ph.D. Student at University of Oxford, Oxford UK&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://ng.linkedin.com/in/azeez-oluwafemi&quot;&gt;Azeez Oluwafemi&lt;/a&gt;, Research Engineer at Borealis AI, Vancouver Canada.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://ug.linkedin.com/in/joan-kirunga&quot;&gt;Joan Kirunga&lt;/a&gt; , Senior Data Scientist at BBox, Kigali Rwanda&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.linkedin.com/in/victorakinwande/&quot;&gt;Victor Akinwande&lt;/a&gt;, Phd Student, at the main CMU in Pennsylvania, USA&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;what-are-the-costs&quot;&gt;What are the costs:&lt;/h3&gt;

&lt;p&gt;I have met some students who got scholarships from the Rwandan government if they are Rwandans or the &lt;a href=&quot;https://www.africa.engineering.cmu.edu/impact/mastercard-scholars.html&quot;&gt;prestigious Mastercard Scholarship.&lt;/a&gt; According to &lt;a href=&quot;https://www.africa.engineering.cmu.edu/admissions/tuition/index.html&quot;&gt;their website,&lt;/a&gt; the tuition fees can range from 8k to 16k USD per year. If you have a good track of leadership and community-building experience, you can easily find a scholarship that funds your studies at CMU Africa.&lt;/p&gt;

&lt;h2 id=&quot;africa-master-of-machine-intelligence&quot;&gt;&lt;a href=&quot;https://aimsammi.org/&quot;&gt;Africa Master of Machine Intelligence&lt;/a&gt;&lt;/h2&gt;

&lt;figure&gt;
  &lt;p&gt;&lt;img src=&quot;/assets/posts/2022-04-06-master-options-africans/ammi-launch-8.jpeg&quot; /&gt;&lt;/p&gt;
  &lt;figcaption&gt;AMMI Kigali launch&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;In 2019, with a partnership with Google and Facebook, &lt;a href=&quot;https://medium.com/@moustaphacisse/introducing-the-african-masters-of-machine-intelligence-at-aims-9441b4346a55&quot;&gt;Moustapha Cisse launched&lt;/a&gt; the African Master of Machine Intelligence.&lt;/p&gt;

&lt;p&gt;AMMI is a one-year master’s to help passionate young Africans access cutting-edge research on Artificial Intelligence without leaving the country and going through the hassle of a student visa. I have seen top researchers from (University College of London) UCL or Google lecturing at the university. In 2019, I had a chance to meet &lt;a href=&quot;https://deisenroth.cc/&quot;&gt;Marc Peter Deisenroth&lt;/a&gt;, who was lecturing at the university. He wrote one of the best books on &lt;a href=&quot;https://books.google.co.uk/books?id=pbONxAEACAAJ&amp;amp;printsec=frontcover&amp;amp;source=gbs_ge_summary_r&amp;amp;cad=0#v=onepage&amp;amp;q&amp;amp;f=false&quot;&gt;Mathematic of Machine Learning.&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The final goal of the university is to produce talent for the Google office in Accra. I can attest I know some of their graduates who joined the Google Accra office and are doing well. I also know others who went for Ph.D. in top universities in Europe.&lt;/p&gt;

&lt;p&gt;Here are some alums from Congo I had a chance to interact with :&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://aimsammi.org/person/muhigiri-cirhuza-alain/&quot;&gt;Muhigiri Chiruza Alain&lt;/a&gt;, The guy is one of those smart people who don’t have an online profile, I am sure he is doing well, maybe doing a Ph.D. at Standford or somewhere.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.linkedin.com/in/salomon-kabongo&quot;&gt;Salomon Kabongo&lt;/a&gt;, Phd Student at L3S Research, Leipzig Germany&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;what-are-the-costs-&quot;&gt;What are the costs :&lt;/h3&gt;

&lt;p&gt;The program is highly competitive and only takes the best talent. Its acceptance rate, which is six percent, makes it one of the most prestigious masters in Africa. Fortunately, it is free; Google and Facebook cover your tuition fees. If you are one of those guys who could quickly get 72 % in undergrad, you have a solid background in Mathematics and Computer Science, and you are passionate about Machine Learning, you should give this program a try.&lt;/p&gt;

&lt;h2 id=&quot;chevening-scholarship&quot;&gt;&lt;a href=&quot;https://www.chevening.org/&quot;&gt;Chevening scholarship.&lt;/a&gt;&lt;/h2&gt;

&lt;figure&gt;
  &lt;p&gt;&lt;img src=&quot;/assets/posts/2022-04-06-master-options-africans/chevener-from-drc.jpeg&quot; /&gt;&lt;/p&gt;
  &lt;figcaption&gt;Chevener from DRC 2022 cohort , caption me if you can 🤪 &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;The program has been there for more than 30 years and provides African leaders the chance to study at any British university of their choice fully funded. Suppose you manage to get a place in one of the best UK universities for Machine Learning or Data Science, Chevening will cover your tuition fees.&lt;/p&gt;

&lt;p&gt;I can guarantee, that if you have dreamt about studying at one of the best British Universities, you can try this scholarship. As a Chevening scholar, I can attest that this is one of the best scholarships you can get if you are admitted to a good university. It gives you access to a good network of computer scientists and people from different backgrounds.&lt;/p&gt;

&lt;p&gt;Chevening alumni are currently among the best leaders in their respective countries.&lt;/p&gt;

&lt;p&gt;Some Alums I have interacted with :&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://www.linkedin.com/in/mbithenzomo/&quot;&gt;Mbithe Nzomo&lt;/a&gt;, Doctoral Researcher, University of Cape Town, South Africa&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;And the list goes on, there handful of Cheveners who did Computer Science and are doing well in life. Head to &lt;a href=&quot;https://www.chevening.org/alumni/&quot;&gt;this link&lt;/a&gt; to learn more about them.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;what-are-the-costs--1&quot;&gt;What are the costs :&lt;/h3&gt;

&lt;p&gt;It is a fully-funded scholarship, and you don’t need to pay anything. You are only required to have two years of work experience and good leadership background. As part of the application process, you will have to write four essays. These are your leadership and Networking skills, your course of studies, and your career plan. The application process is lengthy, but it is worth it. To be honest with you, it is one of the best scholarships in the world as long as you get a place to a good university in UK. One quack, is that part of the terms and conditions of the scholarship you have to return to your home country for at least two years.&lt;/p&gt;

&lt;h2 id=&quot;conclusions&quot;&gt;Conclusions&lt;/h2&gt;

&lt;p&gt;Those are the top three programs any African Engineer who wants to go for a master’s in Computer science can think about. The list is not exhaustive; there are more programs out there, especially South African universities. I am still collecting information about them.
Should I get more time and energy in the future, I will post them in part two of this series. I will share more advice on how to get access to top research papers in the world without attending a university. Additionally, I will share some undergraduate universities you can dream about.
Until my next feature, stay safe, take care, and see you in part two of this series.&lt;/p&gt;
</description>
        <pubDate>Wed, 06 Apr 2022 11:22:56 +0100</pubDate>
        <link>http://localhost:4000/master-options-africans</link>
        <guid isPermaLink="true">http://localhost:4000/master-options-africans</guid>
        
        
        <category>non-tech</category>
        
      </item>
    
      <item>
        <title>install nvidia driver</title>
        <description>&lt;p&gt;I had many driver installed I my virtual machine , so It was actually the reason why I was having the error.&lt;/p&gt;

&lt;p&gt;To fix it I had first to remove all driver I have installed before using :&lt;/p&gt;

&lt;p&gt;-&lt;code class=&quot;language-clojure highlight highlighter-rouge&quot;&gt;&lt;span class=&quot;n&quot;&gt;sudo&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;apt-get&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;purge&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nvidia-*&lt;/span&gt;&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;-&lt;code class=&quot;language-clojure highlight highlighter-rouge&quot;&gt;&lt;span class=&quot;n&quot;&gt;sudo&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;apt-get&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;update&lt;/span&gt;&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;-&lt;code class=&quot;language-clojure highlight highlighter-rouge&quot;&gt;&lt;span class=&quot;n&quot;&gt;sudo&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;apt-get&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;autoremove&lt;/span&gt;&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;After that I when a head and installed the latest version of it nvidia driver:&lt;/p&gt;

&lt;p&gt;I did :&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-clojure highlight highlighter-rouge&quot;&gt;&lt;span class=&quot;n&quot;&gt;apt&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;search&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nvidia-driver&lt;/span&gt;&lt;/code&gt; 
To get the latest version of the driver
After getting the latest version I installed it with :&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Edit Sept 2021 : According to the last comment  by @a-r-j  you can install a couple of dependencies before&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-clojure highlight highlighter-rouge&quot;&gt;&lt;span class=&quot;n&quot;&gt;sudo&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;apt&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;install&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;libnvidia-common-470&lt;/span&gt;&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-clojure highlight highlighter-rouge&quot;&gt;&lt;span class=&quot;n&quot;&gt;sudo&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;apt&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;install&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;libnividia-gl-470&lt;/span&gt;&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Then you can move forward and install the driver.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-clojure highlight highlighter-rouge&quot;&gt;&lt;span class=&quot;n&quot;&gt;sudo&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;apt&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;install&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nvidia-driver-470&lt;/span&gt;&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;And after installing it I rebooted my machine and checked with :&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-clojure highlight highlighter-rouge&quot;&gt;&lt;span class=&quot;n&quot;&gt;nvidia-smi&lt;/span&gt;&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;And tata ☄️&lt;/p&gt;

&lt;p&gt;The results :&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://imgur.com/a/xfpvrtb.jpg&quot; alt=&quot;Imgur Image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Ressources :&lt;/p&gt;

&lt;p&gt;https://www.cyberciti.biz/faq/ubuntu-linux-install-nvidia-driver-latest-proprietary-driver/&lt;/p&gt;

</description>
        <pubDate>Sun, 27 Mar 2022 14:36:01 +0100</pubDate>
        <link>http://localhost:4000/install-nvidia-driver</link>
        <guid isPermaLink="true">http://localhost:4000/install-nvidia-driver</guid>
        
        
      </item>
    
      <item>
        <title>Local Boy visited the Etihad Stadium. Here is how it went.</title>
        <description>
&lt;figure&gt;
  &lt;p&gt;&lt;img src=&quot;/assets/posts/2021-12-27-local-boy-visited-the-etihad-stadium-here-is-how-it-went/dream-come-true.jpeg&quot; /&gt;&lt;/p&gt;
  &lt;figcaption&gt;dream come true&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;I had been dreaming about visiting the Etihad Stadium one day in my life. When I first applied for the Chevening Scholarship, watching a Mancity game was one of the reasons why I wanted to come to the UK.&lt;/p&gt;

&lt;p&gt;I knew Boxing Day was a big day for the English Premier League, which is why I booked my ticket for the Manchester City against Leicester Game despite fear of postponement due to the new variant of COVID.&lt;/p&gt;

&lt;p&gt;I traveled from Colchester to Manchester on the 24th of December and, I booked my stay at the Britannica Hotel in the heart of Manchester Piccadilly.&lt;/p&gt;

&lt;p&gt;On the day of the game,  I took the bus to the Stadium. At first glimpse of the Stadium, I was in tears. My childhood dream became a reality. I was at the Etihad Stadium.&lt;/p&gt;

&lt;p&gt;I arrived a bit late and missed the chance to have a full official Stadium tour. I still managed to visit the Stadium by myself later on, and took some nice pictures.&lt;/p&gt;

&lt;h3 id=&quot;meeting-the-legend&quot;&gt;Meeting the legend&lt;/h3&gt;

&lt;p&gt;Firstly, let’s start with a bit of history. I have been supporting Manchester City since 2008 when I found that Vincent Kompany has some Congolese origin and comes from the same region as I. I wish I had the chance to meet him when he played for ManCity, but sadly he retired from professional football years ago.&lt;/p&gt;

&lt;p&gt;I have been a massive fan of him for his leadership on and off the pitch. I consider him my role model; therefore, I had to take my first picture of the day with his statue.&lt;/p&gt;

&lt;figure&gt;
  &lt;p&gt;&lt;img src=&quot;/assets/posts/2021-12-27-local-boy-visited-the-etihad-stadium-here-is-how-it-went/meeting-the-legend.jpeg&quot; /&gt;&lt;/p&gt;
  &lt;figcaption&gt;legend&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h3 id=&quot;the-dream-come-true&quot;&gt;The Dream come true&lt;/h3&gt;

&lt;p&gt;Secondly, I went to the Stadium and took the picture that sums up my childhood dreams, Espoir in the UK.&lt;/p&gt;

&lt;figure&gt;
  &lt;p&gt;&lt;img src=&quot;/assets/posts/2021-12-27-local-boy-visited-the-etihad-stadium-here-is-how-it-went/etihad-stadium.jpeg&quot; /&gt;&lt;/p&gt;
  &lt;figcaption&gt;at Stadium&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h3 id=&quot;cream-on-the-cake-i-watched-a-game-live&quot;&gt;Cream on the cake, I watched a game live.&lt;/h3&gt;

&lt;p&gt;And Finally, before the game, I took another one to show people and everyone I have been watching football with in Goma, Bukavu, Kigali pubs that their friend and the only Manchester City fan they know has made it to the Stadium.&lt;/p&gt;

&lt;figure&gt;
  &lt;p&gt;&lt;img src=&quot;/assets/posts/2021-12-27-local-boy-visited-the-etihad-stadium-here-is-how-it-went/before-game.jpeg&quot; /&gt;&lt;/p&gt;
  &lt;figcaption&gt;before the game&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h3 id=&quot;final-result-and-fpl-disappointments&quot;&gt;Final result and FPL disappointments&lt;/h3&gt;

&lt;p&gt;The game did end up well; we won 6-3; unfortunately, my FPL captain Phil Foden got only one point. I think this is the old FPL course. You_ should not captain a player you will be watching live. _&lt;/p&gt;

&lt;figure&gt;
  &lt;p&gt;&lt;img src=&quot;/assets/posts/2021-12-27-local-boy-visited-the-etihad-stadium-here-is-how-it-went/fpl-team-result.jpeg&quot; /&gt;&lt;/p&gt;
  &lt;figcaption&gt;fpl team result&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;I wish I had the chance to catch the Canal + Broadcasting car to get a chance to pass on TV, but unfortunately, I couldn’t find where the car was standing.&lt;/p&gt;

&lt;p&gt;Now I can say that I finished my year in style. I hope to watch more games in the future and more importantly to catch the CANAL + car next time I visit the Stadium.&lt;/p&gt;

</description>
        <pubDate>Mon, 27 Dec 2021 18:20:39 +0000</pubDate>
        <link>http://localhost:4000/blog/2021/local-boy-visited-the-etihad-stadium-here-is-how-it-went/</link>
        <guid isPermaLink="true">http://localhost:4000/blog/2021/local-boy-visited-the-etihad-stadium-here-is-how-it-went/</guid>
        
        
        <category>non-tech</category>
        
      </item>
    
      <item>
        <title>Lessons learned while Applying to Master’s Degrees In the UK.</title>
        <description>&lt;p&gt;A bit earlier last year, in August 2020, I learned about the &lt;a href=&quot;https://www.chevening.org/&quot;&gt;Chevening scholarship&lt;/a&gt; from the Uk government, and I decided to give it a try. Chevening is a fully-funded scholarship by the UK government; it offers mid-career professionals from some countries the opportunity to study for a master’s degree in any university in the UK.&lt;/p&gt;

&lt;p&gt;Part of the conditions to get the scholarship is to gain entry to any UK University. As a result, Chevening will cover your tuition fees and give you a monthly stipend of 1150 pounds. I needed the scholarship and admission to a UK university, and that is how my journey started.&lt;/p&gt;

&lt;p&gt;In this blog, I will relate my experience with the application process. I will talk about my failures, my success stories, and what I learned during the process.&lt;/p&gt;

&lt;p&gt;But before talking about the universities, let me talk about myself and my background to give you more context.&lt;/p&gt;

&lt;h2 id=&quot;a-local-boy-from-bukavu-who-studied-in-goma&quot;&gt;A local boy from Bukavu who studied in Goma&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;https://lh5.googleusercontent.com/M1sxZv-WK7g0gK0ZnDlbeDh2SewMyBlXvD2M5gMXmu6OCQHLQYo5YAiMq-GMkWQGBe4CWu0vdWMrrPZ5Lubf5gG2CGbn1dztFiegOJR9HdsJ5THG34m_R5xm67fd9cTwbapETJ_7&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The younger me, after my first year of undergraduate, receiving a scholarship worth 120 USD&lt;/p&gt;

&lt;p&gt;I am from DR Congo, and I graduated from ULPGL in Goma, Eastern Congo; and as you may guess, I did my undergraduate studies in French. I did what we called Polytechnic and graduated in Genie Electrique et Informati­que, which is an equivalent of a Bachelor’s degree in Electrical Engineering and Computer science. As I spent six years in undergraduates studies, I thought it was a master’s degree, but I am sure it was not.&lt;/p&gt;

&lt;p&gt;I didn’t graduate with distinction despite finishing some years with 72 %. I finished my last year with 64%, which was not a distinction. On average, my grades were around 67% which was an equi­valent of 2:2 degree in the UK or second class.&lt;/p&gt;

&lt;p&gt;I think if I had the same grades in the UK , I would have been a first class degree. However since I came from a poor ranking country , the universities in the UK considered the degree  as second class. I learned this a bit late , but you can check the &lt;a href=&quot;https://www.westminster.ac.uk/sites/default/public-files/general-documents/overseas-academic-qualifications-equivalency-chart.pdf&quot;&gt;following article&lt;/a&gt; for the equivalences per country.&lt;/p&gt;

&lt;p&gt;On top of my undergraduate, I had around five years of experience as a software engineer. I also had some Machine Learning experience acquired via community work and personal projects. I got the chance to participate in a research paper that got published, &lt;a href=&quot;https://arxiv.org/abs/2003.11529&quot;&gt;Masakhane’s paper&lt;/a&gt;. By the time I was applying, I was working on another paper that was not yet published.&lt;/p&gt;

&lt;p&gt;My dream was to find a specialized master in Natural language processing or computational linguistic in the UK. That was my first mistake. Instead of looking for a specialized master in NLP, I could have looked for a more general master in either Machine learning, Data Science or Artificial Intelligence with NLP Courses.&lt;/p&gt;

&lt;h2 id=&quot;the-application-process&quot;&gt;The Application Process&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;https://lh6.googleusercontent.com/Sk5MSRpTRSYxLIfvDHzd5WDqRlHLWNUonEr7nXG0albajremxmTQC4kP6zPAQPHrn-lkf4gC5ztC8imUeIlxeiXI4p3eGyelVBE8ip8lE4efMYroUv7hBHYzUQ2ayYZQsB4zR7KC&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;the-ielts-tests&quot;&gt;The IELTS Tests&lt;/h2&gt;

&lt;p&gt;Coming from a francophone university I knew that it was important for me to have an English test either an IELTS or a TOEFL. I decide to go for the IELTS since I was applying for universities in the UK.&lt;/p&gt;

&lt;p&gt;I sat for the IELTS test back in November 2020, and I was glad to get a 6.5. That was the least score to gain admission to a top university in the UK for 2021-2022. To learn more about my IELTS experience, &lt;a href=&quot;https://murhabazi.com/ielts-how-it-went/&quot;&gt;check out this blog post&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;the-recommendation-letters&quot;&gt;The recommendation letters&lt;/h2&gt;

&lt;p&gt;I was glad and lucky with the recommendations I got. I didn’t leave my University on a good note, so it was not the best place to look for recommendations. However, I managed to get recommendations from my academic mentor from my research group, a former classmate who is already a Ph.D. student and a teaching assistant, and another one from a former colleague. There were generic recommendations where I needed to edit the university and the program I was applying for.&lt;/p&gt;

&lt;h2 id=&quot;my-universities-choices&quot;&gt;My Universities choices&lt;/h2&gt;

&lt;p&gt;As a guy who came from a low-tier university with poor grades, with strong imposter syndrome, and who was afraid to live in big cities; I didn’t bother applying to the top four: Oxbridge, UCL, Imperial, which are the top universities In Machine learning In the UK,In Europe and in the World.&lt;/p&gt;

&lt;p&gt;I went ahead as the first batch and applied to the speech and Natural language processing at Edinburgh and the same program at Sheffield and Artificial Intelligence at Cardiff University. But honestly speaking, they were decent programs.&lt;/p&gt;

&lt;h2 id=&quot;mistakes-i-made-while-applying&quot;&gt;Mistakes I made while applying.&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;https://lh3.googleusercontent.com/ebv42UX_vWcm20D0DMKyW_-PRM69lw2b1BZDOBbY9gp_hT5-zHnkQsBSCZcG3sWi8p_JBLoBVSnLEXfRRJ-TF1QHGfwTx0niP9KUsVox9SLauNAS4Scklxcmcdb6y5gyac1ExGii&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Below are the first mistakes I made during the application process to those universities :&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;My final course transcripts: Instead of applying with all my transcripts illustrating all the courses I studied in my undergrad, I used the transcript for my two last years at University. I later discovered that I should have used all my transcripts, and it was late to edit my application with those transcripts.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Since my degree did not have any equivalent in the UK educational system, I thought it was a master’s degree since I spent six years as an undergrad, but I was mistaken. I Should have called it as it was &lt;em&gt;Licence en Genie Electrique et Informatique&lt;/em&gt;. I finally asked the &lt;a href=&quot;https://academia.stackexchange.com/questions/166060/diploma-equivalence-from-the-old-belgian-system/166062#166062&quot;&gt;question on stack exchange&lt;/a&gt;, and I got the final answer.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;My statement of purpose: The statement of purpose is the letter that opens your doors to universities. While applying to this, mine was good, but it was not outstanding for a student with my experience. I did not illustrate how my undergrad studies and professional experience prepared me for the master.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;the-first-bach-outcome&quot;&gt;The first Bach outcome&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;https://lh4.googleusercontent.com/i0ZDVfj9GVCscjHgI3N9q_6nXKbswhHilgSg2UP8T7VB2VHDR0u2j4B8U62_29GxGYf1YNOv6ZmHp8SP-OFdUaImqKDDitUm0H1CcxWhKmyxeLG2ukmL2ytEG88zmBwBTstb3dXi&quot; /&gt;&lt;/p&gt;

&lt;p&gt;I got back-to-back rejections from those three universities, and the demons of Imposter Syndrome came back again.&lt;/p&gt;

&lt;p&gt;Those universities required students to have at least a 2:1 grade (first class or distinction and from good universities ), that was not my case.&lt;/p&gt;

&lt;p&gt;After failing to get admissions, start looking for middle-class university or Europa leagues teams for those familiar with Premier League.&lt;/p&gt;

&lt;h2 id=&quot;the-second-batch-of-applications&quot;&gt;The second batch of Applications&lt;/h2&gt;

&lt;p&gt;I came across the Data Science master at Cardiff, Data science at Sussex, Big Data and text Analytics at Essex, Data Science and AI at New Castle, and Artificial Intelligence at Leeds university. From my research, those were decent universities with good NLP courses in their curriculum and good researchers doing NLP.&lt;/p&gt;

&lt;h3 id=&quot;what-i-did-to-improve-my-applications&quot;&gt;What I did to improve my Applications&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;https://lh3.googleusercontent.com/Bs6HkG1xpJT1qpqtxl_yWv96rXTduSruS20mbneVlWhk6qJsuPoSEmnvOHImCTrE1lg6Q0r1fzG15qqtLalGeBRuVa_3hjFwWVdxjO3m4z7TJVmYElGIkPMlbhY1v3WqKpuAd7Vo&quot; /&gt;&lt;/p&gt;

&lt;p&gt;I started looking for graduates forums online to learn about the UK’s application process and stumbled across the &lt;a href=&quot;https://www.reddit.com/r/gradadmissions/&quot;&gt;grad admission subreddit&lt;/a&gt;, &lt;a href=&quot;https://www.reddit.com/r/gradadmissions/comments/9pd2b8/the_statement_of_purpose_prompt_for_ucsd_is_a/&quot;&gt;the statement of purpose subreddit.&lt;/a&gt; I found a fantastic &lt;a href=&quot;http://writeivy.com/structure-is-magic-a-guide-to-the-graduate-sop/&quot;&gt;guide&lt;/a&gt; on the subreddit written  by a graduate study expert, and I applied all the advice written in that blog. The statement of purpose subreddit has many students going in the same process as you and who are willing to help you review your SOP. I was glad I got help from the subreddit, and it helped me proofread my new statement.&lt;/p&gt;

&lt;p&gt;I went ahead and got all my transcripts from my university and translated them. I also got a letter confirming that I finished my studies at my university since we did not have our certificate signed by our Minister now. Don’t ask me why because I don’t have an answer for that.&lt;/p&gt;

&lt;p&gt;I applied to those universities with my updated transcripts, statements of purpose, and recommendation letters and started waiting for the results.&lt;/p&gt;

&lt;h3 id=&quot;the-second-batchs-outcome&quot;&gt;The second batch’s Outcome&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;https://lh4.googleusercontent.com/CU1lBE_xq9SWKo07bw3Z0ruD_usjutTMG3tci1Q0ewBBuZ5ZclIHj1R2DKAllST8wfNSnzqHzWYxiOvBgAgjwNLSaEVhEAM4XJhKv3uTEFe4wSoU1GGY6IWf5MnIaG27BTNiIBAF&quot; /&gt;&lt;/p&gt;

&lt;p&gt;I got another rejection from Leeds. But, to my dismay, in April, I got an Unconditional offer from Essex. The offer was enough to put me in an excellent position to get the Chevening scholarship. I submitted it as my second choice to Chevening with Cardiff, and Edinburgh being my 1st and third choices.&lt;/p&gt;

&lt;p&gt;I later received another offer from Sussex, New Castle, and a conditional offer from the Data Science program at Cardiff University.&lt;/p&gt;

&lt;p&gt;By the time I got the offer from those universities, I had already passed the Chevening Interview, and I could not amend my choices. Should I have got a chance to do so, I could have picked Sussex as my first choice and Cardiff as my third choice.&lt;/p&gt;

&lt;h2 id=&quot;the-final-decision&quot;&gt;The Final Decision&lt;/h2&gt;

&lt;p&gt;No matter how many offers I got, I had to attend only one university.&lt;/p&gt;

&lt;h3 id=&quot;why-sussex&quot;&gt;Why Sussex?&lt;/h3&gt;

&lt;p&gt;I found the master of Data Science at Susses a perfect choice for me and my career goals and NLP ambitions. The course has two NLP modules in the curriculum and has some good professors doing NLP research. It was near Brighton on the beach and had a good reputation for African students. And while applying for the university, I found that it has my local African university in its choices.&lt;/p&gt;

&lt;h3 id=&quot;why-cardiff-at-third&quot;&gt;Why Cardiff At third&lt;/h3&gt;

&lt;p&gt;Although their international department communication was good, I did not meet their condition in time to secure my scholarship. It required me to have my final degree signed by our minister on time but with our country’s politics that was not possible. There is the letter called &lt;em&gt;A qui de droit&lt;/em&gt;, which the university declined. After many back and forth with the grad admission team, I finally got an unconditional offer, but it was too late for the Chevening application deadline.&lt;/p&gt;

&lt;h3 id=&quot;so-i-ended-up-in-essex&quot;&gt;So I Ended up in Essex&lt;/h3&gt;

&lt;p&gt;Since it was the only university I submitted to Chevening after the interview, I got an offer. It was impossible to change it later, it was the only option I had, but if I had the choice to change, I could have chosen Sussex or Cardiff instead.&lt;/p&gt;

&lt;p&gt;I am not very happy with the university admission department, and they are somehow slowish. To illustrate that, we are one month before the course start date, but I haven’t yet got my Confirmation of Acceptance of Studies ( CAS). Anyway, it is already late to change my university choice. I will have to go there.&lt;/p&gt;

&lt;h2 id=&quot;what-i-am-looking-forward-to&quot;&gt;What I am looking forward to&lt;/h2&gt;

&lt;p&gt;I think the calm in the town will help me to concentrate on my studies. I am looking forward to interacting with Prof &lt;a href=&quot;https://www.essex.ac.uk/people/jamee22406/shoaib-jameel&quot;&gt;Dr. Shoaib Jameel&lt;/a&gt; and integrating his NLP group to research on great questions answering, search and information retrieval.&lt;/p&gt;

&lt;p&gt;Colchester seems to be a quiet city for me but  it is just one hour from London. I plan to enjoy my studies there and spend some weekends in London.&lt;/p&gt;

&lt;p&gt;I will enjoy my journey In the UK as much as possible and watch at least two premiers leagues games. The Manchester derby and a London derby.&lt;/p&gt;

&lt;h2 id=&quot;what-next-after-my-master&quot;&gt;What Next After My master&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;https://lh4.googleusercontent.com/xB8NvTHRCRnGLIBgb_hSJydGN4MG1lkY2ZC1IHW41UI0YNIOJ2lEervFw6KlgnThwtztOg3b6n7IibusYwo3I7BdXjVuPOLATkDWHTVwPwsdPPiqkTDzYizsB-T2urG3R5HAruly&quot; /&gt;&lt;/p&gt;

&lt;p&gt;What next After graduating, I will come back home and give back to the community. Yes, I will come back, my African people. I am not planning to stay in the UK; I will return to Africa; those are my ambitions. I hope to learn a lot during my studies and work hard for two years and see if I will still have the energy to apply for a Ph.D., this time in a top-four team.&lt;/p&gt;

&lt;h2 id=&quot;fears-reflections-and-joys&quot;&gt;Fears, Reflections, and Joys&lt;/h2&gt;

&lt;p&gt;The good news is that I got the Chevening scholarship, and I will be a sponsored student in the Uk for the following year. I have heard people congratulate me for that and tell me that it is a prestigious scholarship but let wait and see.&lt;/p&gt;

&lt;p&gt;I regret that I didn’t get admission from Edinburgh or Sheffield, which was kind of Europa League teams. I somehow regret the fact that I didn’t even apply to Oxbridge, University College London, or Imperial College London, like Man City, Man United, Liverpool, and Chelsea.&lt;/p&gt;

&lt;p&gt;My biggest fear is losing my job. I have a paying job that is paying me more than the monthly stipend Chevening gives. Will I be able to work part-time while studying? If not, how will I be able to support my family while studying?&lt;/p&gt;

&lt;p&gt;How will I get money to enjoy weekends in the UK, football matches, and meet beautiful girls?&lt;/p&gt;

&lt;p&gt;But sometimes I just sit down and say, it will be fine.&lt;/p&gt;

&lt;h2 id=&quot;lessons-learned&quot;&gt;Lessons learned&lt;/h2&gt;

&lt;p&gt;To sum up, here are the few lessons I learned:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;If you are in CS, this tool, &lt;a href=&quot;http://csrankings.org/#/index?nlp&amp;amp;ir&amp;amp;uk&quot;&gt;CSRankings: Computer Science Rankings&lt;/a&gt;, is a must-have you can use to learn more about university rankings. What I enjoyed about the website is that it doesn’t only give you university rankings. Still, it can rank university per topic or area and give you a list of professors working on a given topic. Those pieces of information are instrumental when you are writing your statement of purpose.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Join graduate communities on Reddit &lt;a href=&quot;https://www.reddit.com/r/gradadmissions/&quot;&gt;r/gradadminssion&lt;/a&gt;, &lt;a href=&quot;https://www.reddit.com/r/StatementOfPurpose/&quot;&gt;r/statement of purpose&lt;/a&gt;, and you will enjoy the help and resources you can find on those groups. I am not a fan of other social media platforms, but I am sure you can get more support and find similar groups to those subreddits.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;During the application time, most of the universities organize open days, virtual or on-site events. Make sure you attend those events and learn as much as you can about your department and the international admission department. This will help to put you in a good position when writing your statement of purpose and should put you in touch with the international admission team.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Make sure all your documents are ready on time. If you did not do your undergraduate in English, ensure you have certified translations of all your documents on time.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;If you are from a French university, start early to get an IELTS or TOEFL score on time,&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Make sure your recommendations are ready.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Never underestimate your contributions to community projects and the work you did back in university.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;And start applying early to learn as much as you can in the process.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;And to conclude, I will leave you with one of my favorite quotes:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://lh6.googleusercontent.com/DWzKbI2f7bbP_eCsfYXxShke33eQuW3zotTW_NlC1OJmA8EepOryVwKHSHn39N05-QX2xwjtKI8dpglgdYmorgICSvOCzg2WNK7liaJdF8m4ywfg2ct9wUwbyWqFvLI6c-6MHKob&quot; /&gt;&lt;/p&gt;

</description>
        <pubDate>Fri, 05 Nov 2021 23:00:36 +0000</pubDate>
        <link>http://localhost:4000/blog/2021/lessons-learned-while-applying-to-masters-degrees-in-the-uk/</link>
        <guid isPermaLink="true">http://localhost:4000/blog/2021/lessons-learned-while-applying-to-masters-degrees-in-the-uk/</guid>
        
        
        <category>non-tech</category>
        
      </item>
    
      <item>
        <title>How I break up with pip and fall in love with poetry my new girlfriend.</title>
        <description>&lt;p&gt;I have recently stumbled across &lt;a href=&quot;https://python-poetry.org/&quot;&gt;poetry&lt;/a&gt; new dependency management for python and decided to give it a try.&lt;/p&gt;

&lt;p&gt;I have been a die hard fan of pip and had used it in most of my projects before I discovered poetry. Furthermore, I had heard about pyenv in the past but was reluctant to use it in my projects for preference reasons. Since Python dependency management is an interesting topic, I would like to explain the difference in another article such as &lt;a href=&quot;https://github.com/pypa/pip&quot;&gt;pip&lt;/a&gt; vs &lt;a href=&quot;https://github.com/pyenv/pyenv&quot;&gt;pyenv&lt;/a&gt; vs &lt;a href=&quot;https://github.com/python-poetry/poetry&quot;&gt;petry&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;When I discovered poetry and tested it, I fell in love with it.&lt;/p&gt;

&lt;h2 id=&quot;what-is-poetry&quot;&gt;What is poetry?&lt;/h2&gt;

&lt;p&gt;Although I will be talking about girlfriends, falling in love , and breakups in this article, the poetry I am talking about is not about love, prose, poems, or Shakespeare.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Poetry is a tool for &lt;strong&gt;dependency management&lt;/strong&gt; and &lt;strong&gt;packaging&lt;/strong&gt; in Python. It allows you to declare the libraries your project depends on and it will manage (install/update) them for you.&lt;/em&gt; It supports Python 2.7 and 3.5+&lt;/p&gt;

&lt;p&gt;If you work with python and install packages you should be familiar with &lt;code class=&quot;language-clojure highlight highlighter-rouge&quot;&gt;&lt;span class=&quot;n&quot;&gt;pip&lt;/span&gt;&lt;/code&gt; my old girlfriend.&lt;/p&gt;

&lt;h2 id=&quot;why-we-should-use-poetry-in-lieu-of-pip&quot;&gt;Why we should use poetry in lieu of pip?&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;https://lh3.googleusercontent.com/WEae7StOBGmS6vkaiYroiIwEZWQ2c4DFa56RwhpuOLrzlJfMXlTfUxnpu299Xme-cRGZBdqA0HYUdVUQvduv3cwDSZHr8TMt6o4nwd4DmnWCRjco2xXlHndDSWn_rsQAPRM5saY=s0&quot; /&gt;&lt;/p&gt;

&lt;p&gt;After 2 weeks of usages and successful migration of five personal projects from &lt;code class=&quot;language-bash highlighter-rouge&quot;&gt;pip&lt;/code&gt; to &lt;code class=&quot;language-clojure highlight highlighter-rouge&quot;&gt;&lt;span class=&quot;n&quot;&gt;poetry&lt;/span&gt;&lt;/code&gt;, I can choose poetry because :&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;It has a good dependency resolver. It does the job better than PIP. Read the interesting article &lt;a href=&quot;https://www.activestate.com/resources/quick-reads/python-dependencies-everything-you-need-to-know/&quot;&gt;www.activestate.com&lt;/a&gt;. The author explicitly said&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
  &lt;p&gt;Unfortunately, pip makes no attempt to resolve dependency conflicts. For example, if you install two packages, package A may require a different version of a dependency than package B requires.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;And another advantage I found is that anytime you add a new dependency to the project poetry update for you the &lt;code class=&quot;language-bash highlighter-rouge&quot;&gt;pyproject.toml&lt;/code&gt; with the new top-level dependency, it, therefore, avoid you to do &lt;code class=&quot;language-clojure highlight highlighter-rouge&quot;&gt;&lt;span class=&quot;n&quot;&gt;pip&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;freeze&lt;/span&gt;&lt;/code&gt; to generate a new requirement file for your project.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;You can use the same tool to build and publish your packages.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In my opinion, I think &lt;code class=&quot;language-bash highlighter-rouge&quot;&gt;poetry&lt;/code&gt; outweigh &lt;code class=&quot;language-clojure highlight highlighter-rouge&quot;&gt;&lt;span class=&quot;n&quot;&gt;pip&lt;/span&gt;&lt;/code&gt; in many aspects. It is like pip but on steroids&lt;/p&gt;

&lt;p&gt;In the following sections, I will guide you on how to migrate an existing project from pip to poetry.&lt;/p&gt;

&lt;h2 id=&quot;installing-poetry-in-your-system&quot;&gt;Installing Poetry in your system&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;https://lh5.googleusercontent.com/prDFZYFCdhOvTIpSpv8fItqiZ3GHrHEypuEhY0J2IyORNHoOwd6JlneUEUEGlcE-yRR0xVGkOUlwIeDWc5DfSCMrpJqX5m_CQxcERZ2fUzLmmOeV-dF-OYUbMAAg0t0uvxhAN-o=s0&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Installing poetry is very straightforward, if you have python installed and &lt;code class=&quot;language-clojure highlight highlighter-rouge&quot;&gt;&lt;span class=&quot;n&quot;&gt;curl&lt;/span&gt;&lt;/code&gt; you can easily install it by running :&lt;/p&gt;

&lt;h4 id=&quot;osx--linux--bashonwindows-install-instructions&quot;&gt;osx / linux / bashonwindows install instructions&lt;/h4&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
curl &lt;span class=&quot;nt&quot;&gt;-sSL&lt;/span&gt; https://raw.githubusercontent.com/python-poetry/poetry/master/get-poetry.py | python -

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;windows-powershell-install-instructions&quot;&gt;windows powershell install instructions&lt;/h3&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;Invoke-WebRequest &lt;span class=&quot;nt&quot;&gt;-Uri&lt;/span&gt; https://raw.githubusercontent.com/python-poetry/poetry/master/install-poetry.py &lt;span class=&quot;nt&quot;&gt;-UseBasicParsing&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;.Content | python -

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Warning&lt;/strong&gt;: The previous &lt;code class=&quot;language-bash highlighter-rouge&quot;&gt;get-poetry.py&lt;/code&gt; installer is now deprecated, if you are currently using it you should migrate to the new, supported, &lt;code class=&quot;language-clojure highlight highlighter-rouge&quot;&gt;&lt;span class=&quot;n&quot;&gt;install-poetry.py&lt;/span&gt;&lt;/code&gt; installer.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;The installer installs the &lt;code class=&quot;language-bash highlighter-rouge&quot;&gt;poetry&lt;/code&gt; tool to Poetry’s &lt;code class=&quot;language-clojure highlight highlighter-rouge&quot;&gt;&lt;span class=&quot;n&quot;&gt;bin&lt;/span&gt;&lt;/code&gt; directory. This location depends on your system:&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;language-clojure highlight highlighter-rouge&quot;&gt;&lt;span class=&quot;n&quot;&gt;$HOME/.local/bin&lt;/span&gt;&lt;/code&gt; for Unix&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;language-clojure highlight highlighter-rouge&quot;&gt;&lt;span class=&quot;n&quot;&gt;%APPDATA%&lt;/span&gt;&lt;span class=&quot;sc&quot;&gt;\P&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ython&lt;/span&gt;&lt;span class=&quot;sc&quot;&gt;\S&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cripts&lt;/span&gt;&lt;/code&gt; on Windows&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
  &lt;p&gt;If this directory is not on your &lt;code class=&quot;language-bash highlighter-rouge&quot;&gt;PATH&lt;/code&gt;, you will need to add it manually if you want to invoke Poetry with simply &lt;code class=&quot;language-clojure highlight highlighter-rouge&quot;&gt;&lt;span class=&quot;n&quot;&gt;poetry&lt;/span&gt;&lt;/code&gt;.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;Alternatively, you can use the full path to &lt;code class=&quot;language-clojure highlight highlighter-rouge&quot;&gt;&lt;span class=&quot;n&quot;&gt;poetry&lt;/span&gt;&lt;/code&gt; to use it.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;There is also another version of installing it with &lt;code class=&quot;language-clojure highlight highlighter-rouge&quot;&gt;&lt;span class=&quot;n&quot;&gt;pip&lt;/span&gt;&lt;/code&gt; but why would you use your ex to attract your new girlfriend? 🤔🤪&lt;/p&gt;

&lt;p&gt;Once everything is installed you can restart your terminal and run the following command to check the poetry version:&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-clojure highlight highlighter-rouge&quot;&gt;&lt;span class=&quot;n&quot;&gt;poetry&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;--version&lt;/span&gt;&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;If installation is unsuccessfully or encountering incompatibility issues. Please heads up to &lt;a href=&quot;https://github.com/python-poetry/poetry&quot;&gt;Github Poetry&lt;/a&gt; to get a help, to learn more or to fire an issue.&lt;/p&gt;

&lt;h2 id=&quot;the-migration&quot;&gt;The Migration&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;https://lh4.googleusercontent.com/S09PZBBBn_Q9Vx8vpxLNP67_9HmU-JEM50KpnZZaZavhqS3y2tzfDFuvHlL59CJo_UKhtRtYyWofhx5zlUtvUbk3yO5HHsMM4rqs6xH0fCKaGWZsjlBX7T3j_R0WdPjvf1gG3U0=s0&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;generate-top-level-dependencies&quot;&gt;Generate top-level dependencies&lt;/h3&gt;

&lt;p&gt;Before moving to the next step you need to make sure you can generate the top-level dependencies for your project, to do that you will need a package called &lt;a href=&quot;https://pypi.org/project/pipdeptree/&quot;&gt;pipdeptree&lt;/a&gt; . For context, the top-level dependencies are the root of your dependencies tree. What is even the dependency tree? Each package you install using &lt;code class=&quot;language-bash highlighter-rouge&quot;&gt;pip&lt;/code&gt; has the other dependencies that rely on it. And before installing a new package it installs his top-level dependencies. For example, pandas is a package but &lt;code class=&quot;language-bash highlighter-rouge&quot;&gt;pandas&lt;/code&gt; depends on &lt;code class=&quot;language-clojure highlight highlighter-rouge&quot;&gt;&lt;span class=&quot;n&quot;&gt;numpy&lt;/span&gt;&lt;/code&gt;, if you install pandas it install also numpy as a dependent.&lt;/p&gt;

&lt;p&gt;The following command will generate only the top-level dependencies, so if you have installed pandas, it will just generate pandas and not numpy as a requirement.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Why is this important? :&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;This should be filled&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-clojure highlight highlighter-rouge&quot;&gt;&lt;span class=&quot;n&quot;&gt;pipdeptree&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;--warn&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;silence&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;|&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;grep&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;-E&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;'^&lt;/span&gt;&lt;span class=&quot;sc&quot;&gt;\w&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;requirements-new.txt&lt;/span&gt;&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Once you have generated the top-level dependencies, I would suggest you deactivate your virtual environment and delete it to make the break-up complete before moving to the next steps.&lt;/p&gt;

&lt;h3 id=&quot;adding-poetry-to-an-existing-project&quot;&gt;Adding poetry to an existing project.&lt;/h3&gt;

&lt;p&gt;If you have a new project where you are using &lt;code class=&quot;language-bash highlighter-rouge&quot;&gt;pip&lt;/code&gt; and have the &lt;code class=&quot;language-clojure highlight highlighter-rouge&quot;&gt;&lt;span class=&quot;n&quot;&gt;requirements.txt&lt;/span&gt;&lt;/code&gt; file inside you can run the following command to initialize poetry in the project.&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-clojure highlight highlighter-rouge&quot;&gt;&lt;span class=&quot;n&quot;&gt;poetry&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;init&lt;/span&gt;&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;This will prompt you to set up poetry to your existing project and asked you to give some details about your project such as the project name, the python version you want to use, and the description. It will consequently generate the &lt;code class=&quot;language-clojure highlight highlighter-rouge&quot;&gt;&lt;span class=&quot;n&quot;&gt;pyproject.toml&lt;/span&gt;&lt;/code&gt; file which will contain all the details about your project as well as the top-level projects requirement and their versions.&lt;/p&gt;

&lt;h3 id=&quot;creating-virtual-environment&quot;&gt;Creating virtual environment&lt;/h3&gt;

&lt;p&gt;Poetry creates by default virtual environment in a folder called &lt;code class=&quot;language-clojure highlight highlighter-rouge&quot;&gt;&lt;span class=&quot;o&quot;&gt;~&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;/Library/Application&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Support/pypoetry&lt;/span&gt;&lt;/code&gt; but you can change those settings by using the following command :&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-clojure highlight highlighter-rouge&quot;&gt;&lt;span class=&quot;n&quot;&gt;poetry&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;config&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;virtualenvs.in-project&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;true&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;After running that command you can run the following :&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-clojure highlight highlighter-rouge&quot;&gt;&lt;span class=&quot;n&quot;&gt;poetry&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shell&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;It will activate the project’s virtual environment and create a new one if the project does not have one.&lt;/p&gt;

&lt;h3 id=&quot;installing-the-requirements-for-your-projects&quot;&gt;Installing the requirements for your projects.&lt;/h3&gt;

&lt;p&gt;If you have the &lt;code class=&quot;language-clojure highlight highlighter-rouge&quot;&gt;&lt;span class=&quot;n&quot;&gt;requirements-news.txt&lt;/span&gt;&lt;/code&gt; file resulting from the command you run on the first step, you can install all the packages in that and their corresponding version by running the following command:&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-clojure highlight highlighter-rouge&quot;&gt;&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;item&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;in&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;sed&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;-n&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;ss&quot;&gt;'s/==/&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;@&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;/p&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;requirements-new.txt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;; do poetry add &quot;${item}&quot; ; done&lt;/span&gt;&lt;/code&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;This will work only on Linux and Mac, still trying to find the exact version of it for Windows.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;What does that command do?
I loop over every line of the &lt;code class=&quot;language-bash highlighter-rouge&quot;&gt;requirement-new.txt&lt;/code&gt; file take the dependency, and just replace the &lt;code class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;&lt;/code&gt; in the dependency with &lt;code class=&quot;language-clojure highlight highlighter-rouge&quot;&gt;&lt;span class=&quot;o&quot;&gt;@&lt;/span&gt;&lt;/code&gt; and then add it with poetry.&lt;/p&gt;

&lt;p&gt;If for example in the file you have pandas==1.1.1, it will install the following with poetry &lt;code class=&quot;language-clojure highlight highlighter-rouge&quot;&gt;&lt;span class=&quot;n&quot;&gt;poetry&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pandas&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;@&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1.11&lt;/span&gt;&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;If everything goes well you should have the all the top-level packages installed with their dependencies.&lt;/p&gt;

&lt;p&gt;Once the command has successfully run and you have everything installed, you should check if your &lt;code class=&quot;language-clojure highlight highlighter-rouge&quot;&gt;&lt;span class=&quot;n&quot;&gt;pyproject.toml&lt;/span&gt;&lt;/code&gt; file contains all the packages and their top-level dependencies.&lt;/p&gt;

&lt;p&gt;You can now remove the old &lt;code class=&quot;language-bash highlighter-rouge&quot;&gt;requirements.txt&lt;/code&gt; file and the newly create &lt;code class=&quot;language-clojure highlight highlighter-rouge&quot;&gt;&lt;span class=&quot;n&quot;&gt;requirement-new.txt&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;/code&gt;file by running.&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-clojure highlight highlighter-rouge&quot;&gt;&lt;span class=&quot;n&quot;&gt;rm&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;-f&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;requirements.*&lt;/span&gt;&lt;/code&gt;&lt;/p&gt;

&lt;h3 id=&quot;a-section-about-using-poetry-with-conda-enviroment&quot;&gt;A section about using poetry with conda enviroment&lt;/h3&gt;

&lt;p&gt;Some people like to have multiples girlfriend and may like to keep their old conda or pip environment. I haven’t tried this approach yet , but according to&lt;a href=&quot;https://github.com/python-poetry/poetry/issues/105#issuecomment-498042062&quot;&gt; this issue&lt;/a&gt; it is possible to use poetry to install packages in a python environment.&lt;/p&gt;

&lt;p&gt;You just have to configure poetry to not create a virtual environment in a project and install your packages in the conda  or pip environment.&lt;/p&gt;

&lt;p&gt;I think you can try it and let us know in comment how it goes.&lt;/p&gt;

&lt;h3 id=&quot;bonus-the-dockerfile&quot;&gt;Bonus, the Dockerfile.&lt;/h3&gt;

&lt;p&gt;If you have a dockerfile you can edit it and use the following docker images which use multi-stage build to install all your requirement with poetry.&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;FROM python:3.7.5 AS base
LABEL &lt;span class=&quot;nv&quot;&gt;maintainer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;Espoir Murhabazi &amp;lt; first_name.second_name[:3] on gmail.com&amp;gt;&quot;&lt;/span&gt;

ENV  &lt;span class=&quot;nv&quot;&gt;PYTHONUNBUFFERED&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;1  &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;PYTHONDONTWRITEBYTECODE&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;1  &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;PIP_NO_CACHE_DIR&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;off  &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;PIP_DISABLE_PIP_VERSION_CHECK&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;on  &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;PIP_DEFAULT_TIMEOUT&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;100  &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;POETRY_HOME&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;/opt/poetry&quot;&lt;/span&gt;  &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;POETRY_VIRTUALENVS_IN_PROJECT&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;true&lt;/span&gt;  &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;POETRY_NO_INTERACTION&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;1  &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;PYSETUP_PATH&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;/opt/pysetup&quot;&lt;/span&gt;  &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;VENV_PATH&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;/opt/pysetup/.venv&quot;&lt;/span&gt;

ENV  &lt;span class=&quot;nv&quot;&gt;PATH&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$POETRY_HOME&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;/bin:&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$VENV_PATH&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;/bin:&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$PATH&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;

FROM  base  AS  python-deps

RUN  apt-get  update  &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;

&lt;span class=&quot;o&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt;  apt-get  &lt;span class=&quot;nb&quot;&gt;install&lt;/span&gt;  &lt;span class=&quot;nt&quot;&gt;--no-install-recommends&lt;/span&gt;  &lt;span class=&quot;nt&quot;&gt;-y&lt;/span&gt;  &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;

curl  &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;

build-essential
&lt;span class=&quot;c&quot;&gt;# Install Poetry - respects $POETRY_VERSION &amp;amp; $POETRY_HOME&lt;/span&gt;
ENV &lt;span class=&quot;nv&quot;&gt;POETRY_VERSION&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;1.1.7

RUN curl &lt;span class=&quot;nt&quot;&gt;-sSL&lt;/span&gt; https://raw.githubusercontent.com/sdispater/poetry/master/get-poetry.py | python
WORKDIR &lt;span class=&quot;nv&quot;&gt;$PYSETUP_PATH&lt;/span&gt;

COPY ./poetry.lock ./pyproject.toml ./
RUN poetry &lt;span class=&quot;nb&quot;&gt;install&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--no-dev&lt;/span&gt;
FROM base AS runtime
COPY &lt;span class=&quot;nt&quot;&gt;--from&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;python-deps &lt;span class=&quot;nv&quot;&gt;$POETRY_HOME&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$POETRY_HOME&lt;/span&gt;
COPY &lt;span class=&quot;nt&quot;&gt;--from&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;python-deps &lt;span class=&quot;nv&quot;&gt;$PYSETUP_PATH&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$PYSETUP_PATH&lt;/span&gt;
RUN useradd &lt;span class=&quot;nt&quot;&gt;-ms&lt;/span&gt; /bin/bash espy
COPY &lt;span class=&quot;nb&quot;&gt;.&lt;/span&gt; /home/espy
WORKDIR /home/espy
USER espy
CMD &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot; you command &quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Basically, what the docker file does, it uses a multi-stage build to first install the packages in the first step and copy only the packages installed in the second as well as the project repository. One of the advantages of the multi-stage build is that it uses only the necessary files your project needs and therefore reduce the memory of your docker container.&lt;/p&gt;

&lt;p&gt;You can learn more about multi-stage build using the &lt;a href=&quot;[https://pythonspeed.com/articles/multi-stage-docker-python/](https://pythonspeed.com/articles/multi-stage-docker-python/)&quot;&gt;following tutorial.&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;And the story of my break up comes to an end. As you may know, all separations are not smooth, sometimes the daemons of your old girlfriend come and start causing troubles in your new relationship. So if you find any issue during this break-up, feel free to let me know in the comment I will try to help you as much as I can 🤔.&lt;/p&gt;
</description>
        <pubDate>Sun, 17 Oct 2021 16:33:42 +0100</pubDate>
        <link>http://localhost:4000/blog/2021/how-i-break-up-with-pip-and-fall-in-love-with-poetry-my-new-girlfriend/</link>
        <guid isPermaLink="true">http://localhost:4000/blog/2021/how-i-break-up-with-pip-and-fall-in-love-with-poetry-my-new-girlfriend/</guid>
        
        
        <category>tech</category>
        
      </item>
    
      <item>
        <title>Mots d'un grand frère  à ses petits frère qui debutent avec la programmation</title>
        <description>
&lt;figure&gt;
  &lt;p&gt;&lt;img src=&quot;/assets/posts/2021-07-23-mots-d-un-grand-frere-a-ses-petits-frere-qui-debutent-avec-la-programmation/andela-guys.jpg&quot; /&gt;&lt;/p&gt;
  &lt;figcaption&gt;cover&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;&lt;em&gt;Caption : My crew , un groupe des developeur qui m’ont fait plaisir: &lt;a href=&quot;https://github.com/Karlmusingo&quot;&gt;Karl Musingo&lt;/a&gt;: G3 ULPGL, &lt;a href=&quot;https://www.linkedin.com/in/hadad-bwenge-345ba7103&quot;&gt;Hadad Bwenge&lt;/a&gt; : , &lt;a href=&quot;https://www.linkedin.com/in/glody-mutombo&quot;&gt;Jean Vincent Mutombo&lt;/a&gt; (UTB - Gisenyi), &lt;a href=&quot;https://rw.linkedin.com/in/grace-lungu-262306190&quot;&gt;Grace Lungu&lt;/a&gt; (International University of East Africa Kampala) among the best Engineer we had back in the days at Andela.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Je me rappelle il y a cinq ans, je finissais mes études universitaires et à l’époque je rêvais de travailler dans une soi-disant Startup, car je lisais sur les réseaux sociaux que les startups faisaient le buzz à Kigali. À l’époque Je n’avais rien à mettre sur mon CV, je n’avais même pas suffisamment de connaissances dans des langages de programmation, que ce soit en Java ou en aucun autre d’ailleurs, et Dieu merci, c’est avec ce peu de connaissances que j’ai débuté ma carrière en développement informatique, mais je présume que cela n’est pas le cas pour mes petits frères et sœurs qui sont en train de finir leurs études aujourd’hui : c’est la raison pour laqu­elle Je me suis décidé d’écrire cet article, pour aider ceux qui cherchent à se faire une carrière dans notre domaine.&lt;/p&gt;

&lt;p&gt;Il sera structuré de la manière suivante :&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Que faire lorsqu’on est encore l’université ?&lt;/li&gt;
  &lt;li&gt;Que  faire après ses études universitaires ?&lt;/li&gt;
  &lt;li&gt;Que  faire après son premier travail ?&lt;/li&gt;
  &lt;li&gt;Quel profil les entreprises recherchent parmi les candidats ?&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Bien que la plupart des conseils de cet article soient valables pour le domaine de l’informatique et de la programmation en particulier, certains d’entre eux sont également applicables pour n’importe quelle autre carrière.&lt;/p&gt;

&lt;h2 id=&quot;first-thing-first-&quot;&gt;First Thing First :&lt;/h2&gt;

&lt;p&gt;Pour vous rassurer j’aimerais vous dire une réalité fondamentale :&lt;/p&gt;

&lt;p&gt;Les entreprises ont besoin des personnels qualifiés,  de bons développeurs et ces derniers sont et resteront rares pour les jours  à venir. Une chose est sûre, soyez confiant, si vous êtes qualifié les entreprises vous chercheront.&lt;/p&gt;

&lt;p&gt;La grande question est comment se qualifier ?&lt;/p&gt;

&lt;h2 id=&quot;langlais-langage-par-excellence&quot;&gt;L’Anglais Langage Par Excellence&lt;/h2&gt;

&lt;figure&gt;
  &lt;p&gt;&lt;img src=&quot;/assets/posts/2021-07-23-mots-d-un-grand-frere-a-ses-petits-frere-qui-debutent-avec-la-programmation/learn-english.png&quot; /&gt;&lt;/p&gt;
  &lt;figcaption&gt;learn english please&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Avant tout, pour une bonne carrière en développement informatique, ou en informatique en général, pour l’amour du ciel, je le répète pour l’amour du Ciel : APPRENEZ L’ANGLAIS, APPRENEZ L’ANGLAIS, LEARN ENGLISH PEOPLE.&lt;/p&gt;

&lt;p&gt;Vous n’irez nulle part avec la langue Française en informatique. Même les Français et les Belges qui nous ont appris cette langue, utilisent l’anglais comme langue d’apprentissage de l’informatique.&lt;/p&gt;

&lt;p&gt;Que faire si vous n’êtes pas  à l’aise avec la langue de Shakespeare ? :&lt;/p&gt;

&lt;p&gt;Pour ceux qui ont eu la chance de faire de bonnes écoles secondaires comme moi, nous avons déjà une bonne base en anglais, c’est déjà un bon départ.&lt;/p&gt;

&lt;p&gt;Néanmoins, pour tout le monde, le mieux à faire c’est de chercher des clubs d’anglais et de les rejoindre. Là vous allez rencontrer des gens qui vont vous stimuler à apprendre et parler couramment l’anglais. Il y en a plein, presque dans toutes les universités du pays(RDC).&lt;/p&gt;

&lt;p&gt;À la limite, soyez à l’aise pour lire et comprendre un tutoriel en anglais. Le reste viendra avec un processus continu de pratique quotidienne. Personnellement cela m’a pris trois ans pour être à l’aise avec l’anglais.&lt;/p&gt;

&lt;h2 id=&quot;tirez-profits-de-vos-cours-à-luniversité&quot;&gt;Tirez profits de vos Cours à l’université&lt;/h2&gt;

&lt;figure&gt;
  &lt;p&gt;&lt;img src=&quot;/assets/posts/2021-07-23-mots-d-un-grand-frere-a-ses-petits-frere-qui-debutent-avec-la-programmation/please-graduate.jpg&quot; /&gt;&lt;/p&gt;
  &lt;figcaption&gt;obtenez ce diplome svp...&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;On aura beau dire que nos programmes universitaires ne sont pas à jour, ou même qu’ils ne sont pas adaptés, mais ce sont ces mêmes programmes qui nous donnent une bonne base. Je suis toujours  reconnaissant pour les cours que j’ai appris à l’université en RDC.&lt;/p&gt;

&lt;p&gt;Mes cours d’Algorithmiques, de programmation orientée objet en Java, des bases des données en général, de génie logiciel, ont été une bonne fondation pour ma carrière en développement informatique.&lt;/p&gt;

&lt;p&gt;Et au-delà de ces cours, soyez curieux, ne vous limitez pas à apprendre pour avoir des pourcentages élevés ou pour vous limiter à passer un examen, lisez ici et là pour apprendre plus de ce que les autres apprennent.&lt;/p&gt;

&lt;p&gt;Plus le temps passera, plus vous serez surpris de ce que vous aurez appris. Défiez vos enseignants en allant au-delà de ce qui vous est transmis à l’université.&lt;/p&gt;

&lt;p&gt;Les cours vous permettront d’avoir ce qu’on appelle les &lt;strong&gt;hard skills&lt;/strong&gt;, mais en plus de hard skills, une des compétences que j’ai acquis après l’université, ce sont les &lt;strong&gt;softs skills&lt;/strong&gt;.&lt;/p&gt;

&lt;h2 id=&quot;les-softs-skills-pour-vous-différencier&quot;&gt;Les Softs Skills pour vous différencier&lt;/h2&gt;

&lt;figure&gt;
  &lt;p&gt;&lt;img src=&quot;/assets/posts/2021-07-23-mots-d-un-grand-frere-a-ses-petits-frere-qui-debutent-avec-la-programmation/lukaku-team-work.jpg&quot; /&gt;&lt;/p&gt;
  &lt;figcaption&gt;le travail en equipe est tres important&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;En plus des cours à l’université, ne sous-estimer jamais les petits clubs qu’on a ici et là, que ça soit à l’église, à la fac ou au sein de la communauté en général.  Elles nous donnent ce qu’on appelle  dans le monde professionnel : le leadership, l’expérience et les soft skills.&lt;/p&gt;

&lt;p&gt;Au sein des organisations estudiantines et des clubs, vous acquérez les softs skills. C’est là où vous pouvez apprendre à travailler en équipe,  à communiquer,  à gérer les conflits ou à gérer toute sorte de personnalité. C’est le genre d’expérience qui vous sera très utile tout au long de votre vie. Apprenez aussi à être discipliné, ponctuel, à communiquer avec les clients, ces genres de petites qualités vous permettront de faire la différence dans un monde ou tout le monde semble les ignorer.&lt;/p&gt;

&lt;p&gt;Avez-vous eu l’occasion de devenir chef de promotion ? Membre du gouvernement estudiantin ? Ne ratez jamais ces genres d’occasions. Elles vous ajouteront quelques lignes à mettre sur votre CV.&lt;/p&gt;

&lt;h2 id=&quot;faites-connaître-votre-travail&quot;&gt;Faites connaître votre travail.&lt;/h2&gt;

&lt;figure&gt;
  &lt;p&gt;&lt;img src=&quot;/assets/posts/2021-07-23-mots-d-un-grand-frere-a-ses-petits-frere-qui-debutent-avec-la-programmation/git-configurations.png&quot; /&gt;&lt;/p&gt;
  &lt;figcaption&gt;get other know your work&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Une fois ces connaissances acquises, hard et soft skills, comment les gens sauront que vous les avez ? Travailler sur votre réputation en ligne.  Si vous n’avez pas un compte Github, s’il vous plait arrêtez de lire ce post et aller en créer un, puis revenez au post.&lt;/p&gt;

&lt;p&gt;Une fois que vous aurez un compte GitHub, que mettre sur celui-ci ? Si vous êtes encore à la fac, travailler sur un TFC ou un mémoire qui vous permettra d’apprendre des projets qui vous seront utiles. Apprenez des plus et soyez curieux, ajoutez tout ce qui vous vient en tête dans le projet. Éviter la paresse et le fait d’écrire un travail juste pour avoir des points. Vos travaux sont les premiers projets que vous présenterez  à vos employeurs. Ayez l’habitude de mettre n’importe quel projet sur lequel vous travaillez en ligne.&lt;/p&gt;

&lt;p&gt;C’est un système de gestion de la boutique de votre tante maternelle que vous avez conçu ? Postez-le, on ne sait jamais.&lt;/p&gt;

&lt;h2 id=&quot;soyez-curieux-et-apprenez-à-googler&quot;&gt;Soyez Curieux et Apprenez à Googler&lt;/h2&gt;

&lt;figure&gt;
  &lt;p&gt;&lt;img src=&quot;/assets/posts/2021-07-23-mots-d-un-grand-frere-a-ses-petits-frere-qui-debutent-avec-la-programmation/google-stuff.jpg&quot; /&gt;&lt;/p&gt;
  &lt;figcaption&gt;learn how to google&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Soyez curieux et apprenez à poser des questions : Voulez vous apprendre quelque chose, une simple requête google peut vous faire tomber sur 2 ou 3 ressources utiles.   Je ne peux pas vous dire comment apprendre à être curieux, car je ne sais pas comment on le fait.&lt;/p&gt;

&lt;h2 id=&quot;limportance-de-la-communauté--networking&quot;&gt;L’importance de la communauté  (Networking)&lt;/h2&gt;

&lt;figure&gt;
  &lt;p&gt;&lt;img src=&quot;/assets/posts/2021-07-23-mots-d-un-grand-frere-a-ses-petits-frere-qui-debutent-avec-la-programmation/gdg-comunity.jpg&quot; /&gt;&lt;/p&gt;
  &lt;figcaption&gt;networking&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Rejoignez les communautés locales ou virtuelles vous y apprendrez beaucoup. Créez  un compte Twitter et rejoignez des groupes de discussion. Mais éviter de passer trop de temps sur des réseaux sociaux lorsque vous travaillez, ce sont de grands bouffeurs de temps.&lt;/p&gt;

&lt;p&gt;Rejoignez Une communauté. Les communautés de développeurs existent partout dans le pays et vous pouvez les rejoindre gratuitement. À Lubumbashi, il y des mouvements tels que &lt;a href=&quot;https://itot.africa/&quot;&gt;ltot Africa&lt;/a&gt;, ou DevsCast. À Goma nous avons&lt;a href=&quot;https://uptodatedevelopers.com/&quot;&gt; Uptodate Developers&lt;/a&gt; et autres. A Kinshasa nous avons &lt;a href=&quot;https://www.kinshasadigital.com/&quot;&gt;Kinshasa Digital Academy&lt;/a&gt; et tant d’autres, récemment à Bukavu nous avons vu Google developer group, etc Nous avons &lt;a href=&quot;https://www.abelmbula.com/&quot;&gt;Abel Mbula&lt;/a&gt; qui f­ait du bon travail à Kisangani.  Il y a aussi diverses communautés en ligne comme freeCodeCamp, &lt;a href=&quot;https://dev.to/&quot;&gt;dev.to&lt;/a&gt;, Codenwebie.&lt;/p&gt;

&lt;p&gt;Ces genres de communautés offrent des opportunités de network, en les côtoyant, Dieu sait ce que vous pouvez avoir comme réseaux. Comme le dit un vieil adage swahili de chez nous, &lt;strong&gt;mutu ni batu&lt;/strong&gt;, Une personne, c’est d’autres personnes.&lt;/p&gt;

&lt;h2 id=&quot;que-cherchent-les-entreprises-&quot;&gt;Que cherchent les entreprises ?&lt;/h2&gt;

&lt;p&gt;Avec des profils en ligne, avec deux ou trois projets en ligne, que ce que les entreprises cherchent d’autre auprès des développeurs ?&lt;/p&gt;

&lt;p&gt;Les entreprises n’ont rien à faire de vos études, de vos projets, de vos connaissances. No one care, Je répète, ils s’en moquent.&lt;/p&gt;

&lt;p&gt;Ce qui les intéresse, c’est comment vos connaissances peuvent résoudre leurs problèmes. C’est  la chose fondamentale à savoir pour avoir une carrière dans ce domaine. Vous devez chercher à savoir les problèmes que les entreprises ont et leur dire comment vos connaissances leur seront utiles. C’est tout.&lt;/p&gt;

&lt;h2 id=&quot;quel-langage-de-programmation-apprendre&quot;&gt;Quel langage de programmation Apprendre&lt;/h2&gt;

&lt;figure&gt;
  &lt;p&gt;&lt;img src=&quot;/assets/posts/2021-07-23-mots-d-un-grand-frere-a-ses-petits-frere-qui-debutent-avec-la-programmation/today-developers.jpg&quot; /&gt;&lt;/p&gt;
  &lt;figcaption&gt;don't learn languages learn concepts&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Je ne veux pas me mettre à faire un débat sur les langages dans ce poste ni même dire quel langage est meilleur que l’autre, c’est hors cadre.&lt;/p&gt;

&lt;p&gt;Une chose est sure, le concept de la logique programmation existe depuis toujours et ne changera jamais comme les Algorithmes et les structures des données, les bases du HTTP, les Bases des données ou le Génie logiciel.&lt;/p&gt;

&lt;p&gt;Les langages viennent et partent. Mais ce concept est là depuis des années et reste là. Un bon ouvrier est quelqu’un qui sait utiliser l’outil adapté au problème. Si vous voulez faire dans le développement web, apprenez le  Javascript c’est incontourna­ble, pour les data science le Python est le langage par excellence.&lt;/p&gt;

&lt;p&gt;Le dernier conseil que je peux donner est de maîtriser au moins un langage de programmation et le reste viendra de soi-même.&lt;/p&gt;

&lt;h2 id=&quot;allons-nous-y-arriver&quot;&gt;Allons nous y arriver?&lt;/h2&gt;
&lt;p&gt; &lt;/p&gt;

&lt;p&gt;Soyez sûr que vous  y arriverez, il y a de jeunes formé aux pays qui ont eu la chance d’émerger en tant que développeur au niveau international, la liste est longue, mais je peux citer ceux qui me viennent en tête.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://cd.linkedin.com/in/davidkathoh&quot;&gt;David Kathoh&lt;/a&gt;, &lt;a href=&quot;https://github.com/nkpremices&quot;&gt;Premices Nzanzu&lt;/a&gt;, &lt;a href=&quot;https://github.com/pacyL2K19&quot;&gt;Pacifique ​​LINJANJA &lt;/a&gt; , &lt;a href=&quot;https://ke.linkedin.com/in/makutano-lucien&quot;&gt;Lucien Makutano&lt;/a&gt;, &lt;a href=&quot;https://rw.linkedin.com/in/benjamin-kafirongo-b8b290128&quot;&gt;Benjamin Kafirongo,&lt;/a&gt; my guy &lt;a href=&quot;https://cd.linkedin.com/in/michael-rukamakama-07a45b186&quot;&gt;Michael Rukamakama&lt;/a&gt;, ma bande des gars de Kigali,  toute la bande des gars de Lushi (la liste est longue, mais ils font du bon travail), et tous les autres devs qui trouvent leurs noms dans &lt;a href=&quot;https://commits.top/congo.html&quot;&gt;le top 10 de la liste suivante&lt;/a&gt;. Vous faites la fierté du pays.&lt;/p&gt;

&lt;h2 id=&quot;conclusion-&quot;&gt;Conclusion :&lt;/h2&gt;

&lt;p&gt;Pendant mes études universitaires je n’ai pas eu des professionnels expérimentés dans mon domaine pour me prodiguer ces genres des conseils, mais vous avez la chance de m’avoir moi. 🤪 j’espère que vous en tirerez profit. Si vous êtes à la fac et avez certaines astuces pour avoir une bonne carrière en programmation, faites nous savoir. S’il y a certains points avec lesquels vous n’êtes pas d’accord dans mon poste faite moi savoir. Si vous avez d’autres points à ajouter dans le blog, veuillez les partager en commentaire.&lt;/p&gt;

&lt;p&gt;Cheers 🤲🏼&lt;/p&gt;
</description>
        <pubDate>Fri, 23 Jul 2021 14:57:09 +0100</pubDate>
        <link>http://localhost:4000/mots-d-un-grand-frere-a-ses-petits-frere-qui-debutent-avec-la-programmation</link>
        <guid isPermaLink="true">http://localhost:4000/mots-d-un-grand-frere-a-ses-petits-frere-qui-debutent-avec-la-programmation</guid>
        
        
        <category>non-tech</category>
        
      </item>
    
  </channel>
</rss>
